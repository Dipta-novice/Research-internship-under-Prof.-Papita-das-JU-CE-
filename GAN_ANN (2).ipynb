{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vWjMqz4BsWWe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "epwbAS0P1SYs",
        "outputId": "c674f2cd-fa81-4bbe-e19f-f2d776e77dfc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c8f7c82-f448-40db-a09c-42f8d6c1aa6a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c8f7c82-f448-40db-a09c-42f8d6c1aa6a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Book2n.xlsx to Book2n.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_excel('Book2n.xlsx')"
      ],
      "metadata": {
        "id": "yzRCUJ4J1s7L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lCDmn8paM4Po",
        "outputId": "de1dcb43-9850-4bbb-fd0f-b26cfc33f154"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f84ef3ff-24f9-4a45-9792-2e04a4867d6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0229</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>29.210345</td>\n",
              "      <td>97.270448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0153</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>29.472414</td>\n",
              "      <td>98.143138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>29.475862</td>\n",
              "      <td>98.154621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>29.517241</td>\n",
              "      <td>98.292414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>29.620690</td>\n",
              "      <td>98.636897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>15</td>\n",
              "      <td>0.1484</td>\n",
              "      <td>5.117241</td>\n",
              "      <td>5.117241</td>\n",
              "      <td>24.882759</td>\n",
              "      <td>82.859586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>30</td>\n",
              "      <td>0.1252</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>25.682759</td>\n",
              "      <td>85.523586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0791</td>\n",
              "      <td>2.727586</td>\n",
              "      <td>2.727586</td>\n",
              "      <td>27.272414</td>\n",
              "      <td>90.817138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0563</td>\n",
              "      <td>1.941379</td>\n",
              "      <td>1.941379</td>\n",
              "      <td>28.058621</td>\n",
              "      <td>93.435207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>29.310345</td>\n",
              "      <td>97.603448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f84ef3ff-24f9-4a45-9792-2e04a4867d6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f84ef3ff-24f9-4a45-9792-2e04a4867d6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f84ef3ff-24f9-4a45-9792-2e04a4867d6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)  time  absorbance  \\\n",
              "0           10           1.0         6               30    15      0.0229   \n",
              "1           10           1.0         6               30    30      0.0153   \n",
              "2           10           1.0         6               30    45      0.0152   \n",
              "3           10           1.0         6               30    60      0.0140   \n",
              "4           10           1.0         6               30   120      0.0110   \n",
              "..         ...           ...       ...              ...   ...         ...   \n",
              "86          30           1.0         6               40    15      0.1484   \n",
              "87          30           1.0         6               40    30      0.1252   \n",
              "88          30           1.0         6               40    45      0.0791   \n",
              "89          30           1.0         6               40    60      0.0563   \n",
              "90          30           1.0         6               40   120      0.0200   \n",
              "\n",
              "        conc  real conc    removal  %removal    \n",
              "0   0.789655   0.789655  29.210345   97.270448  \n",
              "1   0.527586   0.527586  29.472414   98.143138  \n",
              "2   0.524138   0.524138  29.475862   98.154621  \n",
              "3   0.482759   0.482759  29.517241   98.292414  \n",
              "4   0.379310   0.379310  29.620690   98.636897  \n",
              "..       ...        ...        ...         ...  \n",
              "86  5.117241   5.117241  24.882759   82.859586  \n",
              "87  4.317241   4.317241  25.682759   85.523586  \n",
              "88  2.727586   2.727586  27.272414   90.817138  \n",
              "89  1.941379   1.941379  28.058621   93.435207  \n",
              "90  0.689655   0.689655  29.310345   97.603448  \n",
              "\n",
              "[91 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd=data.drop(axis=0,index=[6,20,87,53,66])"
      ],
      "metadata": {
        "id": "V2OwiiWm3Sm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddtest=data.loc[[6,20,87,53,66]]"
      ],
      "metadata": {
        "id": "CRtZgl964otP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddtest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "U89XgvHo5XOk",
        "outputId": "bac0fd5e-e298-4de8-ee98-82b447b5cab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ca554a4d-3bda-4102-9673-47ab70615782\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>29.382759</td>\n",
              "      <td>97.844586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.8246</td>\n",
              "      <td>28.434483</td>\n",
              "      <td>28.434483</td>\n",
              "      <td>1.565517</td>\n",
              "      <td>5.213172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>30</td>\n",
              "      <td>0.1252</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>25.682759</td>\n",
              "      <td>85.523586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.444828</td>\n",
              "      <td>0.444828</td>\n",
              "      <td>29.555172</td>\n",
              "      <td>98.418724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0422</td>\n",
              "      <td>1.455172</td>\n",
              "      <td>1.455172</td>\n",
              "      <td>28.544828</td>\n",
              "      <td>95.054276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca554a4d-3bda-4102-9673-47ab70615782')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca554a4d-3bda-4102-9673-47ab70615782 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca554a4d-3bda-4102-9673-47ab70615782');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)  time  absorbance  \\\n",
              "6           20           1.0         6               30    30      0.0179   \n",
              "20          90           1.0         6               30    15      0.8246   \n",
              "87          30           1.0         6               40    30      0.1252   \n",
              "53          30           1.0         4               30    45      0.0129   \n",
              "66          30           1.0        10               30    15      0.0422   \n",
              "\n",
              "         conc  real conc    removal  %removal    \n",
              "6    0.617241   0.617241  29.382759   97.844586  \n",
              "20  28.434483  28.434483   1.565517    5.213172  \n",
              "87   4.317241   4.317241  25.682759   85.523586  \n",
              "53   0.444828   0.444828  29.555172   98.418724  \n",
              "66   1.455172   1.455172  28.544828   95.054276  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [dd, gen_samples]\n",
        "\n",
        "ddgan = pd.concat(frames)"
      ],
      "metadata": {
        "id": "DTriDxmh5pj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddgan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "mOQ5IyPI6uSb",
        "outputId": "a130407f-d178-41a9-cec7-1e08b2aece83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1d772b91-21ae-4c97-8854-5dd554bd9064\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.022900</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>29.210345</td>\n",
              "      <td>97.270448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>29.472414</td>\n",
              "      <td>98.143138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>0.015200</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>29.475862</td>\n",
              "      <td>98.154621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>29.517241</td>\n",
              "      <td>98.292414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>29.620690</td>\n",
              "      <td>98.636897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>24.343243</td>\n",
              "      <td>3.823370</td>\n",
              "      <td>3.833190</td>\n",
              "      <td>31.374702</td>\n",
              "      <td>28.174414</td>\n",
              "      <td>0.253567</td>\n",
              "      <td>2.680391</td>\n",
              "      <td>1.608387</td>\n",
              "      <td>23.266228</td>\n",
              "      <td>85.779030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>27.789547</td>\n",
              "      <td>0.636933</td>\n",
              "      <td>2.437756</td>\n",
              "      <td>32.717197</td>\n",
              "      <td>12.064705</td>\n",
              "      <td>1.026199</td>\n",
              "      <td>4.306577</td>\n",
              "      <td>6.374099</td>\n",
              "      <td>22.542580</td>\n",
              "      <td>77.305450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>20.776632</td>\n",
              "      <td>5.602990</td>\n",
              "      <td>4.166401</td>\n",
              "      <td>33.044666</td>\n",
              "      <td>52.454540</td>\n",
              "      <td>-0.558148</td>\n",
              "      <td>5.100426</td>\n",
              "      <td>0.960695</td>\n",
              "      <td>24.887739</td>\n",
              "      <td>94.021698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>33.384750</td>\n",
              "      <td>3.927462</td>\n",
              "      <td>5.439592</td>\n",
              "      <td>41.028702</td>\n",
              "      <td>27.715914</td>\n",
              "      <td>0.336721</td>\n",
              "      <td>4.286004</td>\n",
              "      <td>3.990602</td>\n",
              "      <td>30.985470</td>\n",
              "      <td>109.060081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>31.555012</td>\n",
              "      <td>6.422427</td>\n",
              "      <td>4.379303</td>\n",
              "      <td>42.141502</td>\n",
              "      <td>67.401848</td>\n",
              "      <td>-1.691420</td>\n",
              "      <td>7.091534</td>\n",
              "      <td>1.887751</td>\n",
              "      <td>29.565063</td>\n",
              "      <td>110.525993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>237 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d772b91-21ae-4c97-8854-5dd554bd9064')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d772b91-21ae-4c97-8854-5dd554bd9064 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d772b91-21ae-4c97-8854-5dd554bd9064');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)        time  \\\n",
              "0     10.000000      1.000000  6.000000        30.000000   15.000000   \n",
              "1     10.000000      1.000000  6.000000        30.000000   30.000000   \n",
              "2     10.000000      1.000000  6.000000        30.000000   45.000000   \n",
              "3     10.000000      1.000000  6.000000        30.000000   60.000000   \n",
              "4     10.000000      1.000000  6.000000        30.000000  120.000000   \n",
              "..          ...           ...       ...              ...         ...   \n",
              "146   24.343243      3.823370  3.833190        31.374702   28.174414   \n",
              "147   27.789547      0.636933  2.437756        32.717197   12.064705   \n",
              "148   20.776632      5.602990  4.166401        33.044666   52.454540   \n",
              "149   33.384750      3.927462  5.439592        41.028702   27.715914   \n",
              "150   31.555012      6.422427  4.379303        42.141502   67.401848   \n",
              "\n",
              "     absorbance      conc  real conc    removal  %removal    \n",
              "0      0.022900  0.789655   0.789655  29.210345   97.270448  \n",
              "1      0.015300  0.527586   0.527586  29.472414   98.143138  \n",
              "2      0.015200  0.524138   0.524138  29.475862   98.154621  \n",
              "3      0.014000  0.482759   0.482759  29.517241   98.292414  \n",
              "4      0.011000  0.379310   0.379310  29.620690   98.636897  \n",
              "..          ...       ...        ...        ...         ...  \n",
              "146    0.253567  2.680391   1.608387  23.266228   85.779030  \n",
              "147    1.026199  4.306577   6.374099  22.542580   77.305450  \n",
              "148   -0.558148  5.100426   0.960695  24.887739   94.021698  \n",
              "149    0.336721  4.286004   3.990602  30.985470  109.060081  \n",
              "150   -1.691420  7.091534   1.887751  29.565063  110.525993  \n",
              "\n",
              "[237 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer"
      ],
      "metadata": {
        "id": "sWZ6R-_3mdAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt = PowerTransformer()\n"
      ],
      "metadata": {
        "id": "0fkMlh7xozWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff=pt.fit_transform(data)"
      ],
      "metadata": {
        "id": "RnMPFGbfo1Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=data"
      ],
      "metadata": {
        "id": "UE5lqSjJpLcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0dUiZ8VqLX0J",
        "outputId": "23acda87-9b47-4aea-af3a-05b2719c9d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9e0c33cf-5e63-4065-8189-3c9bfcc28d8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0229</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>29.210345</td>\n",
              "      <td>97.270448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0153</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>29.472414</td>\n",
              "      <td>98.143138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>29.475862</td>\n",
              "      <td>98.154621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>29.517241</td>\n",
              "      <td>98.292414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>29.620690</td>\n",
              "      <td>98.636897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>15</td>\n",
              "      <td>0.1484</td>\n",
              "      <td>5.117241</td>\n",
              "      <td>5.117241</td>\n",
              "      <td>24.882759</td>\n",
              "      <td>82.859586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>30</td>\n",
              "      <td>0.1252</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>4.317241</td>\n",
              "      <td>25.682759</td>\n",
              "      <td>85.523586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0791</td>\n",
              "      <td>2.727586</td>\n",
              "      <td>2.727586</td>\n",
              "      <td>27.272414</td>\n",
              "      <td>90.817138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0563</td>\n",
              "      <td>1.941379</td>\n",
              "      <td>1.941379</td>\n",
              "      <td>28.058621</td>\n",
              "      <td>93.435207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>0.689655</td>\n",
              "      <td>29.310345</td>\n",
              "      <td>97.603448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e0c33cf-5e63-4065-8189-3c9bfcc28d8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e0c33cf-5e63-4065-8189-3c9bfcc28d8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e0c33cf-5e63-4065-8189-3c9bfcc28d8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)  time  absorbance  \\\n",
              "0           10           1.0         6               30    15      0.0229   \n",
              "1           10           1.0         6               30    30      0.0153   \n",
              "2           10           1.0         6               30    45      0.0152   \n",
              "3           10           1.0         6               30    60      0.0140   \n",
              "4           10           1.0         6               30   120      0.0110   \n",
              "..         ...           ...       ...              ...   ...         ...   \n",
              "86          30           1.0         6               40    15      0.1484   \n",
              "87          30           1.0         6               40    30      0.1252   \n",
              "88          30           1.0         6               40    45      0.0791   \n",
              "89          30           1.0         6               40    60      0.0563   \n",
              "90          30           1.0         6               40   120      0.0200   \n",
              "\n",
              "        conc  real conc    removal  %removal    \n",
              "0   0.789655   0.789655  29.210345   97.270448  \n",
              "1   0.527586   0.527586  29.472414   98.143138  \n",
              "2   0.524138   0.524138  29.475862   98.154621  \n",
              "3   0.482759   0.482759  29.517241   98.292414  \n",
              "4   0.379310   0.379310  29.620690   98.636897  \n",
              "..       ...        ...        ...         ...  \n",
              "86  5.117241   5.117241  24.882759   82.859586  \n",
              "87  4.317241   4.317241  25.682759   85.523586  \n",
              "88  2.727586   2.727586  27.272414   90.817138  \n",
              "89  1.941379   1.941379  28.058621   93.435207  \n",
              "90  0.689655   0.689655  29.310345   97.603448  \n",
              "\n",
              "[91 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols = ['conc (ppm)', 'ad dose(g/L)', 'ph value', 'Temperature(⁰C)', 'time',\n",
        "       'absorbance', 'conc', 'real conc', 'removal', '%removal  ']"
      ],
      "metadata": {
        "id": "HcCW-v6YX1KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data.iloc[:,-1]\n",
        "x=data.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "hOkCc9eF8ct4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split as tts\n",
        "Xtrain,Xtest,ytrain,ytest=tts(x,y,test_size=0.3,random_state=42)"
      ],
      "metadata": {
        "id": "zlxBHv5z8Wx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "xGPiIRChrS5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN():\n",
        "    \n",
        "    def __init__(self, gan_args):\n",
        "        [self.batch_size, lr, self.noise_dim,\n",
        "         self.data_dim, layers_dim] = gan_args\n",
        "\n",
        "        self.generator = Generator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
        "\n",
        "        self.discriminator = Discriminator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
        "\n",
        "        optimizer = Adam(lr, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.noise_dim,))\n",
        "        record = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(record)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def get_data_batch(self, train, batch_size, seed=0):\n",
        "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
        "        # np.random.seed(seed)\n",
        "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
        "        # iterate through shuffled indices, so every sample gets covered evenly\n",
        "\n",
        "        start_i = (batch_size * seed) % len(train)\n",
        "        stop_i = start_i + batch_size\n",
        "        shuffle_seed = (batch_size * seed) // len(train)\n",
        "        np.random.seed(shuffle_seed)\n",
        "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
        "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
        "        x = train.loc[train_ix[start_i: stop_i]].values\n",
        "        return np.reshape(x, (batch_size, -1))\n",
        "        \n",
        "    def train(self, data, train_arguments):\n",
        "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
        "        \n",
        "        data_cols = data.columns\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):    \n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            batch_data = self.get_data_batch(data, self.batch_size)\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "\n",
        "            # Generate a batch of new images\n",
        "            gen_data = self.generator.predict(noise)\n",
        "    \n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    \n",
        "            # If at save interval => save generated events\n",
        "            if epoch % sample_interval == 0:\n",
        "                #Test here data generation step\n",
        "                # save model checkpoints\n",
        "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
        "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
        "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
        "\n",
        "                #Here is generating the data\n",
        "                z = tf.random.normal((432, self.noise_dim))\n",
        "                gen_data = self.generator(z)\n",
        "                print('generated_data')\n",
        "\n",
        "    def save(self, path, name):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        model_path = os.path.join(path, name)\n",
        "        self.generator.save_weights(model_path)  # Load the generator\n",
        "        return\n",
        "    \n",
        "    def load(self, path):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        self.generator = Generator(self.batch_size)\n",
        "        self.generator = self.generator.load_weights(path)\n",
        "        return self.generator\n",
        "    \n",
        "class Generator():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "    def build_model(self, input_shape, dim, data_dim):\n",
        "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim, activation='relu')(input)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dense(dim * 4, activation='relu')(x)\n",
        "        x = Dense(data_dim)(x)\n",
        "        return Model(inputs=input, outputs=x)\n",
        "\n",
        "class Discriminator():\n",
        "    def __init__(self,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "    \n",
        "    def build_model(self, input_shape, dim):\n",
        "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim * 4, activation='relu')(input)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim, activation='relu')(x)\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "        return Model(inputs=input, outputs=x)"
      ],
      "metadata": {
        "id": "yXP_nCPos_L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqoa6BT_8Rf4",
        "outputId": "8bc84023-464e-43b6-c002-ea13ad39b967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noise_dim = 32\n",
        "dim = 128\n",
        "batch_size = 32\n",
        "\n",
        "log_step = 100\n",
        "epochs = 5000+1\n",
        "learning_rate = 5e-4\n",
        "models_dir = 'model'\n",
        "\n",
        "\n",
        "\n",
        "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
        "train_args = ['', epochs, log_step]"
      ],
      "metadata": {
        "id": "H1ecnbtT7KD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model"
      ],
      "metadata": {
        "id": "jMnChK0u-sh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAN\n",
        "\n",
        "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
        "synthesizer = model(gan_args)\n",
        "synthesizer.train(df,train_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbG_az3l85NA",
        "outputId": "e3dad594-c63e-45ef-c036-755873b17def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "51 [D loss: 0.476674, acc.: 57.81%] [G loss: 0.886220]\n",
            "52 [D loss: 0.665309, acc.: 50.00%] [G loss: 0.702742]\n",
            "53 [D loss: 0.492911, acc.: 57.81%] [G loss: 0.801281]\n",
            "54 [D loss: 0.558983, acc.: 57.81%] [G loss: 0.848239]\n",
            "55 [D loss: 0.589544, acc.: 54.69%] [G loss: 0.902564]\n",
            "56 [D loss: 0.550570, acc.: 53.12%] [G loss: 0.854228]\n",
            "57 [D loss: 0.516468, acc.: 60.94%] [G loss: 1.080936]\n",
            "58 [D loss: 0.715344, acc.: 39.06%] [G loss: 0.755540]\n",
            "59 [D loss: 0.515423, acc.: 57.81%] [G loss: 0.916472]\n",
            "60 [D loss: 0.647125, acc.: 43.75%] [G loss: 0.693462]\n",
            "61 [D loss: 0.542810, acc.: 56.25%] [G loss: 0.815626]\n",
            "62 [D loss: 0.528040, acc.: 56.25%] [G loss: 0.966526]\n",
            "63 [D loss: 0.517214, acc.: 64.06%] [G loss: 0.859374]\n",
            "64 [D loss: 0.506752, acc.: 59.38%] [G loss: 0.854001]\n",
            "65 [D loss: 0.494795, acc.: 67.19%] [G loss: 0.859140]\n",
            "66 [D loss: 0.558831, acc.: 56.25%] [G loss: 0.831381]\n",
            "67 [D loss: 0.453522, acc.: 78.12%] [G loss: 0.960249]\n",
            "68 [D loss: 0.607809, acc.: 50.00%] [G loss: 0.924103]\n",
            "69 [D loss: 0.563135, acc.: 64.06%] [G loss: 0.855356]\n",
            "70 [D loss: 0.637466, acc.: 46.88%] [G loss: 0.771946]\n",
            "71 [D loss: 0.562269, acc.: 56.25%] [G loss: 0.824586]\n",
            "72 [D loss: 0.580997, acc.: 50.00%] [G loss: 0.789785]\n",
            "73 [D loss: 0.510452, acc.: 59.38%] [G loss: 0.877135]\n",
            "74 [D loss: 0.443334, acc.: 81.25%] [G loss: 0.992140]\n",
            "75 [D loss: 0.495191, acc.: 75.00%] [G loss: 1.034733]\n",
            "76 [D loss: 0.444233, acc.: 73.44%] [G loss: 0.967696]\n",
            "77 [D loss: 0.482261, acc.: 65.62%] [G loss: 1.068861]\n",
            "78 [D loss: 0.527949, acc.: 62.50%] [G loss: 1.095181]\n",
            "79 [D loss: 0.514623, acc.: 59.38%] [G loss: 0.948021]\n",
            "80 [D loss: 0.559287, acc.: 57.81%] [G loss: 0.915743]\n",
            "81 [D loss: 0.552026, acc.: 51.56%] [G loss: 0.893301]\n",
            "82 [D loss: 0.433953, acc.: 75.00%] [G loss: 1.073024]\n",
            "83 [D loss: 0.429100, acc.: 82.81%] [G loss: 1.128866]\n",
            "84 [D loss: 0.396353, acc.: 92.19%] [G loss: 1.137254]\n",
            "85 [D loss: 0.372237, acc.: 89.06%] [G loss: 1.162549]\n",
            "86 [D loss: 0.410293, acc.: 73.44%] [G loss: 1.060980]\n",
            "87 [D loss: 0.527563, acc.: 62.50%] [G loss: 0.891285]\n",
            "88 [D loss: 0.502326, acc.: 56.25%] [G loss: 0.860676]\n",
            "89 [D loss: 0.525616, acc.: 53.12%] [G loss: 0.890235]\n",
            "90 [D loss: 0.549369, acc.: 56.25%] [G loss: 0.947045]\n",
            "91 [D loss: 0.437939, acc.: 70.31%] [G loss: 1.050805]\n",
            "92 [D loss: 0.368164, acc.: 81.25%] [G loss: 1.149516]\n",
            "93 [D loss: 0.348759, acc.: 93.75%] [G loss: 1.273682]\n",
            "94 [D loss: 0.329290, acc.: 95.31%] [G loss: 1.360923]\n",
            "95 [D loss: 0.317766, acc.: 92.19%] [G loss: 1.277910]\n",
            "96 [D loss: 0.517865, acc.: 62.50%] [G loss: 0.954366]\n",
            "97 [D loss: 0.672797, acc.: 46.88%] [G loss: 0.836869]\n",
            "98 [D loss: 0.820913, acc.: 40.62%] [G loss: 0.686185]\n",
            "99 [D loss: 0.634246, acc.: 46.88%] [G loss: 0.787089]\n",
            "100 [D loss: 0.698979, acc.: 46.88%] [G loss: 0.851138]\n",
            "generated_data\n",
            "101 [D loss: 0.504369, acc.: 59.38%] [G loss: 1.127847]\n",
            "102 [D loss: 0.698281, acc.: 42.19%] [G loss: 0.789129]\n",
            "103 [D loss: 0.479579, acc.: 59.38%] [G loss: 1.227901]\n",
            "104 [D loss: 0.534648, acc.: 75.00%] [G loss: 0.990243]\n",
            "105 [D loss: 0.460623, acc.: 67.19%] [G loss: 0.908165]\n",
            "106 [D loss: 0.426758, acc.: 73.44%] [G loss: 0.986517]\n",
            "107 [D loss: 0.413848, acc.: 84.38%] [G loss: 0.928117]\n",
            "108 [D loss: 0.461005, acc.: 71.88%] [G loss: 0.935014]\n",
            "109 [D loss: 0.322227, acc.: 96.88%] [G loss: 1.053470]\n",
            "110 [D loss: 0.435785, acc.: 78.12%] [G loss: 0.883707]\n",
            "111 [D loss: 0.525091, acc.: 56.25%] [G loss: 0.868801]\n",
            "112 [D loss: 0.537628, acc.: 56.25%] [G loss: 0.811503]\n",
            "113 [D loss: 0.493779, acc.: 54.69%] [G loss: 0.812774]\n",
            "114 [D loss: 0.437125, acc.: 71.88%] [G loss: 0.912743]\n",
            "115 [D loss: 0.511757, acc.: 64.06%] [G loss: 0.901541]\n",
            "116 [D loss: 0.463729, acc.: 73.44%] [G loss: 1.033532]\n",
            "117 [D loss: 0.400478, acc.: 87.50%] [G loss: 1.125802]\n",
            "118 [D loss: 0.591346, acc.: 56.25%] [G loss: 0.974177]\n",
            "119 [D loss: 0.498838, acc.: 65.62%] [G loss: 0.860046]\n",
            "120 [D loss: 0.488397, acc.: 62.50%] [G loss: 0.869117]\n",
            "121 [D loss: 0.506367, acc.: 71.88%] [G loss: 0.809176]\n",
            "122 [D loss: 0.491499, acc.: 65.62%] [G loss: 0.956829]\n",
            "123 [D loss: 0.426594, acc.: 84.38%] [G loss: 1.030882]\n",
            "124 [D loss: 0.425777, acc.: 76.56%] [G loss: 1.155581]\n",
            "125 [D loss: 0.427695, acc.: 78.12%] [G loss: 1.066289]\n",
            "126 [D loss: 0.471558, acc.: 76.56%] [G loss: 1.080459]\n",
            "127 [D loss: 0.557479, acc.: 57.81%] [G loss: 0.902747]\n",
            "128 [D loss: 0.549619, acc.: 59.38%] [G loss: 0.871589]\n",
            "129 [D loss: 0.479784, acc.: 73.44%] [G loss: 0.964277]\n",
            "130 [D loss: 0.441825, acc.: 79.69%] [G loss: 1.074656]\n",
            "131 [D loss: 0.449068, acc.: 79.69%] [G loss: 1.209049]\n",
            "132 [D loss: 0.530867, acc.: 65.62%] [G loss: 1.143701]\n",
            "133 [D loss: 0.394607, acc.: 87.50%] [G loss: 1.377681]\n",
            "134 [D loss: 0.535412, acc.: 70.31%] [G loss: 0.997890]\n",
            "135 [D loss: 0.417785, acc.: 78.12%] [G loss: 1.071456]\n",
            "136 [D loss: 0.570165, acc.: 56.25%] [G loss: 0.802581]\n",
            "137 [D loss: 0.422830, acc.: 78.12%] [G loss: 0.941613]\n",
            "138 [D loss: 0.371840, acc.: 89.06%] [G loss: 1.065588]\n",
            "139 [D loss: 0.475409, acc.: 79.69%] [G loss: 1.011635]\n",
            "140 [D loss: 0.412042, acc.: 84.38%] [G loss: 1.135146]\n",
            "141 [D loss: 0.349850, acc.: 89.06%] [G loss: 1.154766]\n",
            "142 [D loss: 0.393892, acc.: 87.50%] [G loss: 1.133717]\n",
            "143 [D loss: 0.387615, acc.: 89.06%] [G loss: 1.083446]\n",
            "144 [D loss: 0.435502, acc.: 73.44%] [G loss: 1.016512]\n",
            "145 [D loss: 0.496816, acc.: 65.62%] [G loss: 0.989695]\n",
            "146 [D loss: 0.529986, acc.: 71.88%] [G loss: 0.901012]\n",
            "147 [D loss: 0.458175, acc.: 76.56%] [G loss: 1.008701]\n",
            "148 [D loss: 0.502804, acc.: 73.44%] [G loss: 1.031324]\n",
            "149 [D loss: 0.462753, acc.: 76.56%] [G loss: 1.063887]\n",
            "150 [D loss: 0.541524, acc.: 70.31%] [G loss: 0.872762]\n",
            "151 [D loss: 0.545099, acc.: 70.31%] [G loss: 0.828103]\n",
            "152 [D loss: 0.626148, acc.: 62.50%] [G loss: 0.784029]\n",
            "153 [D loss: 0.597900, acc.: 57.81%] [G loss: 0.818508]\n",
            "154 [D loss: 0.580254, acc.: 56.25%] [G loss: 0.816752]\n",
            "155 [D loss: 0.881414, acc.: 51.56%] [G loss: 0.821889]\n",
            "156 [D loss: 0.638938, acc.: 54.69%] [G loss: 0.795646]\n",
            "157 [D loss: 0.552773, acc.: 71.88%] [G loss: 0.796265]\n",
            "158 [D loss: 0.559702, acc.: 65.62%] [G loss: 0.747360]\n",
            "159 [D loss: 0.660829, acc.: 56.25%] [G loss: 0.724046]\n",
            "160 [D loss: 0.716945, acc.: 53.12%] [G loss: 0.800855]\n",
            "161 [D loss: 0.565454, acc.: 67.19%] [G loss: 0.906166]\n",
            "162 [D loss: 0.604432, acc.: 65.62%] [G loss: 0.772776]\n",
            "163 [D loss: 0.724852, acc.: 43.75%] [G loss: 0.779619]\n",
            "164 [D loss: 0.597876, acc.: 53.12%] [G loss: 0.805811]\n",
            "165 [D loss: 0.609811, acc.: 62.50%] [G loss: 0.896237]\n",
            "166 [D loss: 0.774122, acc.: 42.19%] [G loss: 0.741113]\n",
            "167 [D loss: 0.678906, acc.: 53.12%] [G loss: 0.682047]\n",
            "168 [D loss: 0.737819, acc.: 34.38%] [G loss: 0.717082]\n",
            "169 [D loss: 0.726177, acc.: 37.50%] [G loss: 0.743111]\n",
            "170 [D loss: 0.732421, acc.: 37.50%] [G loss: 0.761305]\n",
            "171 [D loss: 0.663492, acc.: 53.12%] [G loss: 0.953215]\n",
            "172 [D loss: 0.743964, acc.: 39.06%] [G loss: 0.758444]\n",
            "173 [D loss: 0.811203, acc.: 42.19%] [G loss: 0.811195]\n",
            "174 [D loss: 0.645445, acc.: 53.12%] [G loss: 0.764772]\n",
            "175 [D loss: 0.768267, acc.: 42.19%] [G loss: 0.956901]\n",
            "176 [D loss: 0.661780, acc.: 56.25%] [G loss: 0.844961]\n",
            "177 [D loss: 0.910744, acc.: 43.75%] [G loss: 0.823650]\n",
            "178 [D loss: 0.694907, acc.: 56.25%] [G loss: 1.122087]\n",
            "179 [D loss: 0.689975, acc.: 53.12%] [G loss: 0.987502]\n",
            "180 [D loss: 0.749372, acc.: 43.75%] [G loss: 0.828584]\n",
            "181 [D loss: 0.649537, acc.: 59.38%] [G loss: 0.812195]\n",
            "182 [D loss: 0.736142, acc.: 46.88%] [G loss: 0.815820]\n",
            "183 [D loss: 0.703437, acc.: 50.00%] [G loss: 0.851480]\n",
            "184 [D loss: 0.670045, acc.: 51.56%] [G loss: 0.877104]\n",
            "185 [D loss: 0.700894, acc.: 51.56%] [G loss: 0.851643]\n",
            "186 [D loss: 0.685094, acc.: 50.00%] [G loss: 0.773525]\n",
            "187 [D loss: 0.665042, acc.: 53.12%] [G loss: 0.922907]\n",
            "188 [D loss: 0.753857, acc.: 40.62%] [G loss: 0.809173]\n",
            "189 [D loss: 0.714670, acc.: 45.31%] [G loss: 0.868369]\n",
            "190 [D loss: 0.656686, acc.: 57.81%] [G loss: 0.931335]\n",
            "191 [D loss: 0.638413, acc.: 62.50%] [G loss: 0.903064]\n",
            "192 [D loss: 0.650809, acc.: 51.56%] [G loss: 0.982942]\n",
            "193 [D loss: 0.669239, acc.: 54.69%] [G loss: 0.820037]\n",
            "194 [D loss: 0.789434, acc.: 51.56%] [G loss: 1.035190]\n",
            "195 [D loss: 0.612006, acc.: 64.06%] [G loss: 1.061490]\n",
            "196 [D loss: 0.699179, acc.: 56.25%] [G loss: 0.942594]\n",
            "197 [D loss: 0.677517, acc.: 50.00%] [G loss: 0.839042]\n",
            "198 [D loss: 0.665736, acc.: 57.81%] [G loss: 0.874653]\n",
            "199 [D loss: 0.666323, acc.: 53.12%] [G loss: 0.841794]\n",
            "200 [D loss: 0.716088, acc.: 46.88%] [G loss: 0.945612]\n",
            "generated_data\n",
            "201 [D loss: 0.690781, acc.: 50.00%] [G loss: 0.883485]\n",
            "202 [D loss: 0.683427, acc.: 48.44%] [G loss: 0.956211]\n",
            "203 [D loss: 0.689823, acc.: 43.75%] [G loss: 0.866401]\n",
            "204 [D loss: 0.699450, acc.: 59.38%] [G loss: 1.045561]\n",
            "205 [D loss: 0.725565, acc.: 51.56%] [G loss: 0.909190]\n",
            "206 [D loss: 0.726037, acc.: 60.94%] [G loss: 1.021311]\n",
            "207 [D loss: 0.669854, acc.: 59.38%] [G loss: 0.912658]\n",
            "208 [D loss: 0.682415, acc.: 53.12%] [G loss: 0.888060]\n",
            "209 [D loss: 0.631274, acc.: 67.19%] [G loss: 0.991504]\n",
            "210 [D loss: 0.652190, acc.: 59.38%] [G loss: 0.890367]\n",
            "211 [D loss: 0.687215, acc.: 53.12%] [G loss: 0.888695]\n",
            "212 [D loss: 0.678393, acc.: 53.12%] [G loss: 0.834260]\n",
            "213 [D loss: 0.664326, acc.: 54.69%] [G loss: 0.901663]\n",
            "214 [D loss: 0.665518, acc.: 56.25%] [G loss: 0.816133]\n",
            "215 [D loss: 0.724439, acc.: 48.44%] [G loss: 0.928360]\n",
            "216 [D loss: 0.663657, acc.: 50.00%] [G loss: 0.978005]\n",
            "217 [D loss: 0.699638, acc.: 51.56%] [G loss: 0.938140]\n",
            "218 [D loss: 0.745371, acc.: 37.50%] [G loss: 0.954747]\n",
            "219 [D loss: 0.665858, acc.: 64.06%] [G loss: 0.967340]\n",
            "220 [D loss: 0.676965, acc.: 56.25%] [G loss: 0.875564]\n",
            "221 [D loss: 0.662898, acc.: 57.81%] [G loss: 0.875032]\n",
            "222 [D loss: 0.637451, acc.: 56.25%] [G loss: 0.855015]\n",
            "223 [D loss: 0.655551, acc.: 62.50%] [G loss: 0.875116]\n",
            "224 [D loss: 0.686296, acc.: 50.00%] [G loss: 1.139339]\n",
            "225 [D loss: 0.698682, acc.: 53.12%] [G loss: 1.112134]\n",
            "226 [D loss: 0.671281, acc.: 59.38%] [G loss: 0.962172]\n",
            "227 [D loss: 0.656604, acc.: 65.62%] [G loss: 0.876867]\n",
            "228 [D loss: 0.658919, acc.: 62.50%] [G loss: 0.847546]\n",
            "229 [D loss: 0.753715, acc.: 37.50%] [G loss: 0.783491]\n",
            "230 [D loss: 0.652258, acc.: 53.12%] [G loss: 0.751551]\n",
            "231 [D loss: 0.703596, acc.: 51.56%] [G loss: 0.830626]\n",
            "232 [D loss: 0.664174, acc.: 54.69%] [G loss: 0.958853]\n",
            "233 [D loss: 0.630881, acc.: 57.81%] [G loss: 0.873390]\n",
            "234 [D loss: 0.691855, acc.: 56.25%] [G loss: 0.922579]\n",
            "235 [D loss: 0.717460, acc.: 43.75%] [G loss: 1.055156]\n",
            "236 [D loss: 0.742275, acc.: 46.88%] [G loss: 0.926490]\n",
            "237 [D loss: 0.686246, acc.: 48.44%] [G loss: 0.979666]\n",
            "238 [D loss: 0.725996, acc.: 39.06%] [G loss: 0.880940]\n",
            "239 [D loss: 0.686168, acc.: 45.31%] [G loss: 0.929580]\n",
            "240 [D loss: 0.690901, acc.: 59.38%] [G loss: 0.966516]\n",
            "241 [D loss: 0.660830, acc.: 60.94%] [G loss: 0.926577]\n",
            "242 [D loss: 0.685045, acc.: 59.38%] [G loss: 0.969641]\n",
            "243 [D loss: 0.663131, acc.: 56.25%] [G loss: 0.982194]\n",
            "244 [D loss: 0.615153, acc.: 64.06%] [G loss: 1.001040]\n",
            "245 [D loss: 0.688555, acc.: 59.38%] [G loss: 1.100672]\n",
            "246 [D loss: 0.624994, acc.: 62.50%] [G loss: 1.035763]\n",
            "247 [D loss: 0.737593, acc.: 53.12%] [G loss: 1.001055]\n",
            "248 [D loss: 0.901767, acc.: 43.75%] [G loss: 1.081837]\n",
            "249 [D loss: 0.688568, acc.: 53.12%] [G loss: 1.116975]\n",
            "250 [D loss: 0.682870, acc.: 56.25%] [G loss: 1.178837]\n",
            "251 [D loss: 0.684104, acc.: 53.12%] [G loss: 1.078167]\n",
            "252 [D loss: 0.652056, acc.: 57.81%] [G loss: 0.989638]\n",
            "253 [D loss: 0.646943, acc.: 56.25%] [G loss: 0.980694]\n",
            "254 [D loss: 0.647485, acc.: 54.69%] [G loss: 0.973733]\n",
            "255 [D loss: 0.656805, acc.: 53.12%] [G loss: 0.882209]\n",
            "256 [D loss: 0.721389, acc.: 48.44%] [G loss: 0.884017]\n",
            "257 [D loss: 0.642807, acc.: 57.81%] [G loss: 0.926273]\n",
            "258 [D loss: 0.664025, acc.: 57.81%] [G loss: 0.909570]\n",
            "259 [D loss: 0.688666, acc.: 51.56%] [G loss: 0.838769]\n",
            "260 [D loss: 0.651218, acc.: 56.25%] [G loss: 0.861065]\n",
            "261 [D loss: 0.709132, acc.: 48.44%] [G loss: 0.913536]\n",
            "262 [D loss: 0.685215, acc.: 53.12%] [G loss: 0.905300]\n",
            "263 [D loss: 0.676206, acc.: 53.12%] [G loss: 0.904723]\n",
            "264 [D loss: 0.685148, acc.: 50.00%] [G loss: 0.932577]\n",
            "265 [D loss: 0.659617, acc.: 51.56%] [G loss: 0.849166]\n",
            "266 [D loss: 0.686451, acc.: 48.44%] [G loss: 0.920318]\n",
            "267 [D loss: 0.650634, acc.: 53.12%] [G loss: 0.892792]\n",
            "268 [D loss: 0.719957, acc.: 42.19%] [G loss: 0.960614]\n",
            "269 [D loss: 0.625788, acc.: 54.69%] [G loss: 0.896730]\n",
            "270 [D loss: 0.671710, acc.: 54.69%] [G loss: 0.949930]\n",
            "271 [D loss: 0.652605, acc.: 54.69%] [G loss: 0.941979]\n",
            "272 [D loss: 0.676858, acc.: 51.56%] [G loss: 0.996161]\n",
            "273 [D loss: 0.687075, acc.: 51.56%] [G loss: 0.993876]\n",
            "274 [D loss: 0.694375, acc.: 54.69%] [G loss: 0.949724]\n",
            "275 [D loss: 0.680156, acc.: 56.25%] [G loss: 1.031254]\n",
            "276 [D loss: 0.655510, acc.: 57.81%] [G loss: 0.936502]\n",
            "277 [D loss: 0.660946, acc.: 56.25%] [G loss: 0.959948]\n",
            "278 [D loss: 0.647960, acc.: 60.94%] [G loss: 0.909125]\n",
            "279 [D loss: 0.679000, acc.: 51.56%] [G loss: 0.929603]\n",
            "280 [D loss: 0.650198, acc.: 51.56%] [G loss: 0.975583]\n",
            "281 [D loss: 0.672282, acc.: 53.12%] [G loss: 0.948822]\n",
            "282 [D loss: 0.668211, acc.: 51.56%] [G loss: 0.896631]\n",
            "283 [D loss: 0.649301, acc.: 59.38%] [G loss: 0.916075]\n",
            "284 [D loss: 0.663328, acc.: 48.44%] [G loss: 0.969305]\n",
            "285 [D loss: 0.686755, acc.: 48.44%] [G loss: 0.917112]\n",
            "286 [D loss: 0.705582, acc.: 43.75%] [G loss: 0.969153]\n",
            "287 [D loss: 0.680232, acc.: 53.12%] [G loss: 0.896798]\n",
            "288 [D loss: 0.657375, acc.: 57.81%] [G loss: 0.920121]\n",
            "289 [D loss: 0.658308, acc.: 56.25%] [G loss: 0.883349]\n",
            "290 [D loss: 0.695325, acc.: 50.00%] [G loss: 0.904773]\n",
            "291 [D loss: 0.652368, acc.: 56.25%] [G loss: 0.877486]\n",
            "292 [D loss: 0.657362, acc.: 57.81%] [G loss: 0.871047]\n",
            "293 [D loss: 0.637112, acc.: 59.38%] [G loss: 0.937620]\n",
            "294 [D loss: 0.668245, acc.: 51.56%] [G loss: 0.914162]\n",
            "295 [D loss: 0.648899, acc.: 53.12%] [G loss: 0.940822]\n",
            "296 [D loss: 0.657286, acc.: 51.56%] [G loss: 0.951505]\n",
            "297 [D loss: 0.646601, acc.: 59.38%] [G loss: 1.039536]\n",
            "298 [D loss: 0.667769, acc.: 60.94%] [G loss: 0.931842]\n",
            "299 [D loss: 0.678217, acc.: 51.56%] [G loss: 0.896558]\n",
            "300 [D loss: 0.632383, acc.: 60.94%] [G loss: 0.919585]\n",
            "generated_data\n",
            "301 [D loss: 0.684637, acc.: 56.25%] [G loss: 0.868315]\n",
            "302 [D loss: 0.704899, acc.: 51.56%] [G loss: 0.975883]\n",
            "303 [D loss: 0.619171, acc.: 64.06%] [G loss: 0.932816]\n",
            "304 [D loss: 0.682549, acc.: 53.12%] [G loss: 0.912877]\n",
            "305 [D loss: 0.660297, acc.: 54.69%] [G loss: 0.911910]\n",
            "306 [D loss: 0.675708, acc.: 53.12%] [G loss: 0.863766]\n",
            "307 [D loss: 0.668098, acc.: 54.69%] [G loss: 0.874074]\n",
            "308 [D loss: 0.690091, acc.: 48.44%] [G loss: 0.933091]\n",
            "309 [D loss: 0.693614, acc.: 46.88%] [G loss: 0.868922]\n",
            "310 [D loss: 0.692492, acc.: 48.44%] [G loss: 0.944403]\n",
            "311 [D loss: 0.686493, acc.: 50.00%] [G loss: 0.932486]\n",
            "312 [D loss: 0.638027, acc.: 53.12%] [G loss: 0.950690]\n",
            "313 [D loss: 0.661272, acc.: 54.69%] [G loss: 0.987810]\n",
            "314 [D loss: 0.656935, acc.: 51.56%] [G loss: 1.093867]\n",
            "315 [D loss: 0.725190, acc.: 48.44%] [G loss: 0.938555]\n",
            "316 [D loss: 0.681594, acc.: 53.12%] [G loss: 0.882369]\n",
            "317 [D loss: 0.658249, acc.: 53.12%] [G loss: 0.948850]\n",
            "318 [D loss: 0.661920, acc.: 56.25%] [G loss: 0.944697]\n",
            "319 [D loss: 0.676461, acc.: 54.69%] [G loss: 0.986219]\n",
            "320 [D loss: 0.659019, acc.: 53.12%] [G loss: 1.183360]\n",
            "321 [D loss: 0.631205, acc.: 59.38%] [G loss: 1.113581]\n",
            "322 [D loss: 0.679899, acc.: 56.25%] [G loss: 0.924940]\n",
            "323 [D loss: 0.663276, acc.: 64.06%] [G loss: 0.882843]\n",
            "324 [D loss: 0.650124, acc.: 65.62%] [G loss: 0.896932]\n",
            "325 [D loss: 0.660266, acc.: 51.56%] [G loss: 0.881086]\n",
            "326 [D loss: 0.647155, acc.: 57.81%] [G loss: 0.972181]\n",
            "327 [D loss: 0.650331, acc.: 56.25%] [G loss: 0.986228]\n",
            "328 [D loss: 0.704068, acc.: 53.12%] [G loss: 0.894090]\n",
            "329 [D loss: 0.642135, acc.: 50.00%] [G loss: 0.953595]\n",
            "330 [D loss: 0.660905, acc.: 59.38%] [G loss: 0.959442]\n",
            "331 [D loss: 0.681453, acc.: 50.00%] [G loss: 0.941932]\n",
            "332 [D loss: 0.668019, acc.: 60.94%] [G loss: 0.931197]\n",
            "333 [D loss: 0.660613, acc.: 57.81%] [G loss: 0.926750]\n",
            "334 [D loss: 0.661937, acc.: 54.69%] [G loss: 0.915027]\n",
            "335 [D loss: 0.614951, acc.: 62.50%] [G loss: 0.970657]\n",
            "336 [D loss: 0.661349, acc.: 59.38%] [G loss: 0.925443]\n",
            "337 [D loss: 0.671164, acc.: 59.38%] [G loss: 0.909500]\n",
            "338 [D loss: 0.639930, acc.: 59.38%] [G loss: 0.852016]\n",
            "339 [D loss: 0.618652, acc.: 64.06%] [G loss: 0.822679]\n",
            "340 [D loss: 0.677565, acc.: 51.56%] [G loss: 0.877105]\n",
            "341 [D loss: 0.657569, acc.: 56.25%] [G loss: 0.916902]\n",
            "342 [D loss: 0.657684, acc.: 57.81%] [G loss: 0.893221]\n",
            "343 [D loss: 0.626909, acc.: 64.06%] [G loss: 0.979352]\n",
            "344 [D loss: 0.639224, acc.: 59.38%] [G loss: 0.978412]\n",
            "345 [D loss: 0.650231, acc.: 54.69%] [G loss: 0.968999]\n",
            "346 [D loss: 0.648463, acc.: 64.06%] [G loss: 0.961908]\n",
            "347 [D loss: 0.610632, acc.: 65.62%] [G loss: 0.934548]\n",
            "348 [D loss: 0.657425, acc.: 53.12%] [G loss: 0.933617]\n",
            "349 [D loss: 0.651413, acc.: 54.69%] [G loss: 0.920407]\n",
            "350 [D loss: 0.671627, acc.: 51.56%] [G loss: 0.898282]\n",
            "351 [D loss: 0.661051, acc.: 62.50%] [G loss: 0.873586]\n",
            "352 [D loss: 0.625016, acc.: 62.50%] [G loss: 0.855777]\n",
            "353 [D loss: 0.636370, acc.: 59.38%] [G loss: 0.959763]\n",
            "354 [D loss: 0.638212, acc.: 62.50%] [G loss: 0.896552]\n",
            "355 [D loss: 0.621566, acc.: 57.81%] [G loss: 0.969956]\n",
            "356 [D loss: 0.639965, acc.: 59.38%] [G loss: 0.912031]\n",
            "357 [D loss: 0.670517, acc.: 54.69%] [G loss: 0.930333]\n",
            "358 [D loss: 0.644731, acc.: 65.62%] [G loss: 0.833922]\n",
            "359 [D loss: 0.618068, acc.: 62.50%] [G loss: 0.821572]\n",
            "360 [D loss: 0.639206, acc.: 54.69%] [G loss: 0.902994]\n",
            "361 [D loss: 0.640405, acc.: 59.38%] [G loss: 0.917400]\n",
            "362 [D loss: 0.603813, acc.: 67.19%] [G loss: 0.902353]\n",
            "363 [D loss: 0.660289, acc.: 57.81%] [G loss: 0.929617]\n",
            "364 [D loss: 0.623206, acc.: 65.62%] [G loss: 0.938379]\n",
            "365 [D loss: 0.653840, acc.: 53.12%] [G loss: 0.888831]\n",
            "366 [D loss: 0.616021, acc.: 57.81%] [G loss: 0.929565]\n",
            "367 [D loss: 0.647212, acc.: 64.06%] [G loss: 0.866856]\n",
            "368 [D loss: 0.617144, acc.: 64.06%] [G loss: 0.875829]\n",
            "369 [D loss: 0.639541, acc.: 57.81%] [G loss: 0.930227]\n",
            "370 [D loss: 0.628751, acc.: 59.38%] [G loss: 0.967778]\n",
            "371 [D loss: 0.583697, acc.: 60.94%] [G loss: 0.952208]\n",
            "372 [D loss: 0.631204, acc.: 53.12%] [G loss: 0.964780]\n",
            "373 [D loss: 0.597354, acc.: 67.19%] [G loss: 0.953576]\n",
            "374 [D loss: 0.604561, acc.: 59.38%] [G loss: 0.920217]\n",
            "375 [D loss: 0.607544, acc.: 65.62%] [G loss: 0.901584]\n",
            "376 [D loss: 0.649172, acc.: 59.38%] [G loss: 0.871774]\n",
            "377 [D loss: 0.687026, acc.: 56.25%] [G loss: 1.012856]\n",
            "378 [D loss: 0.615107, acc.: 65.62%] [G loss: 0.928484]\n",
            "379 [D loss: 0.627629, acc.: 62.50%] [G loss: 0.979865]\n",
            "380 [D loss: 0.625401, acc.: 57.81%] [G loss: 0.940691]\n",
            "381 [D loss: 0.632976, acc.: 57.81%] [G loss: 0.948655]\n",
            "382 [D loss: 0.695282, acc.: 57.81%] [G loss: 0.993869]\n",
            "383 [D loss: 0.608201, acc.: 67.19%] [G loss: 0.919959]\n",
            "384 [D loss: 0.664148, acc.: 59.38%] [G loss: 0.934304]\n",
            "385 [D loss: 0.640821, acc.: 60.94%] [G loss: 0.940504]\n",
            "386 [D loss: 0.601238, acc.: 65.62%] [G loss: 0.948766]\n",
            "387 [D loss: 0.623693, acc.: 60.94%] [G loss: 0.999581]\n",
            "388 [D loss: 0.629390, acc.: 64.06%] [G loss: 1.003194]\n",
            "389 [D loss: 0.642126, acc.: 64.06%] [G loss: 0.958233]\n",
            "390 [D loss: 0.602473, acc.: 64.06%] [G loss: 1.016659]\n",
            "391 [D loss: 0.645835, acc.: 62.50%] [G loss: 1.012287]\n",
            "392 [D loss: 0.602737, acc.: 60.94%] [G loss: 1.021939]\n",
            "393 [D loss: 0.594336, acc.: 68.75%] [G loss: 1.032560]\n",
            "394 [D loss: 0.570025, acc.: 71.88%] [G loss: 1.001063]\n",
            "395 [D loss: 0.657658, acc.: 59.38%] [G loss: 1.027419]\n",
            "396 [D loss: 0.585902, acc.: 65.62%] [G loss: 0.987311]\n",
            "397 [D loss: 0.731502, acc.: 56.25%] [G loss: 1.052040]\n",
            "398 [D loss: 0.630425, acc.: 68.75%] [G loss: 0.989846]\n",
            "399 [D loss: 0.659170, acc.: 54.69%] [G loss: 0.976670]\n",
            "400 [D loss: 0.616941, acc.: 65.62%] [G loss: 1.020666]\n",
            "generated_data\n",
            "401 [D loss: 0.624412, acc.: 62.50%] [G loss: 0.972069]\n",
            "402 [D loss: 0.594828, acc.: 68.75%] [G loss: 0.931964]\n",
            "403 [D loss: 0.617774, acc.: 64.06%] [G loss: 0.910214]\n",
            "404 [D loss: 0.655498, acc.: 54.69%] [G loss: 0.991074]\n",
            "405 [D loss: 0.610630, acc.: 64.06%] [G loss: 0.930640]\n",
            "406 [D loss: 0.601592, acc.: 65.62%] [G loss: 0.985356]\n",
            "407 [D loss: 0.610580, acc.: 65.62%] [G loss: 0.890534]\n",
            "408 [D loss: 0.585954, acc.: 67.19%] [G loss: 0.983551]\n",
            "409 [D loss: 0.603532, acc.: 65.62%] [G loss: 0.899546]\n",
            "410 [D loss: 0.598356, acc.: 67.19%] [G loss: 0.932797]\n",
            "411 [D loss: 0.664069, acc.: 50.00%] [G loss: 0.964053]\n",
            "412 [D loss: 0.584831, acc.: 67.19%] [G loss: 1.069732]\n",
            "413 [D loss: 0.601344, acc.: 56.25%] [G loss: 0.976307]\n",
            "414 [D loss: 0.623070, acc.: 56.25%] [G loss: 0.883608]\n",
            "415 [D loss: 0.610922, acc.: 64.06%] [G loss: 0.868394]\n",
            "416 [D loss: 0.550751, acc.: 70.31%] [G loss: 0.904906]\n",
            "417 [D loss: 0.583419, acc.: 68.75%] [G loss: 0.919647]\n",
            "418 [D loss: 0.591039, acc.: 59.38%] [G loss: 0.935739]\n",
            "419 [D loss: 0.564142, acc.: 62.50%] [G loss: 1.004957]\n",
            "420 [D loss: 0.644905, acc.: 53.12%] [G loss: 1.000923]\n",
            "421 [D loss: 0.585148, acc.: 67.19%] [G loss: 0.925991]\n",
            "422 [D loss: 0.610227, acc.: 68.75%] [G loss: 0.833196]\n",
            "423 [D loss: 0.505319, acc.: 78.12%] [G loss: 0.879097]\n",
            "424 [D loss: 0.553161, acc.: 57.81%] [G loss: 0.906880]\n",
            "425 [D loss: 0.575020, acc.: 73.44%] [G loss: 0.981175]\n",
            "426 [D loss: 0.545989, acc.: 73.44%] [G loss: 1.004297]\n",
            "427 [D loss: 0.549418, acc.: 71.88%] [G loss: 1.045535]\n",
            "428 [D loss: 0.551628, acc.: 70.31%] [G loss: 1.016715]\n",
            "429 [D loss: 0.520605, acc.: 78.12%] [G loss: 1.093657]\n",
            "430 [D loss: 0.493920, acc.: 82.81%] [G loss: 1.048904]\n",
            "431 [D loss: 0.622182, acc.: 62.50%] [G loss: 1.366728]\n",
            "432 [D loss: 0.533986, acc.: 81.25%] [G loss: 1.162447]\n",
            "433 [D loss: 0.558216, acc.: 73.44%] [G loss: 0.922633]\n",
            "434 [D loss: 0.570646, acc.: 60.94%] [G loss: 0.998800]\n",
            "435 [D loss: 0.490146, acc.: 81.25%] [G loss: 1.047953]\n",
            "436 [D loss: 0.788525, acc.: 51.56%] [G loss: 1.024447]\n",
            "437 [D loss: 0.614733, acc.: 64.06%] [G loss: 1.004952]\n",
            "438 [D loss: 0.624988, acc.: 62.50%] [G loss: 0.938220]\n",
            "439 [D loss: 0.584157, acc.: 67.19%] [G loss: 0.978075]\n",
            "440 [D loss: 0.634995, acc.: 51.56%] [G loss: 0.949787]\n",
            "441 [D loss: 0.622624, acc.: 57.81%] [G loss: 0.922581]\n",
            "442 [D loss: 0.581991, acc.: 67.19%] [G loss: 0.794837]\n",
            "443 [D loss: 0.662523, acc.: 60.94%] [G loss: 0.806691]\n",
            "444 [D loss: 0.666416, acc.: 43.75%] [G loss: 0.941308]\n",
            "445 [D loss: 0.648575, acc.: 54.69%] [G loss: 0.935150]\n",
            "446 [D loss: 0.555413, acc.: 73.44%] [G loss: 0.986017]\n",
            "447 [D loss: 0.548555, acc.: 73.44%] [G loss: 1.029073]\n",
            "448 [D loss: 0.518511, acc.: 76.56%] [G loss: 0.983229]\n",
            "449 [D loss: 0.570963, acc.: 68.75%] [G loss: 1.013565]\n",
            "450 [D loss: 0.589376, acc.: 65.62%] [G loss: 0.977682]\n",
            "451 [D loss: 0.608217, acc.: 62.50%] [G loss: 0.918349]\n",
            "452 [D loss: 0.749780, acc.: 43.75%] [G loss: 0.930172]\n",
            "453 [D loss: 0.662958, acc.: 53.12%] [G loss: 0.931595]\n",
            "454 [D loss: 0.584866, acc.: 64.06%] [G loss: 1.039516]\n",
            "455 [D loss: 0.538450, acc.: 75.00%] [G loss: 1.004426]\n",
            "456 [D loss: 0.515991, acc.: 81.25%] [G loss: 1.002862]\n",
            "457 [D loss: 0.530554, acc.: 71.88%] [G loss: 0.851726]\n",
            "458 [D loss: 0.681485, acc.: 56.25%] [G loss: 1.010806]\n",
            "459 [D loss: 0.559468, acc.: 71.88%] [G loss: 1.106610]\n",
            "460 [D loss: 0.650331, acc.: 57.81%] [G loss: 1.006604]\n",
            "461 [D loss: 0.625141, acc.: 59.38%] [G loss: 0.915692]\n",
            "462 [D loss: 0.591276, acc.: 67.19%] [G loss: 0.887671]\n",
            "463 [D loss: 0.608299, acc.: 56.25%] [G loss: 0.926560]\n",
            "464 [D loss: 0.578074, acc.: 68.75%] [G loss: 0.915665]\n",
            "465 [D loss: 0.558520, acc.: 71.88%] [G loss: 0.917456]\n",
            "466 [D loss: 0.562374, acc.: 71.88%] [G loss: 0.912977]\n",
            "467 [D loss: 0.571571, acc.: 70.31%] [G loss: 0.901584]\n",
            "468 [D loss: 0.609428, acc.: 64.06%] [G loss: 0.926409]\n",
            "469 [D loss: 0.570409, acc.: 68.75%] [G loss: 0.929398]\n",
            "470 [D loss: 0.609444, acc.: 60.94%] [G loss: 0.952351]\n",
            "471 [D loss: 0.594205, acc.: 67.19%] [G loss: 0.910493]\n",
            "472 [D loss: 0.578289, acc.: 70.31%] [G loss: 0.962779]\n",
            "473 [D loss: 0.561427, acc.: 65.62%] [G loss: 0.992792]\n",
            "474 [D loss: 0.573806, acc.: 75.00%] [G loss: 0.931668]\n",
            "475 [D loss: 0.575317, acc.: 64.06%] [G loss: 0.934186]\n",
            "476 [D loss: 0.540458, acc.: 73.44%] [G loss: 0.926382]\n",
            "477 [D loss: 0.549168, acc.: 71.88%] [G loss: 1.024697]\n",
            "478 [D loss: 0.609955, acc.: 57.81%] [G loss: 0.843649]\n",
            "479 [D loss: 0.566523, acc.: 68.75%] [G loss: 0.915202]\n",
            "480 [D loss: 0.655302, acc.: 54.69%] [G loss: 0.836888]\n",
            "481 [D loss: 0.644970, acc.: 53.12%] [G loss: 0.988066]\n",
            "482 [D loss: 0.607597, acc.: 67.19%] [G loss: 1.017363]\n",
            "483 [D loss: 0.566369, acc.: 73.44%] [G loss: 0.885591]\n",
            "484 [D loss: 0.523992, acc.: 78.12%] [G loss: 1.019998]\n",
            "485 [D loss: 0.526981, acc.: 76.56%] [G loss: 0.966904]\n",
            "486 [D loss: 0.527401, acc.: 79.69%] [G loss: 0.992452]\n",
            "487 [D loss: 0.678232, acc.: 51.56%] [G loss: 0.878204]\n",
            "488 [D loss: 0.591182, acc.: 60.94%] [G loss: 0.922754]\n",
            "489 [D loss: 0.619913, acc.: 59.38%] [G loss: 0.952293]\n",
            "490 [D loss: 0.655405, acc.: 56.25%] [G loss: 1.021729]\n",
            "491 [D loss: 0.558340, acc.: 67.19%] [G loss: 1.344007]\n",
            "492 [D loss: 0.554805, acc.: 70.31%] [G loss: 1.113401]\n",
            "493 [D loss: 0.636475, acc.: 59.38%] [G loss: 0.890334]\n",
            "494 [D loss: 0.576160, acc.: 62.50%] [G loss: 0.871843]\n",
            "495 [D loss: 0.571499, acc.: 71.88%] [G loss: 0.930063]\n",
            "496 [D loss: 0.595549, acc.: 53.12%] [G loss: 0.895075]\n",
            "497 [D loss: 0.584897, acc.: 62.50%] [G loss: 0.974621]\n",
            "498 [D loss: 0.567404, acc.: 70.31%] [G loss: 1.045504]\n",
            "499 [D loss: 0.625698, acc.: 60.94%] [G loss: 1.080350]\n",
            "500 [D loss: 0.578767, acc.: 62.50%] [G loss: 1.117918]\n",
            "generated_data\n",
            "501 [D loss: 0.536770, acc.: 75.00%] [G loss: 1.088565]\n",
            "502 [D loss: 0.564277, acc.: 71.88%] [G loss: 0.973738]\n",
            "503 [D loss: 0.607393, acc.: 75.00%] [G loss: 0.909162]\n",
            "504 [D loss: 0.643765, acc.: 65.62%] [G loss: 0.877196]\n",
            "505 [D loss: 0.610909, acc.: 64.06%] [G loss: 0.974837]\n",
            "506 [D loss: 0.566610, acc.: 73.44%] [G loss: 0.923658]\n",
            "507 [D loss: 0.604687, acc.: 64.06%] [G loss: 0.926623]\n",
            "508 [D loss: 0.591087, acc.: 67.19%] [G loss: 0.888384]\n",
            "509 [D loss: 0.622629, acc.: 60.94%] [G loss: 0.902674]\n",
            "510 [D loss: 0.583163, acc.: 67.19%] [G loss: 0.935901]\n",
            "511 [D loss: 0.590868, acc.: 64.06%] [G loss: 0.960196]\n",
            "512 [D loss: 0.569083, acc.: 76.56%] [G loss: 0.905172]\n",
            "513 [D loss: 0.572089, acc.: 65.62%] [G loss: 1.014151]\n",
            "514 [D loss: 0.561680, acc.: 68.75%] [G loss: 0.943482]\n",
            "515 [D loss: 0.584349, acc.: 62.50%] [G loss: 1.042269]\n",
            "516 [D loss: 0.566011, acc.: 73.44%] [G loss: 0.873353]\n",
            "517 [D loss: 0.593674, acc.: 64.06%] [G loss: 0.870709]\n",
            "518 [D loss: 0.561885, acc.: 67.19%] [G loss: 0.941147]\n",
            "519 [D loss: 0.534909, acc.: 70.31%] [G loss: 0.881127]\n",
            "520 [D loss: 0.509209, acc.: 82.81%] [G loss: 0.944893]\n",
            "521 [D loss: 0.521398, acc.: 79.69%] [G loss: 0.968047]\n",
            "522 [D loss: 0.507679, acc.: 78.12%] [G loss: 0.925182]\n",
            "523 [D loss: 0.532505, acc.: 73.44%] [G loss: 1.008094]\n",
            "524 [D loss: 0.526441, acc.: 76.56%] [G loss: 1.030045]\n",
            "525 [D loss: 0.583722, acc.: 68.75%] [G loss: 1.045489]\n",
            "526 [D loss: 0.573836, acc.: 68.75%] [G loss: 0.992577]\n",
            "527 [D loss: 0.552015, acc.: 67.19%] [G loss: 0.994781]\n",
            "528 [D loss: 0.534708, acc.: 76.56%] [G loss: 1.049139]\n",
            "529 [D loss: 0.697849, acc.: 43.75%] [G loss: 1.057362]\n",
            "530 [D loss: 0.569699, acc.: 67.19%] [G loss: 0.999726]\n",
            "531 [D loss: 0.566649, acc.: 68.75%] [G loss: 1.155545]\n",
            "532 [D loss: 0.622896, acc.: 64.06%] [G loss: 1.264130]\n",
            "533 [D loss: 0.552630, acc.: 70.31%] [G loss: 1.102071]\n",
            "534 [D loss: 0.577263, acc.: 64.06%] [G loss: 1.079921]\n",
            "535 [D loss: 0.576142, acc.: 67.19%] [G loss: 1.069752]\n",
            "536 [D loss: 0.606899, acc.: 60.94%] [G loss: 1.050605]\n",
            "537 [D loss: 0.626829, acc.: 68.75%] [G loss: 1.067525]\n",
            "538 [D loss: 0.620777, acc.: 60.94%] [G loss: 1.159251]\n",
            "539 [D loss: 0.613187, acc.: 62.50%] [G loss: 1.099327]\n",
            "540 [D loss: 0.591289, acc.: 64.06%] [G loss: 1.131285]\n",
            "541 [D loss: 0.595683, acc.: 70.31%] [G loss: 1.131991]\n",
            "542 [D loss: 0.515003, acc.: 79.69%] [G loss: 1.106334]\n",
            "543 [D loss: 0.605035, acc.: 68.75%] [G loss: 1.013049]\n",
            "544 [D loss: 0.462803, acc.: 87.50%] [G loss: 1.120770]\n",
            "545 [D loss: 0.456882, acc.: 84.38%] [G loss: 1.206223]\n",
            "546 [D loss: 0.484269, acc.: 71.88%] [G loss: 1.203271]\n",
            "547 [D loss: 0.517289, acc.: 76.56%] [G loss: 1.079888]\n",
            "548 [D loss: 0.577636, acc.: 54.69%] [G loss: 1.022169]\n",
            "549 [D loss: 0.553739, acc.: 73.44%] [G loss: 1.080637]\n",
            "550 [D loss: 0.687117, acc.: 50.00%] [G loss: 1.043990]\n",
            "551 [D loss: 0.686164, acc.: 42.19%] [G loss: 0.982228]\n",
            "552 [D loss: 0.610967, acc.: 59.38%] [G loss: 1.080392]\n",
            "553 [D loss: 0.611955, acc.: 60.94%] [G loss: 0.965470]\n",
            "554 [D loss: 0.516541, acc.: 78.12%] [G loss: 1.032405]\n",
            "555 [D loss: 0.474137, acc.: 85.94%] [G loss: 1.130791]\n",
            "556 [D loss: 0.439687, acc.: 84.38%] [G loss: 1.434495]\n",
            "557 [D loss: 0.521907, acc.: 75.00%] [G loss: 1.099025]\n",
            "558 [D loss: 0.537394, acc.: 65.62%] [G loss: 1.098618]\n",
            "559 [D loss: 0.480461, acc.: 79.69%] [G loss: 1.198699]\n",
            "560 [D loss: 0.534387, acc.: 75.00%] [G loss: 1.066153]\n",
            "561 [D loss: 0.522823, acc.: 73.44%] [G loss: 1.002901]\n",
            "562 [D loss: 0.595621, acc.: 57.81%] [G loss: 0.930752]\n",
            "563 [D loss: 0.581345, acc.: 67.19%] [G loss: 0.846034]\n",
            "564 [D loss: 0.537796, acc.: 75.00%] [G loss: 0.896723]\n",
            "565 [D loss: 0.531235, acc.: 76.56%] [G loss: 0.874329]\n",
            "566 [D loss: 0.540712, acc.: 71.88%] [G loss: 0.933801]\n",
            "567 [D loss: 0.523951, acc.: 75.00%] [G loss: 0.852558]\n",
            "568 [D loss: 0.502279, acc.: 75.00%] [G loss: 0.866002]\n",
            "569 [D loss: 0.538454, acc.: 67.19%] [G loss: 0.879395]\n",
            "570 [D loss: 0.551736, acc.: 76.56%] [G loss: 0.872318]\n",
            "571 [D loss: 0.512598, acc.: 73.44%] [G loss: 0.898705]\n",
            "572 [D loss: 0.496341, acc.: 79.69%] [G loss: 0.916504]\n",
            "573 [D loss: 0.474173, acc.: 79.69%] [G loss: 0.869439]\n",
            "574 [D loss: 0.523174, acc.: 67.19%] [G loss: 0.938880]\n",
            "575 [D loss: 0.518347, acc.: 70.31%] [G loss: 0.953686]\n",
            "576 [D loss: 0.539727, acc.: 75.00%] [G loss: 1.037738]\n",
            "577 [D loss: 0.531812, acc.: 78.12%] [G loss: 0.893189]\n",
            "578 [D loss: 0.561250, acc.: 62.50%] [G loss: 0.952375]\n",
            "579 [D loss: 0.508107, acc.: 78.12%] [G loss: 0.985364]\n",
            "580 [D loss: 0.587797, acc.: 71.88%] [G loss: 0.945543]\n",
            "581 [D loss: 0.574951, acc.: 68.75%] [G loss: 0.939664]\n",
            "582 [D loss: 0.606124, acc.: 64.06%] [G loss: 0.876148]\n",
            "583 [D loss: 0.568603, acc.: 73.44%] [G loss: 0.933586]\n",
            "584 [D loss: 0.593831, acc.: 65.62%] [G loss: 0.956100]\n",
            "585 [D loss: 0.538502, acc.: 73.44%] [G loss: 0.926438]\n",
            "586 [D loss: 0.580198, acc.: 68.75%] [G loss: 0.874088]\n",
            "587 [D loss: 0.537031, acc.: 71.88%] [G loss: 0.855640]\n",
            "588 [D loss: 0.601478, acc.: 60.94%] [G loss: 0.896737]\n",
            "589 [D loss: 0.623053, acc.: 64.06%] [G loss: 0.846723]\n",
            "590 [D loss: 0.576182, acc.: 70.31%] [G loss: 0.981454]\n",
            "591 [D loss: 0.614525, acc.: 62.50%] [G loss: 0.857754]\n",
            "592 [D loss: 0.601326, acc.: 67.19%] [G loss: 0.743051]\n",
            "593 [D loss: 0.616581, acc.: 65.62%] [G loss: 0.889898]\n",
            "594 [D loss: 0.574643, acc.: 70.31%] [G loss: 0.971196]\n",
            "595 [D loss: 0.568846, acc.: 71.88%] [G loss: 0.981204]\n",
            "596 [D loss: 0.560020, acc.: 65.62%] [G loss: 0.977074]\n",
            "597 [D loss: 0.598745, acc.: 64.06%] [G loss: 1.155667]\n",
            "598 [D loss: 0.599168, acc.: 71.88%] [G loss: 0.988765]\n",
            "599 [D loss: 0.590533, acc.: 64.06%] [G loss: 1.027213]\n",
            "600 [D loss: 0.557445, acc.: 70.31%] [G loss: 0.925790]\n",
            "generated_data\n",
            "601 [D loss: 0.540278, acc.: 71.88%] [G loss: 0.888193]\n",
            "602 [D loss: 0.594268, acc.: 62.50%] [G loss: 0.959420]\n",
            "603 [D loss: 0.574742, acc.: 64.06%] [G loss: 1.066281]\n",
            "604 [D loss: 0.598879, acc.: 65.62%] [G loss: 1.016961]\n",
            "605 [D loss: 0.601235, acc.: 64.06%] [G loss: 1.183207]\n",
            "606 [D loss: 0.552733, acc.: 73.44%] [G loss: 1.154224]\n",
            "607 [D loss: 0.586496, acc.: 65.62%] [G loss: 1.148419]\n",
            "608 [D loss: 0.644932, acc.: 62.50%] [G loss: 0.998745]\n",
            "609 [D loss: 0.551354, acc.: 71.88%] [G loss: 0.924569]\n",
            "610 [D loss: 0.610671, acc.: 60.94%] [G loss: 0.886080]\n",
            "611 [D loss: 0.604567, acc.: 60.94%] [G loss: 0.926440]\n",
            "612 [D loss: 0.537270, acc.: 75.00%] [G loss: 0.898413]\n",
            "613 [D loss: 0.574950, acc.: 68.75%] [G loss: 1.128819]\n",
            "614 [D loss: 0.556270, acc.: 68.75%] [G loss: 1.013643]\n",
            "615 [D loss: 0.534588, acc.: 73.44%] [G loss: 1.029905]\n",
            "616 [D loss: 0.557151, acc.: 68.75%] [G loss: 1.016664]\n",
            "617 [D loss: 0.542103, acc.: 70.31%] [G loss: 0.908294]\n",
            "618 [D loss: 0.540420, acc.: 75.00%] [G loss: 0.956203]\n",
            "619 [D loss: 0.565292, acc.: 65.62%] [G loss: 0.913512]\n",
            "620 [D loss: 0.538648, acc.: 65.62%] [G loss: 0.944315]\n",
            "621 [D loss: 0.533632, acc.: 71.88%] [G loss: 0.980864]\n",
            "622 [D loss: 0.533656, acc.: 70.31%] [G loss: 0.999225]\n",
            "623 [D loss: 0.537801, acc.: 68.75%] [G loss: 0.962900]\n",
            "624 [D loss: 0.520459, acc.: 67.19%] [G loss: 0.901418]\n",
            "625 [D loss: 0.573038, acc.: 62.50%] [G loss: 0.985990]\n",
            "626 [D loss: 0.527941, acc.: 71.88%] [G loss: 1.063280]\n",
            "627 [D loss: 0.526863, acc.: 71.88%] [G loss: 0.942305]\n",
            "628 [D loss: 0.524544, acc.: 70.31%] [G loss: 1.062018]\n",
            "629 [D loss: 0.543971, acc.: 68.75%] [G loss: 0.977741]\n",
            "630 [D loss: 0.551954, acc.: 67.19%] [G loss: 1.011455]\n",
            "631 [D loss: 0.534723, acc.: 67.19%] [G loss: 1.019332]\n",
            "632 [D loss: 0.536266, acc.: 68.75%] [G loss: 0.971917]\n",
            "633 [D loss: 0.528205, acc.: 70.31%] [G loss: 0.984421]\n",
            "634 [D loss: 0.532879, acc.: 68.75%] [G loss: 1.042939]\n",
            "635 [D loss: 0.541099, acc.: 67.19%] [G loss: 0.998902]\n",
            "636 [D loss: 0.534403, acc.: 71.88%] [G loss: 1.011261]\n",
            "637 [D loss: 0.518961, acc.: 68.75%] [G loss: 0.958971]\n",
            "638 [D loss: 0.544214, acc.: 67.19%] [G loss: 0.959489]\n",
            "639 [D loss: 0.545526, acc.: 68.75%] [G loss: 0.875752]\n",
            "640 [D loss: 0.686926, acc.: 50.00%] [G loss: 0.992314]\n",
            "641 [D loss: 0.568139, acc.: 64.06%] [G loss: 0.917986]\n",
            "642 [D loss: 0.599460, acc.: 64.06%] [G loss: 0.917144]\n",
            "643 [D loss: 0.568011, acc.: 67.19%] [G loss: 0.955032]\n",
            "644 [D loss: 0.596396, acc.: 57.81%] [G loss: 0.942163]\n",
            "645 [D loss: 0.586249, acc.: 64.06%] [G loss: 0.982888]\n",
            "646 [D loss: 0.578898, acc.: 68.75%] [G loss: 1.067388]\n",
            "647 [D loss: 0.585805, acc.: 67.19%] [G loss: 1.079325]\n",
            "648 [D loss: 0.584231, acc.: 65.62%] [G loss: 1.040907]\n",
            "649 [D loss: 0.554968, acc.: 68.75%] [G loss: 1.034584]\n",
            "650 [D loss: 0.574729, acc.: 68.75%] [G loss: 0.968728]\n",
            "651 [D loss: 0.581835, acc.: 67.19%] [G loss: 0.998059]\n",
            "652 [D loss: 0.606417, acc.: 65.62%] [G loss: 0.974200]\n",
            "653 [D loss: 0.568385, acc.: 67.19%] [G loss: 0.960631]\n",
            "654 [D loss: 0.570364, acc.: 70.31%] [G loss: 0.956398]\n",
            "655 [D loss: 0.561932, acc.: 67.19%] [G loss: 0.996826]\n",
            "656 [D loss: 0.557774, acc.: 65.62%] [G loss: 1.366465]\n",
            "657 [D loss: 0.586096, acc.: 68.75%] [G loss: 1.067512]\n",
            "658 [D loss: 0.579480, acc.: 60.94%] [G loss: 0.919098]\n",
            "659 [D loss: 0.552831, acc.: 67.19%] [G loss: 0.957120]\n",
            "660 [D loss: 0.560492, acc.: 68.75%] [G loss: 0.911097]\n",
            "661 [D loss: 0.538339, acc.: 71.88%] [G loss: 0.911328]\n",
            "662 [D loss: 0.569197, acc.: 60.94%] [G loss: 0.958231]\n",
            "663 [D loss: 0.530342, acc.: 71.88%] [G loss: 0.991905]\n",
            "664 [D loss: 0.533253, acc.: 70.31%] [G loss: 1.028987]\n",
            "665 [D loss: 0.603002, acc.: 64.06%] [G loss: 0.927733]\n",
            "666 [D loss: 0.520584, acc.: 73.44%] [G loss: 0.948550]\n",
            "667 [D loss: 0.555422, acc.: 65.62%] [G loss: 0.914155]\n",
            "668 [D loss: 0.553050, acc.: 68.75%] [G loss: 1.020962]\n",
            "669 [D loss: 0.539586, acc.: 71.88%] [G loss: 0.979553]\n",
            "670 [D loss: 0.534860, acc.: 68.75%] [G loss: 0.990249]\n",
            "671 [D loss: 0.546047, acc.: 68.75%] [G loss: 1.055629]\n",
            "672 [D loss: 0.534530, acc.: 65.62%] [G loss: 1.110873]\n",
            "673 [D loss: 0.549066, acc.: 67.19%] [G loss: 1.075690]\n",
            "674 [D loss: 0.594659, acc.: 67.19%] [G loss: 1.115974]\n",
            "675 [D loss: 0.529381, acc.: 70.31%] [G loss: 1.083139]\n",
            "676 [D loss: 0.521534, acc.: 71.88%] [G loss: 1.028277]\n",
            "677 [D loss: 0.550377, acc.: 67.19%] [G loss: 1.006988]\n",
            "678 [D loss: 0.528342, acc.: 75.00%] [G loss: 0.970104]\n",
            "679 [D loss: 0.531291, acc.: 68.75%] [G loss: 1.008656]\n",
            "680 [D loss: 0.515816, acc.: 73.44%] [G loss: 1.012053]\n",
            "681 [D loss: 0.523477, acc.: 71.88%] [G loss: 1.004623]\n",
            "682 [D loss: 0.509501, acc.: 75.00%] [G loss: 1.007824]\n",
            "683 [D loss: 0.502374, acc.: 76.56%] [G loss: 0.955458]\n",
            "684 [D loss: 0.540603, acc.: 70.31%] [G loss: 1.013995]\n",
            "685 [D loss: 0.539035, acc.: 73.44%] [G loss: 0.923180]\n",
            "686 [D loss: 0.619261, acc.: 56.25%] [G loss: 0.891845]\n",
            "687 [D loss: 0.544110, acc.: 70.31%] [G loss: 0.920060]\n",
            "688 [D loss: 0.575312, acc.: 71.88%] [G loss: 1.071054]\n",
            "689 [D loss: 0.546547, acc.: 68.75%] [G loss: 1.062594]\n",
            "690 [D loss: 0.560109, acc.: 65.62%] [G loss: 1.104622]\n",
            "691 [D loss: 0.590822, acc.: 68.75%] [G loss: 0.940416]\n",
            "692 [D loss: 0.585064, acc.: 59.38%] [G loss: 0.916912]\n",
            "693 [D loss: 0.561649, acc.: 70.31%] [G loss: 0.883125]\n",
            "694 [D loss: 0.584267, acc.: 64.06%] [G loss: 1.024681]\n",
            "695 [D loss: 0.575693, acc.: 65.62%] [G loss: 1.004584]\n",
            "696 [D loss: 0.557260, acc.: 71.88%] [G loss: 1.041636]\n",
            "697 [D loss: 0.569198, acc.: 67.19%] [G loss: 1.022605]\n",
            "698 [D loss: 0.567037, acc.: 70.31%] [G loss: 0.993754]\n",
            "699 [D loss: 0.549672, acc.: 75.00%] [G loss: 1.017295]\n",
            "700 [D loss: 0.527309, acc.: 71.88%] [G loss: 1.011263]\n",
            "generated_data\n",
            "701 [D loss: 0.539444, acc.: 68.75%] [G loss: 1.078582]\n",
            "702 [D loss: 0.549294, acc.: 76.56%] [G loss: 1.007529]\n",
            "703 [D loss: 0.574437, acc.: 62.50%] [G loss: 0.936035]\n",
            "704 [D loss: 0.514278, acc.: 76.56%] [G loss: 0.978780]\n",
            "705 [D loss: 0.568824, acc.: 64.06%] [G loss: 0.916100]\n",
            "706 [D loss: 0.563569, acc.: 67.19%] [G loss: 0.999989]\n",
            "707 [D loss: 0.634537, acc.: 62.50%] [G loss: 1.002749]\n",
            "708 [D loss: 0.587819, acc.: 62.50%] [G loss: 1.096632]\n",
            "709 [D loss: 0.572010, acc.: 67.19%] [G loss: 1.038419]\n",
            "710 [D loss: 0.602222, acc.: 60.94%] [G loss: 1.099905]\n",
            "711 [D loss: 0.619225, acc.: 59.38%] [G loss: 1.024087]\n",
            "712 [D loss: 0.562474, acc.: 70.31%] [G loss: 1.000286]\n",
            "713 [D loss: 0.578471, acc.: 67.19%] [G loss: 0.935042]\n",
            "714 [D loss: 0.568433, acc.: 70.31%] [G loss: 0.955193]\n",
            "715 [D loss: 0.530027, acc.: 75.00%] [G loss: 1.004918]\n",
            "716 [D loss: 0.569567, acc.: 68.75%] [G loss: 1.015041]\n",
            "717 [D loss: 0.560050, acc.: 73.44%] [G loss: 1.287310]\n",
            "718 [D loss: 0.597261, acc.: 67.19%] [G loss: 1.130219]\n",
            "719 [D loss: 0.568062, acc.: 70.31%] [G loss: 1.073448]\n",
            "720 [D loss: 0.560780, acc.: 68.75%] [G loss: 1.011883]\n",
            "721 [D loss: 0.561703, acc.: 67.19%] [G loss: 0.997793]\n",
            "722 [D loss: 0.521479, acc.: 73.44%] [G loss: 0.977701]\n",
            "723 [D loss: 0.542071, acc.: 71.88%] [G loss: 0.989908]\n",
            "724 [D loss: 0.554058, acc.: 65.62%] [G loss: 0.995224]\n",
            "725 [D loss: 0.536665, acc.: 70.31%] [G loss: 1.038486]\n",
            "726 [D loss: 0.553605, acc.: 68.75%] [G loss: 0.974971]\n",
            "727 [D loss: 0.568443, acc.: 65.62%] [G loss: 0.972323]\n",
            "728 [D loss: 0.557366, acc.: 71.88%] [G loss: 0.958291]\n",
            "729 [D loss: 0.550854, acc.: 71.88%] [G loss: 0.951630]\n",
            "730 [D loss: 0.557295, acc.: 71.88%] [G loss: 0.942712]\n",
            "731 [D loss: 0.539381, acc.: 68.75%] [G loss: 1.002397]\n",
            "732 [D loss: 0.559259, acc.: 65.62%] [G loss: 0.950569]\n",
            "733 [D loss: 0.574467, acc.: 65.62%] [G loss: 0.903026]\n",
            "734 [D loss: 0.569237, acc.: 64.06%] [G loss: 0.858634]\n",
            "735 [D loss: 0.593041, acc.: 59.38%] [G loss: 0.907293]\n",
            "736 [D loss: 0.589687, acc.: 65.62%] [G loss: 0.939428]\n",
            "737 [D loss: 0.529788, acc.: 75.00%] [G loss: 0.934624]\n",
            "738 [D loss: 0.524246, acc.: 75.00%] [G loss: 1.015794]\n",
            "739 [D loss: 0.530474, acc.: 71.88%] [G loss: 0.960167]\n",
            "740 [D loss: 0.645349, acc.: 57.81%] [G loss: 0.954441]\n",
            "741 [D loss: 0.578413, acc.: 67.19%] [G loss: 1.047771]\n",
            "742 [D loss: 0.516366, acc.: 75.00%] [G loss: 1.108628]\n",
            "743 [D loss: 0.555812, acc.: 65.62%] [G loss: 1.039268]\n",
            "744 [D loss: 0.528417, acc.: 75.00%] [G loss: 0.985260]\n",
            "745 [D loss: 0.957427, acc.: 35.94%] [G loss: 0.960562]\n",
            "746 [D loss: 0.592381, acc.: 68.75%] [G loss: 0.991512]\n",
            "747 [D loss: 0.571696, acc.: 65.62%] [G loss: 1.017419]\n",
            "748 [D loss: 0.589479, acc.: 62.50%] [G loss: 1.093935]\n",
            "749 [D loss: 0.560052, acc.: 65.62%] [G loss: 1.072759]\n",
            "750 [D loss: 0.541591, acc.: 70.31%] [G loss: 1.012094]\n",
            "751 [D loss: 0.552629, acc.: 68.75%] [G loss: 1.000770]\n",
            "752 [D loss: 0.580679, acc.: 60.94%] [G loss: 0.957725]\n",
            "753 [D loss: 0.552518, acc.: 68.75%] [G loss: 0.953464]\n",
            "754 [D loss: 0.553504, acc.: 70.31%] [G loss: 0.934938]\n",
            "755 [D loss: 0.553412, acc.: 67.19%] [G loss: 0.921700]\n",
            "756 [D loss: 0.558975, acc.: 68.75%] [G loss: 0.898987]\n",
            "757 [D loss: 0.573508, acc.: 68.75%] [G loss: 0.917125]\n",
            "758 [D loss: 0.557344, acc.: 71.88%] [G loss: 0.909597]\n",
            "759 [D loss: 0.576134, acc.: 65.62%] [G loss: 0.878291]\n",
            "760 [D loss: 0.568539, acc.: 70.31%] [G loss: 0.915040]\n",
            "761 [D loss: 0.558968, acc.: 70.31%] [G loss: 0.915269]\n",
            "762 [D loss: 0.551837, acc.: 68.75%] [G loss: 0.907302]\n",
            "763 [D loss: 0.571291, acc.: 67.19%] [G loss: 0.882335]\n",
            "764 [D loss: 0.568746, acc.: 71.88%] [G loss: 0.895878]\n",
            "765 [D loss: 0.587178, acc.: 60.94%] [G loss: 0.933406]\n",
            "766 [D loss: 0.548331, acc.: 70.31%] [G loss: 0.888311]\n",
            "767 [D loss: 0.563469, acc.: 71.88%] [G loss: 0.866099]\n",
            "768 [D loss: 0.566795, acc.: 68.75%] [G loss: 0.936097]\n",
            "769 [D loss: 0.566375, acc.: 65.62%] [G loss: 0.986861]\n",
            "770 [D loss: 0.586524, acc.: 67.19%] [G loss: 0.910916]\n",
            "771 [D loss: 0.581550, acc.: 62.50%] [G loss: 0.901056]\n",
            "772 [D loss: 0.562275, acc.: 57.81%] [G loss: 0.878935]\n",
            "773 [D loss: 0.575473, acc.: 65.62%] [G loss: 0.881314]\n",
            "774 [D loss: 0.569179, acc.: 65.62%] [G loss: 1.037927]\n",
            "775 [D loss: 0.588227, acc.: 64.06%] [G loss: 0.948751]\n",
            "776 [D loss: 0.542295, acc.: 71.88%] [G loss: 0.953841]\n",
            "777 [D loss: 0.585096, acc.: 57.81%] [G loss: 0.953896]\n",
            "778 [D loss: 0.598988, acc.: 64.06%] [G loss: 0.928380]\n",
            "779 [D loss: 0.572907, acc.: 65.62%] [G loss: 0.949465]\n",
            "780 [D loss: 0.699317, acc.: 60.94%] [G loss: 0.922809]\n",
            "781 [D loss: 0.595821, acc.: 60.94%] [G loss: 0.977345]\n",
            "782 [D loss: 0.609925, acc.: 56.25%] [G loss: 0.887575]\n",
            "783 [D loss: 0.678421, acc.: 54.69%] [G loss: 0.936570]\n",
            "784 [D loss: 0.615063, acc.: 59.38%] [G loss: 0.929590]\n",
            "785 [D loss: 0.630160, acc.: 59.38%] [G loss: 1.031215]\n",
            "786 [D loss: 0.596501, acc.: 62.50%] [G loss: 0.961683]\n",
            "787 [D loss: 0.590832, acc.: 65.62%] [G loss: 0.946564]\n",
            "788 [D loss: 0.593650, acc.: 64.06%] [G loss: 0.978999]\n",
            "789 [D loss: 0.615477, acc.: 56.25%] [G loss: 0.957389]\n",
            "790 [D loss: 0.594405, acc.: 59.38%] [G loss: 0.873783]\n",
            "791 [D loss: 0.578076, acc.: 65.62%] [G loss: 0.936153]\n",
            "792 [D loss: 0.586789, acc.: 62.50%] [G loss: 0.873899]\n",
            "793 [D loss: 0.625873, acc.: 53.12%] [G loss: 0.885923]\n",
            "794 [D loss: 0.586794, acc.: 57.81%] [G loss: 0.916697]\n",
            "795 [D loss: 0.604254, acc.: 56.25%] [G loss: 0.918738]\n",
            "796 [D loss: 0.611923, acc.: 59.38%] [G loss: 0.881205]\n",
            "797 [D loss: 0.594728, acc.: 60.94%] [G loss: 0.889681]\n",
            "798 [D loss: 0.595125, acc.: 56.25%] [G loss: 0.908375]\n",
            "799 [D loss: 0.599638, acc.: 67.19%] [G loss: 0.888188]\n",
            "800 [D loss: 0.589377, acc.: 60.94%] [G loss: 0.890714]\n",
            "generated_data\n",
            "801 [D loss: 0.596026, acc.: 64.06%] [G loss: 0.927215]\n",
            "802 [D loss: 0.599235, acc.: 59.38%] [G loss: 0.898841]\n",
            "803 [D loss: 0.600136, acc.: 60.94%] [G loss: 0.931739]\n",
            "804 [D loss: 0.594088, acc.: 65.62%] [G loss: 0.895052]\n",
            "805 [D loss: 0.621742, acc.: 59.38%] [G loss: 0.916029]\n",
            "806 [D loss: 0.588274, acc.: 62.50%] [G loss: 0.929484]\n",
            "807 [D loss: 0.586749, acc.: 60.94%] [G loss: 0.949983]\n",
            "808 [D loss: 0.593377, acc.: 64.06%] [G loss: 0.973746]\n",
            "809 [D loss: 0.586533, acc.: 67.19%] [G loss: 0.964879]\n",
            "810 [D loss: 0.572092, acc.: 65.62%] [G loss: 0.981480]\n",
            "811 [D loss: 0.611774, acc.: 57.81%] [G loss: 0.930879]\n",
            "812 [D loss: 0.583519, acc.: 64.06%] [G loss: 0.931750]\n",
            "813 [D loss: 0.608375, acc.: 60.94%] [G loss: 0.956645]\n",
            "814 [D loss: 0.602427, acc.: 60.94%] [G loss: 0.962666]\n",
            "815 [D loss: 0.604017, acc.: 59.38%] [G loss: 0.893388]\n",
            "816 [D loss: 0.661159, acc.: 59.38%] [G loss: 1.000302]\n",
            "817 [D loss: 0.596837, acc.: 57.81%] [G loss: 0.971649]\n",
            "818 [D loss: 0.600250, acc.: 62.50%] [G loss: 0.955075]\n",
            "819 [D loss: 0.605027, acc.: 64.06%] [G loss: 0.944830]\n",
            "820 [D loss: 0.592246, acc.: 62.50%] [G loss: 0.939957]\n",
            "821 [D loss: 0.602413, acc.: 62.50%] [G loss: 0.958784]\n",
            "822 [D loss: 0.590459, acc.: 67.19%] [G loss: 0.985939]\n",
            "823 [D loss: 0.582478, acc.: 67.19%] [G loss: 0.960746]\n",
            "824 [D loss: 0.603514, acc.: 60.94%] [G loss: 1.022562]\n",
            "825 [D loss: 0.589174, acc.: 60.94%] [G loss: 0.961541]\n",
            "826 [D loss: 0.609657, acc.: 56.25%] [G loss: 0.893719]\n",
            "827 [D loss: 0.592354, acc.: 65.62%] [G loss: 0.988740]\n",
            "828 [D loss: 0.605039, acc.: 59.38%] [G loss: 0.936050]\n",
            "829 [D loss: 0.602103, acc.: 62.50%] [G loss: 1.000249]\n",
            "830 [D loss: 0.592318, acc.: 59.38%] [G loss: 0.958518]\n",
            "831 [D loss: 0.599079, acc.: 64.06%] [G loss: 0.938885]\n",
            "832 [D loss: 0.597013, acc.: 64.06%] [G loss: 0.978191]\n",
            "833 [D loss: 0.601651, acc.: 64.06%] [G loss: 0.995355]\n",
            "834 [D loss: 0.594419, acc.: 64.06%] [G loss: 0.991300]\n",
            "835 [D loss: 0.587389, acc.: 65.62%] [G loss: 0.971904]\n",
            "836 [D loss: 0.582574, acc.: 65.62%] [G loss: 0.975174]\n",
            "837 [D loss: 0.602888, acc.: 62.50%] [G loss: 0.951161]\n",
            "838 [D loss: 0.591284, acc.: 62.50%] [G loss: 0.954656]\n",
            "839 [D loss: 0.605716, acc.: 64.06%] [G loss: 0.948322]\n",
            "840 [D loss: 0.658928, acc.: 64.06%] [G loss: 0.971254]\n",
            "841 [D loss: 0.599505, acc.: 64.06%] [G loss: 1.027076]\n",
            "842 [D loss: 0.604200, acc.: 62.50%] [G loss: 1.028208]\n",
            "843 [D loss: 0.618874, acc.: 64.06%] [G loss: 0.994697]\n",
            "844 [D loss: 0.599208, acc.: 64.06%] [G loss: 0.953121]\n",
            "845 [D loss: 0.585236, acc.: 62.50%] [G loss: 0.975626]\n",
            "846 [D loss: 0.573750, acc.: 64.06%] [G loss: 0.927099]\n",
            "847 [D loss: 0.577704, acc.: 65.62%] [G loss: 0.967325]\n",
            "848 [D loss: 0.606903, acc.: 60.94%] [G loss: 0.966037]\n",
            "849 [D loss: 0.590700, acc.: 62.50%] [G loss: 1.008484]\n",
            "850 [D loss: 0.592109, acc.: 65.62%] [G loss: 1.065521]\n",
            "851 [D loss: 0.607115, acc.: 64.06%] [G loss: 1.000860]\n",
            "852 [D loss: 0.586756, acc.: 64.06%] [G loss: 0.996053]\n",
            "853 [D loss: 0.604853, acc.: 64.06%] [G loss: 1.060302]\n",
            "854 [D loss: 0.598299, acc.: 64.06%] [G loss: 1.021534]\n",
            "855 [D loss: 0.594187, acc.: 64.06%] [G loss: 1.033428]\n",
            "856 [D loss: 0.600898, acc.: 62.50%] [G loss: 1.006660]\n",
            "857 [D loss: 0.589958, acc.: 64.06%] [G loss: 1.030254]\n",
            "858 [D loss: 0.623497, acc.: 60.94%] [G loss: 0.983953]\n",
            "859 [D loss: 0.590847, acc.: 62.50%] [G loss: 1.012040]\n",
            "860 [D loss: 0.619074, acc.: 59.38%] [G loss: 0.957339]\n",
            "861 [D loss: 0.586750, acc.: 62.50%] [G loss: 0.998894]\n",
            "862 [D loss: 0.594158, acc.: 62.50%] [G loss: 0.967476]\n",
            "863 [D loss: 0.575880, acc.: 64.06%] [G loss: 0.974957]\n",
            "864 [D loss: 0.592477, acc.: 67.19%] [G loss: 0.972431]\n",
            "865 [D loss: 0.586662, acc.: 64.06%] [G loss: 0.979858]\n",
            "866 [D loss: 0.590131, acc.: 65.62%] [G loss: 1.015601]\n",
            "867 [D loss: 0.589156, acc.: 62.50%] [G loss: 0.962498]\n",
            "868 [D loss: 0.642044, acc.: 57.81%] [G loss: 0.962693]\n",
            "869 [D loss: 0.598845, acc.: 64.06%] [G loss: 0.986673]\n",
            "870 [D loss: 0.590871, acc.: 62.50%] [G loss: 0.998179]\n",
            "871 [D loss: 0.584796, acc.: 64.06%] [G loss: 1.002326]\n",
            "872 [D loss: 0.580740, acc.: 65.62%] [G loss: 1.007223]\n",
            "873 [D loss: 0.575211, acc.: 65.62%] [G loss: 0.952980]\n",
            "874 [D loss: 0.555893, acc.: 65.62%] [G loss: 1.019289]\n",
            "875 [D loss: 0.595197, acc.: 59.38%] [G loss: 0.970308]\n",
            "876 [D loss: 0.586533, acc.: 64.06%] [G loss: 1.026340]\n",
            "877 [D loss: 0.592704, acc.: 60.94%] [G loss: 0.910860]\n",
            "878 [D loss: 0.588205, acc.: 60.94%] [G loss: 0.969178]\n",
            "879 [D loss: 0.583436, acc.: 65.62%] [G loss: 0.995111]\n",
            "880 [D loss: 0.593439, acc.: 64.06%] [G loss: 0.958314]\n",
            "881 [D loss: 0.610110, acc.: 62.50%] [G loss: 0.932029]\n",
            "882 [D loss: 0.596710, acc.: 60.94%] [G loss: 0.930268]\n",
            "883 [D loss: 0.604262, acc.: 60.94%] [G loss: 1.002344]\n",
            "884 [D loss: 0.607154, acc.: 60.94%] [G loss: 0.986455]\n",
            "885 [D loss: 0.603421, acc.: 64.06%] [G loss: 1.007535]\n",
            "886 [D loss: 0.609320, acc.: 62.50%] [G loss: 1.001486]\n",
            "887 [D loss: 0.600695, acc.: 64.06%] [G loss: 1.042283]\n",
            "888 [D loss: 0.596897, acc.: 64.06%] [G loss: 0.981469]\n",
            "889 [D loss: 0.597083, acc.: 60.94%] [G loss: 0.980699]\n",
            "890 [D loss: 0.592570, acc.: 64.06%] [G loss: 1.008067]\n",
            "891 [D loss: 0.587884, acc.: 64.06%] [G loss: 0.995046]\n",
            "892 [D loss: 0.587248, acc.: 62.50%] [G loss: 1.008264]\n",
            "893 [D loss: 0.579941, acc.: 64.06%] [G loss: 0.970291]\n",
            "894 [D loss: 0.588972, acc.: 64.06%] [G loss: 0.981540]\n",
            "895 [D loss: 0.587178, acc.: 64.06%] [G loss: 0.992362]\n",
            "896 [D loss: 0.587397, acc.: 64.06%] [G loss: 0.969373]\n",
            "897 [D loss: 0.598633, acc.: 64.06%] [G loss: 0.927896]\n",
            "898 [D loss: 0.590553, acc.: 60.94%] [G loss: 0.965601]\n",
            "899 [D loss: 0.597019, acc.: 62.50%] [G loss: 0.954139]\n",
            "900 [D loss: 0.582214, acc.: 65.62%] [G loss: 0.903808]\n",
            "generated_data\n",
            "901 [D loss: 0.576478, acc.: 65.62%] [G loss: 0.963237]\n",
            "902 [D loss: 0.582855, acc.: 67.19%] [G loss: 0.934616]\n",
            "903 [D loss: 0.582587, acc.: 64.06%] [G loss: 0.951807]\n",
            "904 [D loss: 0.579009, acc.: 59.38%] [G loss: 0.939305]\n",
            "905 [D loss: 0.575444, acc.: 65.62%] [G loss: 0.938374]\n",
            "906 [D loss: 0.609118, acc.: 62.50%] [G loss: 0.928377]\n",
            "907 [D loss: 0.573698, acc.: 65.62%] [G loss: 0.969301]\n",
            "908 [D loss: 0.582830, acc.: 65.62%] [G loss: 0.952904]\n",
            "909 [D loss: 0.599018, acc.: 64.06%] [G loss: 0.951667]\n",
            "910 [D loss: 0.571474, acc.: 65.62%] [G loss: 0.983497]\n",
            "911 [D loss: 0.549402, acc.: 70.31%] [G loss: 0.998877]\n",
            "912 [D loss: 0.536650, acc.: 67.19%] [G loss: 0.958910]\n",
            "913 [D loss: 0.590141, acc.: 68.75%] [G loss: 0.889783]\n",
            "914 [D loss: 0.588159, acc.: 57.81%] [G loss: 0.926856]\n",
            "915 [D loss: 0.667555, acc.: 54.69%] [G loss: 1.012456]\n",
            "916 [D loss: 0.604734, acc.: 62.50%] [G loss: 1.049043]\n",
            "917 [D loss: 0.600852, acc.: 60.94%] [G loss: 1.069144]\n",
            "918 [D loss: 0.609653, acc.: 62.50%] [G loss: 0.976480]\n",
            "919 [D loss: 0.597970, acc.: 62.50%] [G loss: 0.999863]\n",
            "920 [D loss: 0.597069, acc.: 62.50%] [G loss: 0.973939]\n",
            "921 [D loss: 0.580433, acc.: 64.06%] [G loss: 0.993710]\n",
            "922 [D loss: 0.585557, acc.: 62.50%] [G loss: 1.014263]\n",
            "923 [D loss: 0.574929, acc.: 65.62%] [G loss: 0.978877]\n",
            "924 [D loss: 0.574001, acc.: 64.06%] [G loss: 0.942985]\n",
            "925 [D loss: 0.579079, acc.: 62.50%] [G loss: 1.034492]\n",
            "926 [D loss: 0.577333, acc.: 64.06%] [G loss: 0.946577]\n",
            "927 [D loss: 0.596160, acc.: 64.06%] [G loss: 0.987790]\n",
            "928 [D loss: 0.581236, acc.: 65.62%] [G loss: 0.932992]\n",
            "929 [D loss: 0.589146, acc.: 64.06%] [G loss: 0.940422]\n",
            "930 [D loss: 0.606448, acc.: 62.50%] [G loss: 0.911726]\n",
            "931 [D loss: 0.575782, acc.: 65.62%] [G loss: 0.934671]\n",
            "932 [D loss: 0.600643, acc.: 60.94%] [G loss: 0.951527]\n",
            "933 [D loss: 0.573357, acc.: 64.06%] [G loss: 0.926288]\n",
            "934 [D loss: 0.564271, acc.: 65.62%] [G loss: 0.919306]\n",
            "935 [D loss: 0.569872, acc.: 67.19%] [G loss: 0.948064]\n",
            "936 [D loss: 0.562106, acc.: 68.75%] [G loss: 0.933356]\n",
            "937 [D loss: 0.570517, acc.: 65.62%] [G loss: 0.944148]\n",
            "938 [D loss: 0.622867, acc.: 62.50%] [G loss: 0.967193]\n",
            "939 [D loss: 0.572278, acc.: 67.19%] [G loss: 0.970544]\n",
            "940 [D loss: 0.575403, acc.: 67.19%] [G loss: 0.953571]\n",
            "941 [D loss: 0.584541, acc.: 64.06%] [G loss: 0.965995]\n",
            "942 [D loss: 0.582453, acc.: 65.62%] [G loss: 1.006531]\n",
            "943 [D loss: 0.589329, acc.: 65.62%] [G loss: 0.952348]\n",
            "944 [D loss: 0.590908, acc.: 62.50%] [G loss: 1.010990]\n",
            "945 [D loss: 0.575059, acc.: 65.62%] [G loss: 0.999614]\n",
            "946 [D loss: 0.566751, acc.: 64.06%] [G loss: 1.035864]\n",
            "947 [D loss: 0.589501, acc.: 60.94%] [G loss: 1.009936]\n",
            "948 [D loss: 0.588775, acc.: 65.62%] [G loss: 1.028406]\n",
            "949 [D loss: 0.607203, acc.: 59.38%] [G loss: 1.093887]\n",
            "950 [D loss: 0.614634, acc.: 60.94%] [G loss: 1.057982]\n",
            "951 [D loss: 0.597160, acc.: 64.06%] [G loss: 0.999035]\n",
            "952 [D loss: 0.604126, acc.: 60.94%] [G loss: 0.953334]\n",
            "953 [D loss: 0.611536, acc.: 62.50%] [G loss: 0.957718]\n",
            "954 [D loss: 0.589348, acc.: 67.19%] [G loss: 0.958975]\n",
            "955 [D loss: 0.567115, acc.: 68.75%] [G loss: 0.967522]\n",
            "956 [D loss: 0.593463, acc.: 68.75%] [G loss: 0.971758]\n",
            "957 [D loss: 0.570518, acc.: 65.62%] [G loss: 0.974307]\n",
            "958 [D loss: 0.558179, acc.: 68.75%] [G loss: 1.125119]\n",
            "959 [D loss: 0.598707, acc.: 62.50%] [G loss: 1.004378]\n",
            "960 [D loss: 0.586024, acc.: 64.06%] [G loss: 0.979722]\n",
            "961 [D loss: 0.573620, acc.: 67.19%] [G loss: 0.932291]\n",
            "962 [D loss: 0.593738, acc.: 64.06%] [G loss: 0.997539]\n",
            "963 [D loss: 0.597385, acc.: 57.81%] [G loss: 1.011246]\n",
            "964 [D loss: 0.583157, acc.: 64.06%] [G loss: 0.979726]\n",
            "965 [D loss: 0.604968, acc.: 64.06%] [G loss: 0.976084]\n",
            "966 [D loss: 0.588590, acc.: 64.06%] [G loss: 0.971528]\n",
            "967 [D loss: 0.581659, acc.: 65.62%] [G loss: 0.962603]\n",
            "968 [D loss: 0.575896, acc.: 64.06%] [G loss: 0.934519]\n",
            "969 [D loss: 0.757838, acc.: 53.12%] [G loss: 1.028256]\n",
            "970 [D loss: 0.567508, acc.: 64.06%] [G loss: 1.061259]\n",
            "971 [D loss: 0.576805, acc.: 62.50%] [G loss: 1.092475]\n",
            "972 [D loss: 0.599632, acc.: 65.62%] [G loss: 1.072149]\n",
            "973 [D loss: 0.599301, acc.: 62.50%] [G loss: 0.919487]\n",
            "974 [D loss: 0.615199, acc.: 62.50%] [G loss: 0.907117]\n",
            "975 [D loss: 0.593319, acc.: 64.06%] [G loss: 0.931462]\n",
            "976 [D loss: 0.583789, acc.: 62.50%] [G loss: 0.916299]\n",
            "977 [D loss: 0.614381, acc.: 54.69%] [G loss: 0.957259]\n",
            "978 [D loss: 0.601555, acc.: 64.06%] [G loss: 0.899008]\n",
            "979 [D loss: 0.580857, acc.: 65.62%] [G loss: 0.940540]\n",
            "980 [D loss: 0.580589, acc.: 62.50%] [G loss: 1.010458]\n",
            "981 [D loss: 0.602634, acc.: 60.94%] [G loss: 0.936112]\n",
            "982 [D loss: 0.584963, acc.: 62.50%] [G loss: 0.905403]\n",
            "983 [D loss: 0.612733, acc.: 56.25%] [G loss: 0.865913]\n",
            "984 [D loss: 0.591351, acc.: 64.06%] [G loss: 0.970414]\n",
            "985 [D loss: 0.607812, acc.: 60.94%] [G loss: 0.914107]\n",
            "986 [D loss: 0.591072, acc.: 64.06%] [G loss: 0.920465]\n",
            "987 [D loss: 0.593062, acc.: 60.94%] [G loss: 0.924839]\n",
            "988 [D loss: 0.589688, acc.: 60.94%] [G loss: 0.926686]\n",
            "989 [D loss: 0.593738, acc.: 62.50%] [G loss: 0.892656]\n",
            "990 [D loss: 0.585528, acc.: 60.94%] [G loss: 0.887212]\n",
            "991 [D loss: 0.589693, acc.: 64.06%] [G loss: 0.928123]\n",
            "992 [D loss: 0.580382, acc.: 64.06%] [G loss: 0.924086]\n",
            "993 [D loss: 0.579977, acc.: 60.94%] [G loss: 0.924424]\n",
            "994 [D loss: 0.575221, acc.: 62.50%] [G loss: 0.905898]\n",
            "995 [D loss: 0.591440, acc.: 64.06%] [G loss: 0.888476]\n",
            "996 [D loss: 0.586989, acc.: 62.50%] [G loss: 0.938841]\n",
            "997 [D loss: 0.580126, acc.: 62.50%] [G loss: 0.931316]\n",
            "998 [D loss: 0.566235, acc.: 67.19%] [G loss: 0.893835]\n",
            "999 [D loss: 0.568098, acc.: 68.75%] [G loss: 0.895813]\n",
            "1000 [D loss: 0.594536, acc.: 59.38%] [G loss: 0.906588]\n",
            "generated_data\n",
            "1001 [D loss: 0.556387, acc.: 70.31%] [G loss: 0.884839]\n",
            "1002 [D loss: 0.578119, acc.: 64.06%] [G loss: 0.915917]\n",
            "1003 [D loss: 0.554903, acc.: 67.19%] [G loss: 0.994746]\n",
            "1004 [D loss: 0.555395, acc.: 67.19%] [G loss: 0.919538]\n",
            "1005 [D loss: 1.069077, acc.: 35.94%] [G loss: 0.968337]\n",
            "1006 [D loss: 0.586526, acc.: 62.50%] [G loss: 1.024266]\n",
            "1007 [D loss: 0.616701, acc.: 64.06%] [G loss: 1.003450]\n",
            "1008 [D loss: 0.618757, acc.: 60.94%] [G loss: 1.019728]\n",
            "1009 [D loss: 0.638781, acc.: 57.81%] [G loss: 0.937105]\n",
            "1010 [D loss: 0.613254, acc.: 64.06%] [G loss: 0.969578]\n",
            "1011 [D loss: 0.595342, acc.: 60.94%] [G loss: 0.957483]\n",
            "1012 [D loss: 0.604039, acc.: 64.06%] [G loss: 0.968651]\n",
            "1013 [D loss: 0.597398, acc.: 62.50%] [G loss: 0.948381]\n",
            "1014 [D loss: 0.591812, acc.: 62.50%] [G loss: 0.925280]\n",
            "1015 [D loss: 0.587727, acc.: 64.06%] [G loss: 0.933914]\n",
            "1016 [D loss: 0.585985, acc.: 64.06%] [G loss: 0.909434]\n",
            "1017 [D loss: 0.591169, acc.: 65.62%] [G loss: 0.907110]\n",
            "1018 [D loss: 0.593525, acc.: 64.06%] [G loss: 0.914936]\n",
            "1019 [D loss: 0.590777, acc.: 65.62%] [G loss: 0.926439]\n",
            "1020 [D loss: 0.586117, acc.: 64.06%] [G loss: 0.910022]\n",
            "1021 [D loss: 0.585361, acc.: 64.06%] [G loss: 0.920365]\n",
            "1022 [D loss: 0.596625, acc.: 64.06%] [G loss: 0.936448]\n",
            "1023 [D loss: 0.584904, acc.: 62.50%] [G loss: 0.918280]\n",
            "1024 [D loss: 0.589355, acc.: 64.06%] [G loss: 0.901667]\n",
            "1025 [D loss: 0.585292, acc.: 60.94%] [G loss: 0.909651]\n",
            "1026 [D loss: 0.574289, acc.: 65.62%] [G loss: 0.937070]\n",
            "1027 [D loss: 0.581511, acc.: 65.62%] [G loss: 0.913846]\n",
            "1028 [D loss: 0.592422, acc.: 59.38%] [G loss: 0.882951]\n",
            "1029 [D loss: 0.590129, acc.: 60.94%] [G loss: 0.932857]\n",
            "1030 [D loss: 0.580336, acc.: 62.50%] [G loss: 0.921103]\n",
            "1031 [D loss: 0.594920, acc.: 62.50%] [G loss: 0.949924]\n",
            "1032 [D loss: 0.587474, acc.: 62.50%] [G loss: 0.924464]\n",
            "1033 [D loss: 0.596969, acc.: 59.38%] [G loss: 0.943164]\n",
            "1034 [D loss: 0.584709, acc.: 62.50%] [G loss: 0.919330]\n",
            "1035 [D loss: 0.589911, acc.: 59.38%] [G loss: 0.919369]\n",
            "1036 [D loss: 0.623474, acc.: 59.38%] [G loss: 0.930398]\n",
            "1037 [D loss: 0.637107, acc.: 59.38%] [G loss: 1.009141]\n",
            "1038 [D loss: 0.601893, acc.: 60.94%] [G loss: 0.959888]\n",
            "1039 [D loss: 0.589805, acc.: 62.50%] [G loss: 1.073246]\n",
            "1040 [D loss: 0.596531, acc.: 62.50%] [G loss: 0.975289]\n",
            "1041 [D loss: 0.587918, acc.: 62.50%] [G loss: 0.978732]\n",
            "1042 [D loss: 0.586054, acc.: 62.50%] [G loss: 0.946238]\n",
            "1043 [D loss: 0.600503, acc.: 60.94%] [G loss: 0.963624]\n",
            "1044 [D loss: 0.613560, acc.: 62.50%] [G loss: 0.917592]\n",
            "1045 [D loss: 0.589866, acc.: 65.62%] [G loss: 0.944072]\n",
            "1046 [D loss: 0.601896, acc.: 60.94%] [G loss: 0.916812]\n",
            "1047 [D loss: 0.587337, acc.: 70.31%] [G loss: 0.918052]\n",
            "1048 [D loss: 0.593332, acc.: 64.06%] [G loss: 0.930837]\n",
            "1049 [D loss: 0.616223, acc.: 60.94%] [G loss: 0.959553]\n",
            "1050 [D loss: 0.596104, acc.: 65.62%] [G loss: 0.936853]\n",
            "1051 [D loss: 0.608586, acc.: 62.50%] [G loss: 0.936282]\n",
            "1052 [D loss: 0.596340, acc.: 62.50%] [G loss: 0.951074]\n",
            "1053 [D loss: 0.602315, acc.: 59.38%] [G loss: 0.927728]\n",
            "1054 [D loss: 0.579410, acc.: 67.19%] [G loss: 0.908437]\n",
            "1055 [D loss: 0.611277, acc.: 62.50%] [G loss: 0.937902]\n",
            "1056 [D loss: 0.595182, acc.: 67.19%] [G loss: 0.928224]\n",
            "1057 [D loss: 0.596089, acc.: 62.50%] [G loss: 0.922075]\n",
            "1058 [D loss: 0.608541, acc.: 59.38%] [G loss: 0.903086]\n",
            "1059 [D loss: 0.597782, acc.: 64.06%] [G loss: 0.911582]\n",
            "1060 [D loss: 0.600378, acc.: 65.62%] [G loss: 0.862156]\n",
            "1061 [D loss: 0.587388, acc.: 64.06%] [G loss: 0.900196]\n",
            "1062 [D loss: 0.606460, acc.: 60.94%] [G loss: 0.914126]\n",
            "1063 [D loss: 0.603432, acc.: 65.62%] [G loss: 0.901523]\n",
            "1064 [D loss: 0.602706, acc.: 64.06%] [G loss: 0.904778]\n",
            "1065 [D loss: 0.601576, acc.: 59.38%] [G loss: 0.932065]\n",
            "1066 [D loss: 0.608986, acc.: 57.81%] [G loss: 0.905566]\n",
            "1067 [D loss: 0.599037, acc.: 62.50%] [G loss: 0.913921]\n",
            "1068 [D loss: 0.600436, acc.: 62.50%] [G loss: 0.910617]\n",
            "1069 [D loss: 0.590906, acc.: 64.06%] [G loss: 0.950147]\n",
            "1070 [D loss: 0.615126, acc.: 62.50%] [G loss: 0.985687]\n",
            "1071 [D loss: 0.624070, acc.: 62.50%] [G loss: 0.959067]\n",
            "1072 [D loss: 0.597792, acc.: 62.50%] [G loss: 0.912852]\n",
            "1073 [D loss: 0.599740, acc.: 64.06%] [G loss: 0.912432]\n",
            "1074 [D loss: 0.599754, acc.: 60.94%] [G loss: 0.933555]\n",
            "1075 [D loss: 0.598565, acc.: 64.06%] [G loss: 0.948289]\n",
            "1076 [D loss: 0.608214, acc.: 60.94%] [G loss: 0.895999]\n",
            "1077 [D loss: 0.588080, acc.: 64.06%] [G loss: 0.892927]\n",
            "1078 [D loss: 0.593631, acc.: 65.62%] [G loss: 0.914951]\n",
            "1079 [D loss: 0.586198, acc.: 64.06%] [G loss: 0.900285]\n",
            "1080 [D loss: 0.594345, acc.: 59.38%] [G loss: 0.904541]\n",
            "1081 [D loss: 0.587482, acc.: 65.62%] [G loss: 0.881501]\n",
            "1082 [D loss: 0.598148, acc.: 64.06%] [G loss: 0.919461]\n",
            "1083 [D loss: 0.596609, acc.: 60.94%] [G loss: 0.927356]\n",
            "1084 [D loss: 0.595901, acc.: 62.50%] [G loss: 0.938988]\n",
            "1085 [D loss: 0.599068, acc.: 60.94%] [G loss: 0.904227]\n",
            "1086 [D loss: 0.601661, acc.: 59.38%] [G loss: 0.892904]\n",
            "1087 [D loss: 0.593755, acc.: 65.62%] [G loss: 0.899889]\n",
            "1088 [D loss: 0.593491, acc.: 64.06%] [G loss: 0.898778]\n",
            "1089 [D loss: 0.591506, acc.: 64.06%] [G loss: 0.914439]\n",
            "1090 [D loss: 0.585631, acc.: 64.06%] [G loss: 0.962700]\n",
            "1091 [D loss: 0.589195, acc.: 65.62%] [G loss: 0.950213]\n",
            "1092 [D loss: 0.582561, acc.: 64.06%] [G loss: 0.957192]\n",
            "1093 [D loss: 0.591516, acc.: 65.62%] [G loss: 0.909864]\n",
            "1094 [D loss: 0.583982, acc.: 65.62%] [G loss: 0.916121]\n",
            "1095 [D loss: 0.578096, acc.: 67.19%] [G loss: 0.965658]\n",
            "1096 [D loss: 0.581624, acc.: 65.62%] [G loss: 0.945975]\n",
            "1097 [D loss: 0.579849, acc.: 68.75%] [G loss: 0.956349]\n",
            "1098 [D loss: 0.648477, acc.: 59.38%] [G loss: 0.928323]\n",
            "1099 [D loss: 0.578979, acc.: 65.62%] [G loss: 0.980533]\n",
            "1100 [D loss: 0.583493, acc.: 62.50%] [G loss: 1.055641]\n",
            "generated_data\n",
            "1101 [D loss: 0.600399, acc.: 65.62%] [G loss: 0.920537]\n",
            "1102 [D loss: 0.593195, acc.: 65.62%] [G loss: 0.903597]\n",
            "1103 [D loss: 0.592450, acc.: 64.06%] [G loss: 0.916347]\n",
            "1104 [D loss: 0.587557, acc.: 64.06%] [G loss: 0.902390]\n",
            "1105 [D loss: 0.587440, acc.: 65.62%] [G loss: 0.936065]\n",
            "1106 [D loss: 0.583800, acc.: 64.06%] [G loss: 0.927823]\n",
            "1107 [D loss: 0.578870, acc.: 64.06%] [G loss: 0.930744]\n",
            "1108 [D loss: 0.584842, acc.: 65.62%] [G loss: 0.915024]\n",
            "1109 [D loss: 0.587085, acc.: 65.62%] [G loss: 0.899593]\n",
            "1110 [D loss: 0.583613, acc.: 65.62%] [G loss: 0.899263]\n",
            "1111 [D loss: 0.598353, acc.: 60.94%] [G loss: 0.895424]\n",
            "1112 [D loss: 0.567637, acc.: 67.19%] [G loss: 0.921781]\n",
            "1113 [D loss: 0.586205, acc.: 62.50%] [G loss: 0.917714]\n",
            "1114 [D loss: 0.574820, acc.: 65.62%] [G loss: 0.920266]\n",
            "1115 [D loss: 0.588121, acc.: 64.06%] [G loss: 0.875072]\n",
            "1116 [D loss: 0.611520, acc.: 57.81%] [G loss: 0.910665]\n",
            "1117 [D loss: 0.596416, acc.: 60.94%] [G loss: 0.940068]\n",
            "1118 [D loss: 0.574894, acc.: 65.62%] [G loss: 0.942953]\n",
            "1119 [D loss: 0.589526, acc.: 64.06%] [G loss: 0.979172]\n",
            "1120 [D loss: 0.581593, acc.: 64.06%] [G loss: 0.963917]\n",
            "1121 [D loss: 0.601621, acc.: 62.50%] [G loss: 0.909107]\n",
            "1122 [D loss: 0.586261, acc.: 64.06%] [G loss: 0.902861]\n",
            "1123 [D loss: 0.585187, acc.: 64.06%] [G loss: 0.941714]\n",
            "1124 [D loss: 0.574909, acc.: 64.06%] [G loss: 0.957644]\n",
            "1125 [D loss: 0.586167, acc.: 64.06%] [G loss: 0.930556]\n",
            "1126 [D loss: 0.564591, acc.: 70.31%] [G loss: 0.938852]\n",
            "1127 [D loss: 0.589757, acc.: 62.50%] [G loss: 0.831293]\n",
            "1128 [D loss: 0.589103, acc.: 65.62%] [G loss: 0.921703]\n",
            "1129 [D loss: 0.577169, acc.: 64.06%] [G loss: 0.935558]\n",
            "1130 [D loss: 0.587215, acc.: 65.62%] [G loss: 0.912410]\n",
            "1131 [D loss: 0.615245, acc.: 62.50%] [G loss: 0.919574]\n",
            "1132 [D loss: 0.588400, acc.: 62.50%] [G loss: 1.176714]\n",
            "1133 [D loss: 0.672525, acc.: 64.06%] [G loss: 0.975536]\n",
            "1134 [D loss: 0.585086, acc.: 62.50%] [G loss: 0.943030]\n",
            "1135 [D loss: 0.588565, acc.: 65.62%] [G loss: 0.921650]\n",
            "1136 [D loss: 0.587839, acc.: 65.62%] [G loss: 0.927335]\n",
            "1137 [D loss: 0.577419, acc.: 65.62%] [G loss: 0.937830]\n",
            "1138 [D loss: 0.576057, acc.: 65.62%] [G loss: 0.914805]\n",
            "1139 [D loss: 0.576943, acc.: 62.50%] [G loss: 0.951600]\n",
            "1140 [D loss: 0.583213, acc.: 64.06%] [G loss: 0.936139]\n",
            "1141 [D loss: 0.580314, acc.: 64.06%] [G loss: 0.928759]\n",
            "1142 [D loss: 0.577125, acc.: 65.62%] [G loss: 0.926304]\n",
            "1143 [D loss: 0.578247, acc.: 65.62%] [G loss: 0.934759]\n",
            "1144 [D loss: 0.583267, acc.: 64.06%] [G loss: 0.899376]\n",
            "1145 [D loss: 0.581719, acc.: 60.94%] [G loss: 0.946539]\n",
            "1146 [D loss: 0.580859, acc.: 62.50%] [G loss: 0.897900]\n",
            "1147 [D loss: 0.584613, acc.: 65.62%] [G loss: 0.908658]\n",
            "1148 [D loss: 0.585531, acc.: 65.62%] [G loss: 0.925660]\n",
            "1149 [D loss: 0.585609, acc.: 62.50%] [G loss: 0.896147]\n",
            "1150 [D loss: 0.591107, acc.: 64.06%] [G loss: 0.912815]\n",
            "1151 [D loss: 0.579986, acc.: 64.06%] [G loss: 0.934545]\n",
            "1152 [D loss: 0.589657, acc.: 65.62%] [G loss: 0.924157]\n",
            "1153 [D loss: 0.577191, acc.: 67.19%] [G loss: 0.923397]\n",
            "1154 [D loss: 0.570369, acc.: 68.75%] [G loss: 0.941176]\n",
            "1155 [D loss: 0.566352, acc.: 70.31%] [G loss: 0.995686]\n",
            "1156 [D loss: 0.607342, acc.: 67.19%] [G loss: 0.871381]\n",
            "1157 [D loss: 0.594206, acc.: 65.62%] [G loss: 0.901711]\n",
            "1158 [D loss: 0.586487, acc.: 64.06%] [G loss: 0.874800]\n",
            "1159 [D loss: 0.621534, acc.: 64.06%] [G loss: 0.910716]\n",
            "1160 [D loss: 0.576019, acc.: 67.19%] [G loss: 0.915148]\n",
            "1161 [D loss: 0.576634, acc.: 67.19%] [G loss: 0.913561]\n",
            "1162 [D loss: 0.577992, acc.: 67.19%] [G loss: 0.896499]\n",
            "1163 [D loss: 0.632066, acc.: 64.06%] [G loss: 0.905496]\n",
            "1164 [D loss: 0.593536, acc.: 64.06%] [G loss: 0.883344]\n",
            "1165 [D loss: 0.579193, acc.: 65.62%] [G loss: 0.884010]\n",
            "1166 [D loss: 0.579963, acc.: 64.06%] [G loss: 0.928963]\n",
            "1167 [D loss: 0.597552, acc.: 64.06%] [G loss: 0.896709]\n",
            "1168 [D loss: 0.595542, acc.: 60.94%] [G loss: 0.895279]\n",
            "1169 [D loss: 0.593941, acc.: 62.50%] [G loss: 0.886183]\n",
            "1170 [D loss: 0.580105, acc.: 68.75%] [G loss: 0.898482]\n",
            "1171 [D loss: 0.608282, acc.: 56.25%] [G loss: 0.923037]\n",
            "1172 [D loss: 0.604531, acc.: 62.50%] [G loss: 0.913836]\n",
            "1173 [D loss: 0.596248, acc.: 62.50%] [G loss: 0.914922]\n",
            "1174 [D loss: 0.604760, acc.: 62.50%] [G loss: 0.905319]\n",
            "1175 [D loss: 0.593325, acc.: 62.50%] [G loss: 0.902905]\n",
            "1176 [D loss: 0.591258, acc.: 60.94%] [G loss: 0.955793]\n",
            "1177 [D loss: 0.584708, acc.: 62.50%] [G loss: 0.999940]\n",
            "1178 [D loss: 0.609375, acc.: 60.94%] [G loss: 0.927465]\n",
            "1179 [D loss: 0.596846, acc.: 62.50%] [G loss: 0.919368]\n",
            "1180 [D loss: 0.594342, acc.: 62.50%] [G loss: 0.901012]\n",
            "1181 [D loss: 0.589803, acc.: 62.50%] [G loss: 0.894134]\n",
            "1182 [D loss: 0.605375, acc.: 60.94%] [G loss: 0.912421]\n",
            "1183 [D loss: 0.594980, acc.: 62.50%] [G loss: 0.901240]\n",
            "1184 [D loss: 0.590510, acc.: 62.50%] [G loss: 0.975882]\n",
            "1185 [D loss: 0.609751, acc.: 59.38%] [G loss: 0.886595]\n",
            "1186 [D loss: 0.585973, acc.: 60.94%] [G loss: 0.918911]\n",
            "1187 [D loss: 0.603612, acc.: 60.94%] [G loss: 0.924223]\n",
            "1188 [D loss: 0.591799, acc.: 60.94%] [G loss: 0.866925]\n",
            "1189 [D loss: 0.618423, acc.: 60.94%] [G loss: 0.895261]\n",
            "1190 [D loss: 0.610160, acc.: 62.50%] [G loss: 0.900859]\n",
            "1191 [D loss: 0.585248, acc.: 62.50%] [G loss: 0.959175]\n",
            "1192 [D loss: 0.611998, acc.: 62.50%] [G loss: 0.918497]\n",
            "1193 [D loss: 0.595289, acc.: 62.50%] [G loss: 1.005999]\n",
            "1194 [D loss: 0.592007, acc.: 60.94%] [G loss: 0.987137]\n",
            "1195 [D loss: 0.618062, acc.: 60.94%] [G loss: 0.927959]\n",
            "1196 [D loss: 0.609114, acc.: 60.94%] [G loss: 0.907970]\n",
            "1197 [D loss: 0.590627, acc.: 62.50%] [G loss: 0.917926]\n",
            "1198 [D loss: 0.586254, acc.: 62.50%] [G loss: 0.913589]\n",
            "1199 [D loss: 0.592276, acc.: 62.50%] [G loss: 0.880796]\n",
            "1200 [D loss: 0.589052, acc.: 62.50%] [G loss: 0.906821]\n",
            "generated_data\n",
            "1201 [D loss: 0.595386, acc.: 62.50%] [G loss: 0.930055]\n",
            "1202 [D loss: 0.587703, acc.: 62.50%] [G loss: 0.945669]\n",
            "1203 [D loss: 0.597307, acc.: 62.50%] [G loss: 0.946594]\n",
            "1204 [D loss: 0.606679, acc.: 62.50%] [G loss: 0.921285]\n",
            "1205 [D loss: 0.602313, acc.: 62.50%] [G loss: 0.952992]\n",
            "1206 [D loss: 0.609743, acc.: 59.38%] [G loss: 0.896503]\n",
            "1207 [D loss: 0.584423, acc.: 62.50%] [G loss: 0.899619]\n",
            "1208 [D loss: 0.603471, acc.: 62.50%] [G loss: 0.890168]\n",
            "1209 [D loss: 0.598509, acc.: 62.50%] [G loss: 0.888142]\n",
            "1210 [D loss: 0.587411, acc.: 60.94%] [G loss: 0.872684]\n",
            "1211 [D loss: 0.602246, acc.: 60.94%] [G loss: 0.900665]\n",
            "1212 [D loss: 0.590886, acc.: 62.50%] [G loss: 0.893758]\n",
            "1213 [D loss: 0.614425, acc.: 62.50%] [G loss: 0.901479]\n",
            "1214 [D loss: 0.593175, acc.: 62.50%] [G loss: 0.890378]\n",
            "1215 [D loss: 0.592679, acc.: 64.06%] [G loss: 0.919294]\n",
            "1216 [D loss: 0.608459, acc.: 60.94%] [G loss: 0.874771]\n",
            "1217 [D loss: 0.590799, acc.: 64.06%] [G loss: 0.893763]\n",
            "1218 [D loss: 0.589359, acc.: 62.50%] [G loss: 0.904833]\n",
            "1219 [D loss: 0.582323, acc.: 62.50%] [G loss: 0.881730]\n",
            "1220 [D loss: 0.604017, acc.: 62.50%] [G loss: 0.894098]\n",
            "1221 [D loss: 0.591753, acc.: 60.94%] [G loss: 0.902426]\n",
            "1222 [D loss: 0.598079, acc.: 59.38%] [G loss: 0.933045]\n",
            "1223 [D loss: 0.594473, acc.: 64.06%] [G loss: 0.912205]\n",
            "1224 [D loss: 0.607928, acc.: 59.38%] [G loss: 0.910749]\n",
            "1225 [D loss: 0.589801, acc.: 62.50%] [G loss: 0.891963]\n",
            "1226 [D loss: 0.607616, acc.: 60.94%] [G loss: 0.927210]\n",
            "1227 [D loss: 0.610312, acc.: 60.94%] [G loss: 0.919260]\n",
            "1228 [D loss: 0.592866, acc.: 60.94%] [G loss: 0.906554]\n",
            "1229 [D loss: 0.605432, acc.: 62.50%] [G loss: 0.913118]\n",
            "1230 [D loss: 0.599539, acc.: 60.94%] [G loss: 0.899482]\n",
            "1231 [D loss: 0.599848, acc.: 62.50%] [G loss: 0.885442]\n",
            "1232 [D loss: 0.584667, acc.: 62.50%] [G loss: 0.946746]\n",
            "1233 [D loss: 0.606898, acc.: 60.94%] [G loss: 0.981124]\n",
            "1234 [D loss: 0.596618, acc.: 60.94%] [G loss: 0.885388]\n",
            "1235 [D loss: 0.595948, acc.: 62.50%] [G loss: 0.914335]\n",
            "1236 [D loss: 0.598010, acc.: 62.50%] [G loss: 0.915656]\n",
            "1237 [D loss: 0.600270, acc.: 60.94%] [G loss: 0.861764]\n",
            "1238 [D loss: 0.602204, acc.: 62.50%] [G loss: 0.908354]\n",
            "1239 [D loss: 0.620279, acc.: 62.50%] [G loss: 0.882812]\n",
            "1240 [D loss: 0.596294, acc.: 62.50%] [G loss: 0.896723]\n",
            "1241 [D loss: 0.608534, acc.: 62.50%] [G loss: 0.870955]\n",
            "1242 [D loss: 0.596748, acc.: 62.50%] [G loss: 0.938221]\n",
            "1243 [D loss: 0.578262, acc.: 62.50%] [G loss: 0.934946]\n",
            "1244 [D loss: 0.603139, acc.: 60.94%] [G loss: 0.920659]\n",
            "1245 [D loss: 0.603425, acc.: 60.94%] [G loss: 0.925286]\n",
            "1246 [D loss: 0.600159, acc.: 62.50%] [G loss: 0.891656]\n",
            "1247 [D loss: 0.587191, acc.: 62.50%] [G loss: 0.920201]\n",
            "1248 [D loss: 0.591858, acc.: 64.06%] [G loss: 0.909099]\n",
            "1249 [D loss: 0.592136, acc.: 64.06%] [G loss: 0.860395]\n",
            "1250 [D loss: 0.599941, acc.: 60.94%] [G loss: 0.857310]\n",
            "1251 [D loss: 0.598309, acc.: 64.06%] [G loss: 0.860269]\n",
            "1252 [D loss: 0.589421, acc.: 62.50%] [G loss: 0.916896]\n",
            "1253 [D loss: 0.600375, acc.: 64.06%] [G loss: 0.938256]\n",
            "1254 [D loss: 0.619692, acc.: 59.38%] [G loss: 0.882241]\n",
            "1255 [D loss: 0.598889, acc.: 60.94%] [G loss: 0.869857]\n",
            "1256 [D loss: 0.592394, acc.: 64.06%] [G loss: 0.902029]\n",
            "1257 [D loss: 0.587208, acc.: 62.50%] [G loss: 0.874031]\n",
            "1258 [D loss: 0.595741, acc.: 62.50%] [G loss: 0.858807]\n",
            "1259 [D loss: 0.598390, acc.: 62.50%] [G loss: 0.855733]\n",
            "1260 [D loss: 0.590270, acc.: 62.50%] [G loss: 0.871382]\n",
            "1261 [D loss: 0.592434, acc.: 59.38%] [G loss: 0.912847]\n",
            "1262 [D loss: 0.604265, acc.: 56.25%] [G loss: 0.940902]\n",
            "1263 [D loss: 0.606227, acc.: 62.50%] [G loss: 0.917944]\n",
            "1264 [D loss: 0.610952, acc.: 60.94%] [G loss: 0.900049]\n",
            "1265 [D loss: 0.610275, acc.: 57.81%] [G loss: 0.890255]\n",
            "1266 [D loss: 0.588215, acc.: 64.06%] [G loss: 0.914580]\n",
            "1267 [D loss: 0.583917, acc.: 64.06%] [G loss: 0.892877]\n",
            "1268 [D loss: 0.588633, acc.: 64.06%] [G loss: 0.898048]\n",
            "1269 [D loss: 0.613917, acc.: 57.81%] [G loss: 0.881645]\n",
            "1270 [D loss: 0.600339, acc.: 62.50%] [G loss: 0.896739]\n",
            "1271 [D loss: 0.586662, acc.: 64.06%] [G loss: 0.887244]\n",
            "1272 [D loss: 0.598186, acc.: 62.50%] [G loss: 0.881847]\n",
            "1273 [D loss: 0.588888, acc.: 62.50%] [G loss: 0.881793]\n",
            "1274 [D loss: 0.591225, acc.: 64.06%] [G loss: 0.876466]\n",
            "1275 [D loss: 0.591593, acc.: 62.50%] [G loss: 0.866797]\n",
            "1276 [D loss: 0.591136, acc.: 62.50%] [G loss: 0.892013]\n",
            "1277 [D loss: 0.609806, acc.: 60.94%] [G loss: 0.882214]\n",
            "1278 [D loss: 0.587285, acc.: 64.06%] [G loss: 0.869931]\n",
            "1279 [D loss: 0.595032, acc.: 62.50%] [G loss: 0.884558]\n",
            "1280 [D loss: 0.603084, acc.: 62.50%] [G loss: 0.884171]\n",
            "1281 [D loss: 0.616599, acc.: 59.38%] [G loss: 0.893648]\n",
            "1282 [D loss: 0.612524, acc.: 59.38%] [G loss: 0.902698]\n",
            "1283 [D loss: 0.603097, acc.: 60.94%] [G loss: 0.897525]\n",
            "1284 [D loss: 0.594926, acc.: 62.50%] [G loss: 0.888071]\n",
            "1285 [D loss: 0.599225, acc.: 62.50%] [G loss: 0.874718]\n",
            "1286 [D loss: 0.605411, acc.: 59.38%] [G loss: 0.878605]\n",
            "1287 [D loss: 0.593560, acc.: 62.50%] [G loss: 0.879150]\n",
            "1288 [D loss: 0.589192, acc.: 62.50%] [G loss: 0.862036]\n",
            "1289 [D loss: 0.607930, acc.: 56.25%] [G loss: 0.827132]\n",
            "1290 [D loss: 0.592910, acc.: 62.50%] [G loss: 0.899483]\n",
            "1291 [D loss: 0.596045, acc.: 62.50%] [G loss: 0.859806]\n",
            "1292 [D loss: 0.596761, acc.: 62.50%] [G loss: 0.871629]\n",
            "1293 [D loss: 0.605878, acc.: 60.94%] [G loss: 0.887261]\n",
            "1294 [D loss: 0.596750, acc.: 62.50%] [G loss: 0.876846]\n",
            "1295 [D loss: 0.590625, acc.: 64.06%] [G loss: 0.902010]\n",
            "1296 [D loss: 0.593123, acc.: 62.50%] [G loss: 0.864177]\n",
            "1297 [D loss: 0.603734, acc.: 62.50%] [G loss: 0.892707]\n",
            "1298 [D loss: 0.594291, acc.: 62.50%] [G loss: 0.869849]\n",
            "1299 [D loss: 0.601795, acc.: 60.94%] [G loss: 0.892482]\n",
            "1300 [D loss: 0.583139, acc.: 62.50%] [G loss: 0.911068]\n",
            "generated_data\n",
            "1301 [D loss: 0.597727, acc.: 62.50%] [G loss: 0.978966]\n",
            "1302 [D loss: 0.602123, acc.: 62.50%] [G loss: 0.902162]\n",
            "1303 [D loss: 0.594665, acc.: 64.06%] [G loss: 0.903366]\n",
            "1304 [D loss: 0.596684, acc.: 60.94%] [G loss: 0.891506]\n",
            "1305 [D loss: 0.604177, acc.: 59.38%] [G loss: 0.891842]\n",
            "1306 [D loss: 0.590360, acc.: 62.50%] [G loss: 0.915855]\n",
            "1307 [D loss: 0.586783, acc.: 60.94%] [G loss: 0.904155]\n",
            "1308 [D loss: 0.579407, acc.: 62.50%] [G loss: 0.894999]\n",
            "1309 [D loss: 0.588197, acc.: 62.50%] [G loss: 0.888842]\n",
            "1310 [D loss: 0.587627, acc.: 62.50%] [G loss: 0.898929]\n",
            "1311 [D loss: 0.583844, acc.: 62.50%] [G loss: 0.871801]\n",
            "1312 [D loss: 0.596534, acc.: 60.94%] [G loss: 0.882256]\n",
            "1313 [D loss: 0.602073, acc.: 60.94%] [G loss: 0.869480]\n",
            "1314 [D loss: 0.594896, acc.: 60.94%] [G loss: 0.891949]\n",
            "1315 [D loss: 0.595520, acc.: 59.38%] [G loss: 0.880761]\n",
            "1316 [D loss: 0.595759, acc.: 57.81%] [G loss: 0.857138]\n",
            "1317 [D loss: 0.605599, acc.: 56.25%] [G loss: 0.872692]\n",
            "1318 [D loss: 0.615266, acc.: 59.38%] [G loss: 0.886522]\n",
            "1319 [D loss: 0.608653, acc.: 60.94%] [G loss: 0.891315]\n",
            "1320 [D loss: 0.604757, acc.: 60.94%] [G loss: 0.879716]\n",
            "1321 [D loss: 0.603116, acc.: 60.94%] [G loss: 0.913521]\n",
            "1322 [D loss: 0.597128, acc.: 62.50%] [G loss: 0.877687]\n",
            "1323 [D loss: 0.598948, acc.: 62.50%] [G loss: 0.926290]\n",
            "1324 [D loss: 0.593850, acc.: 62.50%] [G loss: 0.937075]\n",
            "1325 [D loss: 0.577979, acc.: 64.06%] [G loss: 0.961931]\n",
            "1326 [D loss: 0.595100, acc.: 62.50%] [G loss: 0.918645]\n",
            "1327 [D loss: 0.583145, acc.: 64.06%] [G loss: 0.969229]\n",
            "1328 [D loss: 0.596285, acc.: 64.06%] [G loss: 0.918022]\n",
            "1329 [D loss: 0.591891, acc.: 64.06%] [G loss: 0.934634]\n",
            "1330 [D loss: 0.587133, acc.: 64.06%] [G loss: 0.907320]\n",
            "1331 [D loss: 0.577445, acc.: 64.06%] [G loss: 0.927438]\n",
            "1332 [D loss: 0.586796, acc.: 64.06%] [G loss: 0.909551]\n",
            "1333 [D loss: 0.573709, acc.: 64.06%] [G loss: 0.979635]\n",
            "1334 [D loss: 0.567183, acc.: 65.62%] [G loss: 0.955653]\n",
            "1335 [D loss: 0.581870, acc.: 67.19%] [G loss: 0.956003]\n",
            "1336 [D loss: 0.571050, acc.: 65.62%] [G loss: 0.937659]\n",
            "1337 [D loss: 0.590790, acc.: 65.62%] [G loss: 0.946763]\n",
            "1338 [D loss: 0.571305, acc.: 65.62%] [G loss: 0.945247]\n",
            "1339 [D loss: 0.573583, acc.: 64.06%] [G loss: 0.930163]\n",
            "1340 [D loss: 0.589619, acc.: 64.06%] [G loss: 0.953366]\n",
            "1341 [D loss: 0.576425, acc.: 64.06%] [G loss: 0.919886]\n",
            "1342 [D loss: 0.582598, acc.: 62.50%] [G loss: 0.907643]\n",
            "1343 [D loss: 0.575429, acc.: 62.50%] [G loss: 0.900334]\n",
            "1344 [D loss: 0.586034, acc.: 64.06%] [G loss: 0.873917]\n",
            "1345 [D loss: 0.612819, acc.: 57.81%] [G loss: 0.934763]\n",
            "1346 [D loss: 0.583668, acc.: 65.62%] [G loss: 0.988925]\n",
            "1347 [D loss: 0.608852, acc.: 62.50%] [G loss: 0.930600]\n",
            "1348 [D loss: 0.579808, acc.: 64.06%] [G loss: 0.908003]\n",
            "1349 [D loss: 0.601019, acc.: 62.50%] [G loss: 0.904241]\n",
            "1350 [D loss: 0.601381, acc.: 64.06%] [G loss: 0.906503]\n",
            "1351 [D loss: 0.599547, acc.: 64.06%] [G loss: 0.923717]\n",
            "1352 [D loss: 0.605425, acc.: 64.06%] [G loss: 0.928792]\n",
            "1353 [D loss: 0.652356, acc.: 60.94%] [G loss: 0.888887]\n",
            "1354 [D loss: 0.601204, acc.: 60.94%] [G loss: 0.938700]\n",
            "1355 [D loss: 0.578491, acc.: 62.50%] [G loss: 0.992855]\n",
            "1356 [D loss: 0.588351, acc.: 62.50%] [G loss: 0.976263]\n",
            "1357 [D loss: 0.594443, acc.: 62.50%] [G loss: 0.991889]\n",
            "1358 [D loss: 0.575608, acc.: 62.50%] [G loss: 0.986185]\n",
            "1359 [D loss: 0.579743, acc.: 64.06%] [G loss: 0.949760]\n",
            "1360 [D loss: 0.581881, acc.: 64.06%] [G loss: 0.965705]\n",
            "1361 [D loss: 0.576687, acc.: 67.19%] [G loss: 1.021908]\n",
            "1362 [D loss: 0.594882, acc.: 60.94%] [G loss: 0.945105]\n",
            "1363 [D loss: 0.584278, acc.: 65.62%] [G loss: 1.022977]\n",
            "1364 [D loss: 0.585333, acc.: 65.62%] [G loss: 0.988703]\n",
            "1365 [D loss: 0.579267, acc.: 67.19%] [G loss: 0.935068]\n",
            "1366 [D loss: 0.591104, acc.: 68.75%] [G loss: 0.919583]\n",
            "1367 [D loss: 0.579394, acc.: 70.31%] [G loss: 0.899315]\n",
            "1368 [D loss: 0.577895, acc.: 68.75%] [G loss: 0.918419]\n",
            "1369 [D loss: 0.595940, acc.: 68.75%] [G loss: 0.907908]\n",
            "1370 [D loss: 0.582213, acc.: 68.75%] [G loss: 0.845398]\n",
            "1371 [D loss: 0.600995, acc.: 64.06%] [G loss: 0.879787]\n",
            "1372 [D loss: 0.597985, acc.: 62.50%] [G loss: 0.856655]\n",
            "1373 [D loss: 0.607147, acc.: 59.38%] [G loss: 0.865084]\n",
            "1374 [D loss: 0.598109, acc.: 60.94%] [G loss: 0.872503]\n",
            "1375 [D loss: 0.609280, acc.: 64.06%] [G loss: 0.844889]\n",
            "1376 [D loss: 0.613648, acc.: 60.94%] [G loss: 0.881317]\n",
            "1377 [D loss: 0.595677, acc.: 62.50%] [G loss: 0.870597]\n",
            "1378 [D loss: 0.592597, acc.: 60.94%] [G loss: 0.909266]\n",
            "1379 [D loss: 0.611845, acc.: 60.94%] [G loss: 0.902397]\n",
            "1380 [D loss: 0.598864, acc.: 65.62%] [G loss: 0.886543]\n",
            "1381 [D loss: 0.607552, acc.: 64.06%] [G loss: 0.910757]\n",
            "1382 [D loss: 0.594647, acc.: 60.94%] [G loss: 0.917259]\n",
            "1383 [D loss: 0.611943, acc.: 53.12%] [G loss: 0.943823]\n",
            "1384 [D loss: 0.587001, acc.: 65.62%] [G loss: 0.925532]\n",
            "1385 [D loss: 0.589638, acc.: 65.62%] [G loss: 0.923476]\n",
            "1386 [D loss: 0.605999, acc.: 59.38%] [G loss: 0.939706]\n",
            "1387 [D loss: 0.603257, acc.: 60.94%] [G loss: 0.903078]\n",
            "1388 [D loss: 0.611154, acc.: 56.25%] [G loss: 0.935065]\n",
            "1389 [D loss: 0.569961, acc.: 62.50%] [G loss: 1.005852]\n",
            "1390 [D loss: 0.587450, acc.: 59.38%] [G loss: 0.897060]\n",
            "1391 [D loss: 0.625247, acc.: 60.94%] [G loss: 1.000503]\n",
            "1392 [D loss: 0.604413, acc.: 59.38%] [G loss: 0.876601]\n",
            "1393 [D loss: 0.599511, acc.: 59.38%] [G loss: 0.884872]\n",
            "1394 [D loss: 0.588006, acc.: 64.06%] [G loss: 0.903173]\n",
            "1395 [D loss: 0.595703, acc.: 64.06%] [G loss: 0.860552]\n",
            "1396 [D loss: 0.590689, acc.: 67.19%] [G loss: 0.904368]\n",
            "1397 [D loss: 0.620658, acc.: 59.38%] [G loss: 0.861902]\n",
            "1398 [D loss: 0.599181, acc.: 62.50%] [G loss: 0.885861]\n",
            "1399 [D loss: 0.585785, acc.: 64.06%] [G loss: 0.901938]\n",
            "1400 [D loss: 0.589604, acc.: 65.62%] [G loss: 0.868956]\n",
            "generated_data\n",
            "1401 [D loss: 0.593160, acc.: 64.06%] [G loss: 0.847601]\n",
            "1402 [D loss: 0.595885, acc.: 62.50%] [G loss: 0.872773]\n",
            "1403 [D loss: 0.591783, acc.: 62.50%] [G loss: 0.878402]\n",
            "1404 [D loss: 0.618056, acc.: 59.38%] [G loss: 0.887910]\n",
            "1405 [D loss: 0.604515, acc.: 59.38%] [G loss: 0.894630]\n",
            "1406 [D loss: 0.615472, acc.: 59.38%] [G loss: 0.874472]\n",
            "1407 [D loss: 0.591929, acc.: 64.06%] [G loss: 0.844800]\n",
            "1408 [D loss: 0.600892, acc.: 60.94%] [G loss: 0.869798]\n",
            "1409 [D loss: 0.595719, acc.: 62.50%] [G loss: 0.878654]\n",
            "1410 [D loss: 0.591027, acc.: 60.94%] [G loss: 0.912007]\n",
            "1411 [D loss: 0.592922, acc.: 62.50%] [G loss: 0.880145]\n",
            "1412 [D loss: 0.593468, acc.: 59.38%] [G loss: 0.874044]\n",
            "1413 [D loss: 0.586506, acc.: 62.50%] [G loss: 0.917944]\n",
            "1414 [D loss: 0.593233, acc.: 62.50%] [G loss: 0.899929]\n",
            "1415 [D loss: 0.594172, acc.: 64.06%] [G loss: 0.891123]\n",
            "1416 [D loss: 0.579754, acc.: 62.50%] [G loss: 0.912802]\n",
            "1417 [D loss: 0.597527, acc.: 62.50%] [G loss: 0.868917]\n",
            "1418 [D loss: 0.592184, acc.: 64.06%] [G loss: 0.860324]\n",
            "1419 [D loss: 0.600739, acc.: 62.50%] [G loss: 0.890024]\n",
            "1420 [D loss: 0.621709, acc.: 54.69%] [G loss: 0.859792]\n",
            "1421 [D loss: 0.605660, acc.: 57.81%] [G loss: 0.892179]\n",
            "1422 [D loss: 0.595339, acc.: 62.50%] [G loss: 0.901780]\n",
            "1423 [D loss: 0.596443, acc.: 62.50%] [G loss: 0.891517]\n",
            "1424 [D loss: 0.590900, acc.: 62.50%] [G loss: 0.880241]\n",
            "1425 [D loss: 0.595939, acc.: 57.81%] [G loss: 0.891143]\n",
            "1426 [D loss: 0.587725, acc.: 65.62%] [G loss: 0.899190]\n",
            "1427 [D loss: 0.586835, acc.: 65.62%] [G loss: 0.874916]\n",
            "1428 [D loss: 0.600787, acc.: 64.06%] [G loss: 0.879022]\n",
            "1429 [D loss: 0.596468, acc.: 62.50%] [G loss: 0.874387]\n",
            "1430 [D loss: 0.604683, acc.: 64.06%] [G loss: 0.866984]\n",
            "1431 [D loss: 0.595245, acc.: 64.06%] [G loss: 0.896640]\n",
            "1432 [D loss: 0.605216, acc.: 62.50%] [G loss: 0.915445]\n",
            "1433 [D loss: 0.594111, acc.: 64.06%] [G loss: 0.883985]\n",
            "1434 [D loss: 0.595276, acc.: 62.50%] [G loss: 0.884371]\n",
            "1435 [D loss: 0.606415, acc.: 60.94%] [G loss: 0.899394]\n",
            "1436 [D loss: 0.588785, acc.: 60.94%] [G loss: 0.898845]\n",
            "1437 [D loss: 0.592519, acc.: 62.50%] [G loss: 0.891782]\n",
            "1438 [D loss: 0.594811, acc.: 62.50%] [G loss: 0.864637]\n",
            "1439 [D loss: 0.593275, acc.: 64.06%] [G loss: 0.870225]\n",
            "1440 [D loss: 0.598774, acc.: 62.50%] [G loss: 0.887899]\n",
            "1441 [D loss: 0.588202, acc.: 62.50%] [G loss: 0.875411]\n",
            "1442 [D loss: 0.590854, acc.: 62.50%] [G loss: 0.875545]\n",
            "1443 [D loss: 0.594618, acc.: 60.94%] [G loss: 0.876799]\n",
            "1444 [D loss: 0.592126, acc.: 64.06%] [G loss: 0.895812]\n",
            "1445 [D loss: 0.606607, acc.: 60.94%] [G loss: 0.855809]\n",
            "1446 [D loss: 0.593923, acc.: 64.06%] [G loss: 0.870270]\n",
            "1447 [D loss: 0.590439, acc.: 60.94%] [G loss: 0.869523]\n",
            "1448 [D loss: 0.599086, acc.: 64.06%] [G loss: 0.889329]\n",
            "1449 [D loss: 0.585900, acc.: 64.06%] [G loss: 0.857223]\n",
            "1450 [D loss: 0.599984, acc.: 60.94%] [G loss: 0.824199]\n",
            "1451 [D loss: 0.599425, acc.: 62.50%] [G loss: 0.868294]\n",
            "1452 [D loss: 0.592459, acc.: 60.94%] [G loss: 0.893960]\n",
            "1453 [D loss: 0.626973, acc.: 60.94%] [G loss: 0.866738]\n",
            "1454 [D loss: 0.578826, acc.: 62.50%] [G loss: 0.922794]\n",
            "1455 [D loss: 0.617557, acc.: 57.81%] [G loss: 0.874752]\n",
            "1456 [D loss: 0.611944, acc.: 62.50%] [G loss: 0.851828]\n",
            "1457 [D loss: 0.573085, acc.: 64.06%] [G loss: 0.865186]\n",
            "1458 [D loss: 0.578826, acc.: 62.50%] [G loss: 0.899566]\n",
            "1459 [D loss: 0.579281, acc.: 62.50%] [G loss: 0.904963]\n",
            "1460 [D loss: 0.599231, acc.: 62.50%] [G loss: 0.868421]\n",
            "1461 [D loss: 0.598684, acc.: 60.94%] [G loss: 0.868757]\n",
            "1462 [D loss: 0.583418, acc.: 64.06%] [G loss: 0.927593]\n",
            "1463 [D loss: 0.593069, acc.: 60.94%] [G loss: 0.861194]\n",
            "1464 [D loss: 0.606940, acc.: 62.50%] [G loss: 0.855386]\n",
            "1465 [D loss: 0.601734, acc.: 60.94%] [G loss: 0.846404]\n",
            "1466 [D loss: 0.591326, acc.: 62.50%] [G loss: 0.862596]\n",
            "1467 [D loss: 0.580021, acc.: 62.50%] [G loss: 0.853194]\n",
            "1468 [D loss: 0.578799, acc.: 64.06%] [G loss: 0.857730]\n",
            "1469 [D loss: 0.584047, acc.: 64.06%] [G loss: 0.867220]\n",
            "1470 [D loss: 0.590194, acc.: 62.50%] [G loss: 0.894113]\n",
            "1471 [D loss: 0.596890, acc.: 64.06%] [G loss: 0.847316]\n",
            "1472 [D loss: 0.577930, acc.: 67.19%] [G loss: 0.886197]\n",
            "1473 [D loss: 0.586324, acc.: 62.50%] [G loss: 0.849105]\n",
            "1474 [D loss: 0.577885, acc.: 64.06%] [G loss: 0.858916]\n",
            "1475 [D loss: 0.594356, acc.: 64.06%] [G loss: 0.838063]\n",
            "1476 [D loss: 0.585509, acc.: 65.62%] [G loss: 0.838372]\n",
            "1477 [D loss: 0.609331, acc.: 60.94%] [G loss: 0.825890]\n",
            "1478 [D loss: 0.604538, acc.: 59.38%] [G loss: 0.843581]\n",
            "1479 [D loss: 0.591543, acc.: 62.50%] [G loss: 0.874587]\n",
            "1480 [D loss: 0.580491, acc.: 65.62%] [G loss: 0.849709]\n",
            "1481 [D loss: 0.589718, acc.: 62.50%] [G loss: 0.859541]\n",
            "1482 [D loss: 0.602584, acc.: 60.94%] [G loss: 0.862252]\n",
            "1483 [D loss: 0.598592, acc.: 65.62%] [G loss: 0.856905]\n",
            "1484 [D loss: 0.583487, acc.: 64.06%] [G loss: 0.903449]\n",
            "1485 [D loss: 0.603163, acc.: 64.06%] [G loss: 0.947706]\n",
            "1486 [D loss: 0.604493, acc.: 64.06%] [G loss: 0.854671]\n",
            "1487 [D loss: 0.593672, acc.: 59.38%] [G loss: 0.846161]\n",
            "1488 [D loss: 0.599855, acc.: 64.06%] [G loss: 0.878942]\n",
            "1489 [D loss: 0.603649, acc.: 62.50%] [G loss: 0.874665]\n",
            "1490 [D loss: 0.588011, acc.: 64.06%] [G loss: 0.898730]\n",
            "1491 [D loss: 0.598634, acc.: 62.50%] [G loss: 0.890053]\n",
            "1492 [D loss: 0.597635, acc.: 60.94%] [G loss: 0.880878]\n",
            "1493 [D loss: 0.587347, acc.: 64.06%] [G loss: 0.883406]\n",
            "1494 [D loss: 0.577518, acc.: 64.06%] [G loss: 0.963666]\n",
            "1495 [D loss: 0.578157, acc.: 64.06%] [G loss: 0.972479]\n",
            "1496 [D loss: 0.584188, acc.: 62.50%] [G loss: 0.971749]\n",
            "1497 [D loss: 0.604728, acc.: 60.94%] [G loss: 0.893449]\n",
            "1498 [D loss: 0.593883, acc.: 62.50%] [G loss: 0.880208]\n",
            "1499 [D loss: 0.588954, acc.: 64.06%] [G loss: 0.876501]\n",
            "1500 [D loss: 0.592336, acc.: 62.50%] [G loss: 0.896469]\n",
            "generated_data\n",
            "1501 [D loss: 0.589064, acc.: 64.06%] [G loss: 0.897760]\n",
            "1502 [D loss: 0.592797, acc.: 62.50%] [G loss: 0.893983]\n",
            "1503 [D loss: 0.582625, acc.: 67.19%] [G loss: 0.868295]\n",
            "1504 [D loss: 0.582176, acc.: 60.94%] [G loss: 0.890473]\n",
            "1505 [D loss: 0.595254, acc.: 65.62%] [G loss: 0.880583]\n",
            "1506 [D loss: 0.593756, acc.: 68.75%] [G loss: 0.861538]\n",
            "1507 [D loss: 0.604801, acc.: 62.50%] [G loss: 0.888338]\n",
            "1508 [D loss: 0.596714, acc.: 62.50%] [G loss: 0.879891]\n",
            "1509 [D loss: 0.589030, acc.: 60.94%] [G loss: 0.880947]\n",
            "1510 [D loss: 0.575944, acc.: 64.06%] [G loss: 0.908984]\n",
            "1511 [D loss: 0.609121, acc.: 60.94%] [G loss: 0.875004]\n",
            "1512 [D loss: 0.578191, acc.: 65.62%] [G loss: 0.899928]\n",
            "1513 [D loss: 0.581797, acc.: 64.06%] [G loss: 0.944705]\n",
            "1514 [D loss: 0.587337, acc.: 64.06%] [G loss: 0.896105]\n",
            "1515 [D loss: 0.611307, acc.: 62.50%] [G loss: 0.874140]\n",
            "1516 [D loss: 0.592095, acc.: 67.19%] [G loss: 0.902720]\n",
            "1517 [D loss: 0.579193, acc.: 65.62%] [G loss: 0.930980]\n",
            "1518 [D loss: 0.588869, acc.: 64.06%] [G loss: 0.866910]\n",
            "1519 [D loss: 0.591964, acc.: 64.06%] [G loss: 0.866104]\n",
            "1520 [D loss: 0.582164, acc.: 67.19%] [G loss: 0.866092]\n",
            "1521 [D loss: 0.591833, acc.: 60.94%] [G loss: 0.953301]\n",
            "1522 [D loss: 0.586115, acc.: 67.19%] [G loss: 0.900081]\n",
            "1523 [D loss: 0.592084, acc.: 64.06%] [G loss: 0.898780]\n",
            "1524 [D loss: 0.592020, acc.: 62.50%] [G loss: 0.879811]\n",
            "1525 [D loss: 0.601272, acc.: 62.50%] [G loss: 0.925463]\n",
            "1526 [D loss: 0.583968, acc.: 64.06%] [G loss: 0.920660]\n",
            "1527 [D loss: 0.582830, acc.: 64.06%] [G loss: 0.922727]\n",
            "1528 [D loss: 0.612783, acc.: 62.50%] [G loss: 0.904841]\n",
            "1529 [D loss: 0.615714, acc.: 59.38%] [G loss: 0.893407]\n",
            "1530 [D loss: 0.572970, acc.: 64.06%] [G loss: 0.922086]\n",
            "1531 [D loss: 0.586893, acc.: 57.81%] [G loss: 0.945239]\n",
            "1532 [D loss: 0.600892, acc.: 57.81%] [G loss: 0.940855]\n",
            "1533 [D loss: 0.600070, acc.: 64.06%] [G loss: 0.853380]\n",
            "1534 [D loss: 0.586424, acc.: 64.06%] [G loss: 0.883545]\n",
            "1535 [D loss: 0.597352, acc.: 64.06%] [G loss: 0.893045]\n",
            "1536 [D loss: 0.576332, acc.: 64.06%] [G loss: 0.906405]\n",
            "1537 [D loss: 0.588848, acc.: 62.50%] [G loss: 0.957116]\n",
            "1538 [D loss: 0.587434, acc.: 65.62%] [G loss: 0.944741]\n",
            "1539 [D loss: 0.599845, acc.: 60.94%] [G loss: 0.914155]\n",
            "1540 [D loss: 0.586034, acc.: 65.62%] [G loss: 0.910080]\n",
            "1541 [D loss: 0.570172, acc.: 64.06%] [G loss: 0.849169]\n",
            "1542 [D loss: 0.661772, acc.: 60.94%] [G loss: 0.890954]\n",
            "1543 [D loss: 0.610573, acc.: 62.50%] [G loss: 0.856872]\n",
            "1544 [D loss: 0.602050, acc.: 60.94%] [G loss: 0.878304]\n",
            "1545 [D loss: 0.595055, acc.: 60.94%] [G loss: 0.865870]\n",
            "1546 [D loss: 0.596100, acc.: 59.38%] [G loss: 0.913531]\n",
            "1547 [D loss: 0.609108, acc.: 62.50%] [G loss: 0.898918]\n",
            "1548 [D loss: 0.583427, acc.: 62.50%] [G loss: 0.885427]\n",
            "1549 [D loss: 0.613947, acc.: 57.81%] [G loss: 0.865317]\n",
            "1550 [D loss: 0.600126, acc.: 60.94%] [G loss: 0.944229]\n",
            "1551 [D loss: 0.588151, acc.: 62.50%] [G loss: 0.876034]\n",
            "1552 [D loss: 0.597652, acc.: 62.50%] [G loss: 0.896138]\n",
            "1553 [D loss: 0.586427, acc.: 65.62%] [G loss: 0.900635]\n",
            "1554 [D loss: 0.588633, acc.: 62.50%] [G loss: 0.901680]\n",
            "1555 [D loss: 0.569686, acc.: 64.06%] [G loss: 0.916229]\n",
            "1556 [D loss: 0.585170, acc.: 65.62%] [G loss: 0.869888]\n",
            "1557 [D loss: 0.585133, acc.: 62.50%] [G loss: 0.881522]\n",
            "1558 [D loss: 0.586694, acc.: 65.62%] [G loss: 0.895606]\n",
            "1559 [D loss: 0.591855, acc.: 64.06%] [G loss: 0.882365]\n",
            "1560 [D loss: 0.591186, acc.: 64.06%] [G loss: 0.852884]\n",
            "1561 [D loss: 0.599222, acc.: 64.06%] [G loss: 0.901709]\n",
            "1562 [D loss: 0.600186, acc.: 62.50%] [G loss: 0.889560]\n",
            "1563 [D loss: 0.587728, acc.: 62.50%] [G loss: 0.880072]\n",
            "1564 [D loss: 0.582721, acc.: 67.19%] [G loss: 0.887557]\n",
            "1565 [D loss: 0.607257, acc.: 57.81%] [G loss: 0.880470]\n",
            "1566 [D loss: 0.590545, acc.: 62.50%] [G loss: 0.876898]\n",
            "1567 [D loss: 0.591277, acc.: 64.06%] [G loss: 0.862456]\n",
            "1568 [D loss: 0.609155, acc.: 59.38%] [G loss: 0.892725]\n",
            "1569 [D loss: 0.583837, acc.: 65.62%] [G loss: 0.867496]\n",
            "1570 [D loss: 0.603006, acc.: 60.94%] [G loss: 0.871286]\n",
            "1571 [D loss: 0.581911, acc.: 62.50%] [G loss: 0.872005]\n",
            "1572 [D loss: 0.601840, acc.: 62.50%] [G loss: 0.869512]\n",
            "1573 [D loss: 0.603977, acc.: 60.94%] [G loss: 0.891097]\n",
            "1574 [D loss: 0.591130, acc.: 62.50%] [G loss: 0.862445]\n",
            "1575 [D loss: 0.586066, acc.: 64.06%] [G loss: 0.861616]\n",
            "1576 [D loss: 0.590315, acc.: 62.50%] [G loss: 0.853505]\n",
            "1577 [D loss: 0.585047, acc.: 64.06%] [G loss: 0.887598]\n",
            "1578 [D loss: 0.600108, acc.: 62.50%] [G loss: 0.862640]\n",
            "1579 [D loss: 0.598082, acc.: 62.50%] [G loss: 0.902852]\n",
            "1580 [D loss: 0.590292, acc.: 64.06%] [G loss: 0.929299]\n",
            "1581 [D loss: 0.594525, acc.: 64.06%] [G loss: 0.889197]\n",
            "1582 [D loss: 0.577888, acc.: 64.06%] [G loss: 0.910074]\n",
            "1583 [D loss: 0.573897, acc.: 67.19%] [G loss: 0.923903]\n",
            "1584 [D loss: 0.573662, acc.: 64.06%] [G loss: 0.911120]\n",
            "1585 [D loss: 0.615855, acc.: 59.38%] [G loss: 0.916505]\n",
            "1586 [D loss: 0.592084, acc.: 60.94%] [G loss: 0.938439]\n",
            "1587 [D loss: 0.584886, acc.: 64.06%] [G loss: 0.953735]\n",
            "1588 [D loss: 0.602202, acc.: 62.50%] [G loss: 0.899574]\n",
            "1589 [D loss: 0.591707, acc.: 64.06%] [G loss: 0.937281]\n",
            "1590 [D loss: 0.624911, acc.: 56.25%] [G loss: 0.919333]\n",
            "1591 [D loss: 0.606752, acc.: 60.94%] [G loss: 0.923717]\n",
            "1592 [D loss: 0.603101, acc.: 62.50%] [G loss: 0.902718]\n",
            "1593 [D loss: 0.592551, acc.: 60.94%] [G loss: 0.898342]\n",
            "1594 [D loss: 0.595463, acc.: 62.50%] [G loss: 0.896154]\n",
            "1595 [D loss: 0.590998, acc.: 62.50%] [G loss: 0.872880]\n",
            "1596 [D loss: 0.604609, acc.: 59.38%] [G loss: 0.893737]\n",
            "1597 [D loss: 0.584805, acc.: 64.06%] [G loss: 0.903589]\n",
            "1598 [D loss: 0.589090, acc.: 60.94%] [G loss: 0.905524]\n",
            "1599 [D loss: 0.588520, acc.: 64.06%] [G loss: 0.895319]\n",
            "1600 [D loss: 0.577035, acc.: 67.19%] [G loss: 0.872319]\n",
            "generated_data\n",
            "1601 [D loss: 0.592887, acc.: 62.50%] [G loss: 0.894732]\n",
            "1602 [D loss: 0.581189, acc.: 65.62%] [G loss: 0.875697]\n",
            "1603 [D loss: 0.571628, acc.: 65.62%] [G loss: 0.938869]\n",
            "1604 [D loss: 0.575851, acc.: 65.62%] [G loss: 0.889794]\n",
            "1605 [D loss: 0.598190, acc.: 57.81%] [G loss: 0.901404]\n",
            "1606 [D loss: 0.588810, acc.: 62.50%] [G loss: 0.855613]\n",
            "1607 [D loss: 0.601446, acc.: 64.06%] [G loss: 0.867419]\n",
            "1608 [D loss: 0.583970, acc.: 67.19%] [G loss: 0.903065]\n",
            "1609 [D loss: 0.619987, acc.: 54.69%] [G loss: 0.865856]\n",
            "1610 [D loss: 0.599800, acc.: 62.50%] [G loss: 0.847227]\n",
            "1611 [D loss: 0.595441, acc.: 65.62%] [G loss: 0.869242]\n",
            "1612 [D loss: 0.588304, acc.: 62.50%] [G loss: 0.847652]\n",
            "1613 [D loss: 0.596181, acc.: 59.38%] [G loss: 0.861126]\n",
            "1614 [D loss: 0.598182, acc.: 59.38%] [G loss: 0.844739]\n",
            "1615 [D loss: 0.623128, acc.: 50.00%] [G loss: 0.842441]\n",
            "1616 [D loss: 0.599409, acc.: 60.94%] [G loss: 0.890458]\n",
            "1617 [D loss: 0.613085, acc.: 60.94%] [G loss: 0.952986]\n",
            "1618 [D loss: 0.605280, acc.: 62.50%] [G loss: 0.952097]\n",
            "1619 [D loss: 0.603733, acc.: 62.50%] [G loss: 0.909294]\n",
            "1620 [D loss: 0.615916, acc.: 62.50%] [G loss: 0.923848]\n",
            "1621 [D loss: 0.608382, acc.: 62.50%] [G loss: 0.956250]\n",
            "1622 [D loss: 0.620984, acc.: 62.50%] [G loss: 0.960440]\n",
            "1623 [D loss: 0.602945, acc.: 62.50%] [G loss: 0.992463]\n",
            "1624 [D loss: 0.594892, acc.: 62.50%] [G loss: 0.930177]\n",
            "1625 [D loss: 0.592467, acc.: 62.50%] [G loss: 0.961087]\n",
            "1626 [D loss: 0.589681, acc.: 64.06%] [G loss: 0.981117]\n",
            "1627 [D loss: 0.610746, acc.: 60.94%] [G loss: 0.926123]\n",
            "1628 [D loss: 0.599485, acc.: 62.50%] [G loss: 0.894020]\n",
            "1629 [D loss: 0.613587, acc.: 62.50%] [G loss: 0.910872]\n",
            "1630 [D loss: 0.606392, acc.: 62.50%] [G loss: 0.930360]\n",
            "1631 [D loss: 0.594367, acc.: 62.50%] [G loss: 0.930507]\n",
            "1632 [D loss: 0.600150, acc.: 62.50%] [G loss: 0.983165]\n",
            "1633 [D loss: 0.594490, acc.: 62.50%] [G loss: 0.923124]\n",
            "1634 [D loss: 0.597913, acc.: 62.50%] [G loss: 0.910207]\n",
            "1635 [D loss: 0.596711, acc.: 62.50%] [G loss: 0.894575]\n",
            "1636 [D loss: 0.598018, acc.: 62.50%] [G loss: 0.913754]\n",
            "1637 [D loss: 0.594563, acc.: 62.50%] [G loss: 0.896871]\n",
            "1638 [D loss: 0.595607, acc.: 62.50%] [G loss: 0.907615]\n",
            "1639 [D loss: 0.594119, acc.: 62.50%] [G loss: 0.925849]\n",
            "1640 [D loss: 0.598129, acc.: 62.50%] [G loss: 0.893045]\n",
            "1641 [D loss: 0.600519, acc.: 62.50%] [G loss: 0.877188]\n",
            "1642 [D loss: 0.597460, acc.: 62.50%] [G loss: 0.893694]\n",
            "1643 [D loss: 0.592454, acc.: 60.94%] [G loss: 0.866268]\n",
            "1644 [D loss: 0.605733, acc.: 57.81%] [G loss: 0.877877]\n",
            "1645 [D loss: 0.591266, acc.: 62.50%] [G loss: 0.883178]\n",
            "1646 [D loss: 0.602692, acc.: 62.50%] [G loss: 0.896532]\n",
            "1647 [D loss: 0.609048, acc.: 59.38%] [G loss: 0.919538]\n",
            "1648 [D loss: 0.599234, acc.: 62.50%] [G loss: 0.914439]\n",
            "1649 [D loss: 0.607267, acc.: 62.50%] [G loss: 0.883141]\n",
            "1650 [D loss: 0.594251, acc.: 62.50%] [G loss: 0.893542]\n",
            "1651 [D loss: 0.613147, acc.: 59.38%] [G loss: 0.994263]\n",
            "1652 [D loss: 0.598380, acc.: 60.94%] [G loss: 0.913097]\n",
            "1653 [D loss: 0.606356, acc.: 62.50%] [G loss: 0.912234]\n",
            "1654 [D loss: 0.592318, acc.: 62.50%] [G loss: 0.951499]\n",
            "1655 [D loss: 0.590273, acc.: 62.50%] [G loss: 0.913505]\n",
            "1656 [D loss: 0.589456, acc.: 62.50%] [G loss: 0.920491]\n",
            "1657 [D loss: 0.592931, acc.: 64.06%] [G loss: 0.938708]\n",
            "1658 [D loss: 0.600862, acc.: 62.50%] [G loss: 0.905918]\n",
            "1659 [D loss: 0.618737, acc.: 60.94%] [G loss: 0.892502]\n",
            "1660 [D loss: 0.607043, acc.: 60.94%] [G loss: 0.893726]\n",
            "1661 [D loss: 0.592617, acc.: 62.50%] [G loss: 0.891025]\n",
            "1662 [D loss: 0.604592, acc.: 60.94%] [G loss: 0.896816]\n",
            "1663 [D loss: 0.596705, acc.: 62.50%] [G loss: 0.923311]\n",
            "1664 [D loss: 0.594528, acc.: 62.50%] [G loss: 1.012419]\n",
            "1665 [D loss: 0.645170, acc.: 51.56%] [G loss: 0.906501]\n",
            "1666 [D loss: 0.587976, acc.: 62.50%] [G loss: 0.882898]\n",
            "1667 [D loss: 0.603414, acc.: 59.38%] [G loss: 0.887008]\n",
            "1668 [D loss: 0.583796, acc.: 62.50%] [G loss: 0.895367]\n",
            "1669 [D loss: 0.602916, acc.: 62.50%] [G loss: 0.909698]\n",
            "1670 [D loss: 0.599041, acc.: 62.50%] [G loss: 0.890442]\n",
            "1671 [D loss: 0.590013, acc.: 60.94%] [G loss: 0.891737]\n",
            "1672 [D loss: 0.589732, acc.: 62.50%] [G loss: 0.897433]\n",
            "1673 [D loss: 0.593988, acc.: 60.94%] [G loss: 0.891421]\n",
            "1674 [D loss: 0.600489, acc.: 60.94%] [G loss: 0.908237]\n",
            "1675 [D loss: 0.590265, acc.: 62.50%] [G loss: 0.922249]\n",
            "1676 [D loss: 0.592334, acc.: 62.50%] [G loss: 0.897236]\n",
            "1677 [D loss: 0.600917, acc.: 60.94%] [G loss: 0.880405]\n",
            "1678 [D loss: 0.589285, acc.: 64.06%] [G loss: 0.884356]\n",
            "1679 [D loss: 0.587312, acc.: 62.50%] [G loss: 0.876875]\n",
            "1680 [D loss: 0.581559, acc.: 62.50%] [G loss: 0.870019]\n",
            "1681 [D loss: 0.600319, acc.: 60.94%] [G loss: 0.866068]\n",
            "1682 [D loss: 0.595986, acc.: 64.06%] [G loss: 0.847119]\n",
            "1683 [D loss: 0.580088, acc.: 65.62%] [G loss: 0.881901]\n",
            "1684 [D loss: 0.589956, acc.: 65.62%] [G loss: 0.835983]\n",
            "1685 [D loss: 0.623745, acc.: 54.69%] [G loss: 0.855876]\n",
            "1686 [D loss: 0.603459, acc.: 57.81%] [G loss: 0.896276]\n",
            "1687 [D loss: 0.596127, acc.: 60.94%] [G loss: 0.909903]\n",
            "1688 [D loss: 0.595991, acc.: 62.50%] [G loss: 0.901623]\n",
            "1689 [D loss: 0.594055, acc.: 60.94%] [G loss: 0.916099]\n",
            "1690 [D loss: 0.592951, acc.: 62.50%] [G loss: 0.923223]\n",
            "1691 [D loss: 0.599115, acc.: 62.50%] [G loss: 0.900165]\n",
            "1692 [D loss: 0.588157, acc.: 62.50%] [G loss: 0.899295]\n",
            "1693 [D loss: 0.588631, acc.: 62.50%] [G loss: 0.908974]\n",
            "1694 [D loss: 0.602046, acc.: 62.50%] [G loss: 0.894418]\n",
            "1695 [D loss: 0.598775, acc.: 60.94%] [G loss: 0.871407]\n",
            "1696 [D loss: 0.607535, acc.: 59.38%] [G loss: 0.882823]\n",
            "1697 [D loss: 0.600370, acc.: 62.50%] [G loss: 0.849015]\n",
            "1698 [D loss: 0.584051, acc.: 62.50%] [G loss: 0.899963]\n",
            "1699 [D loss: 0.596880, acc.: 62.50%] [G loss: 0.963583]\n",
            "1700 [D loss: 0.629227, acc.: 60.94%] [G loss: 0.866140]\n",
            "generated_data\n",
            "1701 [D loss: 0.594768, acc.: 59.38%] [G loss: 0.911413]\n",
            "1702 [D loss: 0.587926, acc.: 62.50%] [G loss: 0.946463]\n",
            "1703 [D loss: 0.596948, acc.: 62.50%] [G loss: 0.900006]\n",
            "1704 [D loss: 0.589793, acc.: 60.94%] [G loss: 0.867046]\n",
            "1705 [D loss: 0.603867, acc.: 62.50%] [G loss: 0.881258]\n",
            "1706 [D loss: 0.605308, acc.: 60.94%] [G loss: 0.909050]\n",
            "1707 [D loss: 0.592910, acc.: 62.50%] [G loss: 0.915722]\n",
            "1708 [D loss: 0.586259, acc.: 60.94%] [G loss: 0.982280]\n",
            "1709 [D loss: 0.608856, acc.: 60.94%] [G loss: 0.913036]\n",
            "1710 [D loss: 0.594152, acc.: 62.50%] [G loss: 0.909440]\n",
            "1711 [D loss: 0.598595, acc.: 62.50%] [G loss: 0.928918]\n",
            "1712 [D loss: 0.597589, acc.: 62.50%] [G loss: 0.940543]\n",
            "1713 [D loss: 0.584943, acc.: 62.50%] [G loss: 0.949605]\n",
            "1714 [D loss: 0.602305, acc.: 62.50%] [G loss: 0.917540]\n",
            "1715 [D loss: 0.598455, acc.: 60.94%] [G loss: 0.900624]\n",
            "1716 [D loss: 0.583798, acc.: 62.50%] [G loss: 0.873675]\n",
            "1717 [D loss: 0.578146, acc.: 62.50%] [G loss: 0.898707]\n",
            "1718 [D loss: 0.559198, acc.: 64.06%] [G loss: 0.899647]\n",
            "1719 [D loss: 0.573275, acc.: 62.50%] [G loss: 0.951390]\n",
            "1720 [D loss: 0.614864, acc.: 64.06%] [G loss: 0.888700]\n",
            "1721 [D loss: 0.588432, acc.: 64.06%] [G loss: 0.892540]\n",
            "1722 [D loss: 0.603079, acc.: 56.25%] [G loss: 0.883844]\n",
            "1723 [D loss: 0.587969, acc.: 62.50%] [G loss: 0.916258]\n",
            "1724 [D loss: 0.584271, acc.: 62.50%] [G loss: 0.945325]\n",
            "1725 [D loss: 0.602509, acc.: 64.06%] [G loss: 0.900606]\n",
            "1726 [D loss: 0.626896, acc.: 50.00%] [G loss: 0.854610]\n",
            "1727 [D loss: 0.597359, acc.: 64.06%] [G loss: 0.860268]\n",
            "1728 [D loss: 0.601552, acc.: 56.25%] [G loss: 0.884483]\n",
            "1729 [D loss: 0.590965, acc.: 60.94%] [G loss: 0.910522]\n",
            "1730 [D loss: 0.607056, acc.: 57.81%] [G loss: 0.921205]\n",
            "1731 [D loss: 0.592558, acc.: 64.06%] [G loss: 0.902773]\n",
            "1732 [D loss: 0.610855, acc.: 62.50%] [G loss: 0.871361]\n",
            "1733 [D loss: 0.598432, acc.: 60.94%] [G loss: 0.934346]\n",
            "1734 [D loss: 0.585406, acc.: 62.50%] [G loss: 0.948918]\n",
            "1735 [D loss: 0.599334, acc.: 60.94%] [G loss: 0.931672]\n",
            "1736 [D loss: 0.575583, acc.: 64.06%] [G loss: 0.952734]\n",
            "1737 [D loss: 0.594810, acc.: 64.06%] [G loss: 0.975478]\n",
            "1738 [D loss: 0.621326, acc.: 62.50%] [G loss: 0.851005]\n",
            "1739 [D loss: 0.606553, acc.: 60.94%] [G loss: 0.855385]\n",
            "1740 [D loss: 0.595814, acc.: 64.06%] [G loss: 0.877763]\n",
            "1741 [D loss: 0.595345, acc.: 62.50%] [G loss: 0.925560]\n",
            "1742 [D loss: 0.604130, acc.: 60.94%] [G loss: 0.941892]\n",
            "1743 [D loss: 0.602458, acc.: 60.94%] [G loss: 0.935855]\n",
            "1744 [D loss: 0.595283, acc.: 62.50%] [G loss: 0.947692]\n",
            "1745 [D loss: 0.586449, acc.: 62.50%] [G loss: 0.898874]\n",
            "1746 [D loss: 0.576443, acc.: 64.06%] [G loss: 0.906951]\n",
            "1747 [D loss: 0.589192, acc.: 62.50%] [G loss: 0.942269]\n",
            "1748 [D loss: 0.600198, acc.: 60.94%] [G loss: 0.912271]\n",
            "1749 [D loss: 0.606030, acc.: 65.62%] [G loss: 0.875608]\n",
            "1750 [D loss: 0.609190, acc.: 54.69%] [G loss: 0.905936]\n",
            "1751 [D loss: 0.615561, acc.: 56.25%] [G loss: 0.901397]\n",
            "1752 [D loss: 0.587274, acc.: 64.06%] [G loss: 0.913174]\n",
            "1753 [D loss: 0.603572, acc.: 60.94%] [G loss: 0.875047]\n",
            "1754 [D loss: 0.583295, acc.: 64.06%] [G loss: 0.893281]\n",
            "1755 [D loss: 0.583959, acc.: 62.50%] [G loss: 0.916940]\n",
            "1756 [D loss: 0.600180, acc.: 60.94%] [G loss: 0.889984]\n",
            "1757 [D loss: 0.613355, acc.: 62.50%] [G loss: 0.873563]\n",
            "1758 [D loss: 0.592580, acc.: 64.06%] [G loss: 0.894757]\n",
            "1759 [D loss: 0.607509, acc.: 60.94%] [G loss: 0.905847]\n",
            "1760 [D loss: 0.585215, acc.: 62.50%] [G loss: 0.903248]\n",
            "1761 [D loss: 0.595357, acc.: 62.50%] [G loss: 0.920685]\n",
            "1762 [D loss: 0.589227, acc.: 62.50%] [G loss: 0.894633]\n",
            "1763 [D loss: 0.603121, acc.: 62.50%] [G loss: 0.893299]\n",
            "1764 [D loss: 0.588064, acc.: 64.06%] [G loss: 0.886367]\n",
            "1765 [D loss: 0.587304, acc.: 64.06%] [G loss: 0.862633]\n",
            "1766 [D loss: 0.595289, acc.: 60.94%] [G loss: 0.884643]\n",
            "1767 [D loss: 0.584050, acc.: 64.06%] [G loss: 0.873869]\n",
            "1768 [D loss: 0.598838, acc.: 64.06%] [G loss: 0.880146]\n",
            "1769 [D loss: 0.601291, acc.: 64.06%] [G loss: 0.847029]\n",
            "1770 [D loss: 0.582955, acc.: 64.06%] [G loss: 0.911552]\n",
            "1771 [D loss: 0.598597, acc.: 62.50%] [G loss: 0.876631]\n",
            "1772 [D loss: 0.583723, acc.: 64.06%] [G loss: 0.887894]\n",
            "1773 [D loss: 0.588575, acc.: 65.62%] [G loss: 0.876447]\n",
            "1774 [D loss: 0.585703, acc.: 62.50%] [G loss: 0.866334]\n",
            "1775 [D loss: 0.592318, acc.: 62.50%] [G loss: 0.866104]\n",
            "1776 [D loss: 0.586335, acc.: 65.62%] [G loss: 0.860859]\n",
            "1777 [D loss: 0.595335, acc.: 62.50%] [G loss: 0.899732]\n",
            "1778 [D loss: 0.599942, acc.: 64.06%] [G loss: 0.890029]\n",
            "1779 [D loss: 0.578925, acc.: 64.06%] [G loss: 0.918538]\n",
            "1780 [D loss: 0.592876, acc.: 64.06%] [G loss: 0.896733]\n",
            "1781 [D loss: 0.601090, acc.: 64.06%] [G loss: 0.863950]\n",
            "1782 [D loss: 0.585866, acc.: 64.06%] [G loss: 0.866167]\n",
            "1783 [D loss: 0.579432, acc.: 62.50%] [G loss: 0.892247]\n",
            "1784 [D loss: 0.588533, acc.: 62.50%] [G loss: 0.888604]\n",
            "1785 [D loss: 0.582525, acc.: 62.50%] [G loss: 0.881956]\n",
            "1786 [D loss: 0.606091, acc.: 60.94%] [G loss: 0.864087]\n",
            "1787 [D loss: 0.600559, acc.: 62.50%] [G loss: 0.852287]\n",
            "1788 [D loss: 0.597102, acc.: 60.94%] [G loss: 0.893133]\n",
            "1789 [D loss: 0.593358, acc.: 59.38%] [G loss: 0.927440]\n",
            "1790 [D loss: 0.593673, acc.: 62.50%] [G loss: 0.914637]\n",
            "1791 [D loss: 0.565842, acc.: 64.06%] [G loss: 0.885258]\n",
            "1792 [D loss: 0.579238, acc.: 62.50%] [G loss: 0.896149]\n",
            "1793 [D loss: 0.575766, acc.: 62.50%] [G loss: 0.847586]\n",
            "1794 [D loss: 0.576625, acc.: 64.06%] [G loss: 0.890537]\n",
            "1795 [D loss: 0.583224, acc.: 60.94%] [G loss: 0.855737]\n",
            "1796 [D loss: 0.625844, acc.: 59.38%] [G loss: 0.883758]\n",
            "1797 [D loss: 0.603870, acc.: 56.25%] [G loss: 0.902834]\n",
            "1798 [D loss: 0.615324, acc.: 64.06%] [G loss: 0.900310]\n",
            "1799 [D loss: 0.588303, acc.: 62.50%] [G loss: 0.890534]\n",
            "1800 [D loss: 0.576688, acc.: 64.06%] [G loss: 0.855344]\n",
            "generated_data\n",
            "1801 [D loss: 0.661287, acc.: 64.06%] [G loss: 0.866636]\n",
            "1802 [D loss: 0.608200, acc.: 62.50%] [G loss: 0.894140]\n",
            "1803 [D loss: 0.607060, acc.: 59.38%] [G loss: 0.884295]\n",
            "1804 [D loss: 0.601540, acc.: 62.50%] [G loss: 0.865609]\n",
            "1805 [D loss: 0.595270, acc.: 64.06%] [G loss: 0.906566]\n",
            "1806 [D loss: 0.587892, acc.: 62.50%] [G loss: 0.922655]\n",
            "1807 [D loss: 0.591347, acc.: 60.94%] [G loss: 0.879781]\n",
            "1808 [D loss: 0.604726, acc.: 62.50%] [G loss: 0.885504]\n",
            "1809 [D loss: 0.594845, acc.: 64.06%] [G loss: 0.908795]\n",
            "1810 [D loss: 0.583137, acc.: 64.06%] [G loss: 0.906271]\n",
            "1811 [D loss: 0.585527, acc.: 65.62%] [G loss: 0.860890]\n",
            "1812 [D loss: 0.585927, acc.: 67.19%] [G loss: 0.842770]\n",
            "1813 [D loss: 0.604282, acc.: 62.50%] [G loss: 0.854495]\n",
            "1814 [D loss: 0.612908, acc.: 60.94%] [G loss: 0.858042]\n",
            "1815 [D loss: 0.599618, acc.: 62.50%] [G loss: 0.884199]\n",
            "1816 [D loss: 0.599298, acc.: 64.06%] [G loss: 0.898579]\n",
            "1817 [D loss: 0.611117, acc.: 60.94%] [G loss: 0.891288]\n",
            "1818 [D loss: 0.600441, acc.: 64.06%] [G loss: 0.864705]\n",
            "1819 [D loss: 0.600851, acc.: 64.06%] [G loss: 0.874964]\n",
            "1820 [D loss: 0.622409, acc.: 60.94%] [G loss: 0.856214]\n",
            "1821 [D loss: 0.584582, acc.: 60.94%] [G loss: 0.906664]\n",
            "1822 [D loss: 0.602346, acc.: 57.81%] [G loss: 0.972081]\n",
            "1823 [D loss: 0.600730, acc.: 62.50%] [G loss: 0.937332]\n",
            "1824 [D loss: 0.590401, acc.: 62.50%] [G loss: 0.910394]\n",
            "1825 [D loss: 0.594374, acc.: 62.50%] [G loss: 1.002932]\n",
            "1826 [D loss: 0.592652, acc.: 60.94%] [G loss: 0.977176]\n",
            "1827 [D loss: 0.583467, acc.: 64.06%] [G loss: 0.990103]\n",
            "1828 [D loss: 0.584531, acc.: 65.62%] [G loss: 0.948776]\n",
            "1829 [D loss: 0.615741, acc.: 57.81%] [G loss: 0.870156]\n",
            "1830 [D loss: 0.602896, acc.: 64.06%] [G loss: 0.865547]\n",
            "1831 [D loss: 0.596747, acc.: 62.50%] [G loss: 0.880393]\n",
            "1832 [D loss: 0.584011, acc.: 62.50%] [G loss: 0.940441]\n",
            "1833 [D loss: 0.582621, acc.: 62.50%] [G loss: 0.930039]\n",
            "1834 [D loss: 0.595990, acc.: 60.94%] [G loss: 0.880982]\n",
            "1835 [D loss: 0.594516, acc.: 57.81%] [G loss: 0.870549]\n",
            "1836 [D loss: 0.596759, acc.: 65.62%] [G loss: 0.899040]\n",
            "1837 [D loss: 0.618188, acc.: 57.81%] [G loss: 0.876581]\n",
            "1838 [D loss: 0.595487, acc.: 62.50%] [G loss: 0.890816]\n",
            "1839 [D loss: 0.590432, acc.: 60.94%] [G loss: 0.910441]\n",
            "1840 [D loss: 0.603067, acc.: 60.94%] [G loss: 0.856016]\n",
            "1841 [D loss: 0.617291, acc.: 59.38%] [G loss: 0.890132]\n",
            "1842 [D loss: 0.566836, acc.: 67.19%] [G loss: 0.969833]\n",
            "1843 [D loss: 0.578196, acc.: 64.06%] [G loss: 0.977836]\n",
            "1844 [D loss: 0.585043, acc.: 67.19%] [G loss: 0.975303]\n",
            "1845 [D loss: 0.594581, acc.: 68.75%] [G loss: 0.863377]\n",
            "1846 [D loss: 0.588674, acc.: 64.06%] [G loss: 0.847231]\n",
            "1847 [D loss: 0.593780, acc.: 64.06%] [G loss: 0.879098]\n",
            "1848 [D loss: 0.605560, acc.: 64.06%] [G loss: 0.843255]\n",
            "1849 [D loss: 0.605086, acc.: 59.38%] [G loss: 0.866130]\n",
            "1850 [D loss: 0.603036, acc.: 56.25%] [G loss: 0.851913]\n",
            "1851 [D loss: 0.608452, acc.: 57.81%] [G loss: 0.841943]\n",
            "1852 [D loss: 0.594131, acc.: 60.94%] [G loss: 0.812818]\n",
            "1853 [D loss: 0.592512, acc.: 59.38%] [G loss: 1.066988]\n",
            "1854 [D loss: 0.575816, acc.: 68.75%] [G loss: 1.100676]\n",
            "1855 [D loss: 0.644860, acc.: 59.38%] [G loss: 0.810245]\n",
            "1856 [D loss: 0.629025, acc.: 56.25%] [G loss: 0.875701]\n",
            "1857 [D loss: 0.594600, acc.: 62.50%] [G loss: 0.873626]\n",
            "1858 [D loss: 0.603183, acc.: 57.81%] [G loss: 0.878172]\n",
            "1859 [D loss: 0.636498, acc.: 51.56%] [G loss: 0.897633]\n",
            "1860 [D loss: 0.587369, acc.: 64.06%] [G loss: 0.919139]\n",
            "1861 [D loss: 0.581743, acc.: 65.62%] [G loss: 0.909059]\n",
            "1862 [D loss: 0.571496, acc.: 68.75%] [G loss: 0.947019]\n",
            "1863 [D loss: 0.568462, acc.: 67.19%] [G loss: 0.932094]\n",
            "1864 [D loss: 0.559744, acc.: 71.88%] [G loss: 0.929478]\n",
            "1865 [D loss: 0.560859, acc.: 70.31%] [G loss: 0.907225]\n",
            "1866 [D loss: 0.561406, acc.: 71.88%] [G loss: 0.935422]\n",
            "1867 [D loss: 0.543557, acc.: 73.44%] [G loss: 0.936974]\n",
            "1868 [D loss: 0.550865, acc.: 71.88%] [G loss: 0.936884]\n",
            "1869 [D loss: 0.564985, acc.: 70.31%] [G loss: 0.924204]\n",
            "1870 [D loss: 0.541261, acc.: 73.44%] [G loss: 0.928693]\n",
            "1871 [D loss: 0.536174, acc.: 71.88%] [G loss: 0.919477]\n",
            "1872 [D loss: 0.582591, acc.: 65.62%] [G loss: 0.938637]\n",
            "1873 [D loss: 0.557072, acc.: 70.31%] [G loss: 0.919864]\n",
            "1874 [D loss: 0.559269, acc.: 70.31%] [G loss: 0.880744]\n",
            "1875 [D loss: 0.559302, acc.: 67.19%] [G loss: 0.819158]\n",
            "1876 [D loss: 0.827079, acc.: 56.25%] [G loss: 0.942410]\n",
            "1877 [D loss: 0.594804, acc.: 62.50%] [G loss: 0.938505]\n",
            "1878 [D loss: 0.601501, acc.: 62.50%] [G loss: 0.933896]\n",
            "1879 [D loss: 0.596503, acc.: 62.50%] [G loss: 0.938491]\n",
            "1880 [D loss: 0.588046, acc.: 62.50%] [G loss: 0.941865]\n",
            "1881 [D loss: 0.596523, acc.: 62.50%] [G loss: 0.952791]\n",
            "1882 [D loss: 0.606395, acc.: 62.50%] [G loss: 0.953873]\n",
            "1883 [D loss: 0.596722, acc.: 62.50%] [G loss: 0.915570]\n",
            "1884 [D loss: 0.604152, acc.: 59.38%] [G loss: 1.135427]\n",
            "1885 [D loss: 0.744684, acc.: 57.81%] [G loss: 0.920535]\n",
            "1886 [D loss: 0.605099, acc.: 62.50%] [G loss: 0.907995]\n",
            "1887 [D loss: 0.598957, acc.: 62.50%] [G loss: 0.918897]\n",
            "1888 [D loss: 0.603704, acc.: 62.50%] [G loss: 0.926422]\n",
            "1889 [D loss: 0.597489, acc.: 62.50%] [G loss: 0.911925]\n",
            "1890 [D loss: 0.602550, acc.: 62.50%] [G loss: 0.913960]\n",
            "1891 [D loss: 0.604666, acc.: 59.38%] [G loss: 0.895168]\n",
            "1892 [D loss: 0.603019, acc.: 62.50%] [G loss: 0.895110]\n",
            "1893 [D loss: 0.599365, acc.: 62.50%] [G loss: 0.913867]\n",
            "1894 [D loss: 0.609733, acc.: 60.94%] [G loss: 0.898724]\n",
            "1895 [D loss: 0.599387, acc.: 62.50%] [G loss: 0.920497]\n",
            "1896 [D loss: 0.611038, acc.: 62.50%] [G loss: 0.924217]\n",
            "1897 [D loss: 0.601140, acc.: 62.50%] [G loss: 0.925672]\n",
            "1898 [D loss: 0.600585, acc.: 62.50%] [G loss: 0.915500]\n",
            "1899 [D loss: 0.604144, acc.: 62.50%] [G loss: 0.929269]\n",
            "1900 [D loss: 0.595263, acc.: 62.50%] [G loss: 0.920075]\n",
            "generated_data\n",
            "1901 [D loss: 0.602736, acc.: 62.50%] [G loss: 0.926654]\n",
            "1902 [D loss: 0.596155, acc.: 62.50%] [G loss: 0.916426]\n",
            "1903 [D loss: 0.594219, acc.: 62.50%] [G loss: 0.926672]\n",
            "1904 [D loss: 0.603223, acc.: 62.50%] [G loss: 0.926448]\n",
            "1905 [D loss: 0.594506, acc.: 62.50%] [G loss: 0.924396]\n",
            "1906 [D loss: 0.602379, acc.: 62.50%] [G loss: 0.903745]\n",
            "1907 [D loss: 0.594162, acc.: 62.50%] [G loss: 0.911982]\n",
            "1908 [D loss: 0.609502, acc.: 60.94%] [G loss: 0.910759]\n",
            "1909 [D loss: 0.597361, acc.: 62.50%] [G loss: 0.922149]\n",
            "1910 [D loss: 0.601292, acc.: 62.50%] [G loss: 0.992751]\n",
            "1911 [D loss: 0.612704, acc.: 62.50%] [G loss: 0.935186]\n",
            "1912 [D loss: 0.596109, acc.: 62.50%] [G loss: 0.923136]\n",
            "1913 [D loss: 0.600642, acc.: 62.50%] [G loss: 0.919523]\n",
            "1914 [D loss: 0.599093, acc.: 62.50%] [G loss: 0.909309]\n",
            "1915 [D loss: 0.608023, acc.: 60.94%] [G loss: 0.911750]\n",
            "1916 [D loss: 0.602344, acc.: 62.50%] [G loss: 0.922524]\n",
            "1917 [D loss: 0.599610, acc.: 62.50%] [G loss: 0.930074]\n",
            "1918 [D loss: 0.594200, acc.: 62.50%] [G loss: 0.930480]\n",
            "1919 [D loss: 0.603840, acc.: 62.50%] [G loss: 0.922813]\n",
            "1920 [D loss: 0.596741, acc.: 62.50%] [G loss: 0.923602]\n",
            "1921 [D loss: 0.600888, acc.: 62.50%] [G loss: 0.932789]\n",
            "1922 [D loss: 0.593503, acc.: 62.50%] [G loss: 0.918070]\n",
            "1923 [D loss: 0.598439, acc.: 62.50%] [G loss: 0.916212]\n",
            "1924 [D loss: 0.600356, acc.: 62.50%] [G loss: 0.919361]\n",
            "1925 [D loss: 0.606342, acc.: 62.50%] [G loss: 0.914775]\n",
            "1926 [D loss: 0.595197, acc.: 62.50%] [G loss: 0.919377]\n",
            "1927 [D loss: 0.594844, acc.: 62.50%] [G loss: 0.909456]\n",
            "1928 [D loss: 0.598201, acc.: 62.50%] [G loss: 0.912011]\n",
            "1929 [D loss: 0.596352, acc.: 62.50%] [G loss: 0.929165]\n",
            "1930 [D loss: 0.601405, acc.: 62.50%] [G loss: 0.897234]\n",
            "1931 [D loss: 0.604010, acc.: 62.50%] [G loss: 0.930850]\n",
            "1932 [D loss: 0.605176, acc.: 62.50%] [G loss: 0.910351]\n",
            "1933 [D loss: 0.597605, acc.: 62.50%] [G loss: 0.944759]\n",
            "1934 [D loss: 0.609262, acc.: 62.50%] [G loss: 0.920591]\n",
            "1935 [D loss: 0.597753, acc.: 62.50%] [G loss: 0.907986]\n",
            "1936 [D loss: 0.595555, acc.: 62.50%] [G loss: 0.915509]\n",
            "1937 [D loss: 0.600049, acc.: 62.50%] [G loss: 0.910903]\n",
            "1938 [D loss: 0.598477, acc.: 62.50%] [G loss: 0.913945]\n",
            "1939 [D loss: 0.600701, acc.: 62.50%] [G loss: 0.916628]\n",
            "1940 [D loss: 0.601597, acc.: 62.50%] [G loss: 0.893906]\n",
            "1941 [D loss: 0.596614, acc.: 62.50%] [G loss: 0.903985]\n",
            "1942 [D loss: 0.594734, acc.: 62.50%] [G loss: 0.900869]\n",
            "1943 [D loss: 0.594874, acc.: 62.50%] [G loss: 0.905677]\n",
            "1944 [D loss: 0.608106, acc.: 62.50%] [G loss: 0.887615]\n",
            "1945 [D loss: 0.599634, acc.: 62.50%] [G loss: 0.893188]\n",
            "1946 [D loss: 0.599062, acc.: 62.50%] [G loss: 0.885873]\n",
            "1947 [D loss: 0.595979, acc.: 62.50%] [G loss: 0.885894]\n",
            "1948 [D loss: 0.601233, acc.: 62.50%] [G loss: 0.889408]\n",
            "1949 [D loss: 0.598595, acc.: 62.50%] [G loss: 0.891267]\n",
            "1950 [D loss: 0.607030, acc.: 62.50%] [G loss: 0.887619]\n",
            "1951 [D loss: 0.598274, acc.: 62.50%] [G loss: 0.894422]\n",
            "1952 [D loss: 0.603014, acc.: 62.50%] [G loss: 0.891807]\n",
            "1953 [D loss: 0.598306, acc.: 62.50%] [G loss: 0.900072]\n",
            "1954 [D loss: 0.597045, acc.: 62.50%] [G loss: 0.898785]\n",
            "1955 [D loss: 0.600045, acc.: 62.50%] [G loss: 0.890021]\n",
            "1956 [D loss: 0.597724, acc.: 62.50%] [G loss: 0.880195]\n",
            "1957 [D loss: 0.601142, acc.: 62.50%] [G loss: 0.888471]\n",
            "1958 [D loss: 0.598072, acc.: 62.50%] [G loss: 0.910000]\n",
            "1959 [D loss: 0.606284, acc.: 62.50%] [G loss: 0.899291]\n",
            "1960 [D loss: 0.599182, acc.: 62.50%] [G loss: 0.908847]\n",
            "1961 [D loss: 0.601075, acc.: 62.50%] [G loss: 0.903979]\n",
            "1962 [D loss: 0.604302, acc.: 62.50%] [G loss: 0.893793]\n",
            "1963 [D loss: 0.597018, acc.: 62.50%] [G loss: 0.906208]\n",
            "1964 [D loss: 0.597785, acc.: 62.50%] [G loss: 0.907922]\n",
            "1965 [D loss: 0.594659, acc.: 62.50%] [G loss: 0.919078]\n",
            "1966 [D loss: 0.597770, acc.: 62.50%] [G loss: 0.907221]\n",
            "1967 [D loss: 0.608730, acc.: 62.50%] [G loss: 0.884153]\n",
            "1968 [D loss: 0.604448, acc.: 62.50%] [G loss: 0.875913]\n",
            "1969 [D loss: 0.605070, acc.: 62.50%] [G loss: 0.891879]\n",
            "1970 [D loss: 0.621344, acc.: 59.38%] [G loss: 0.860651]\n",
            "1971 [D loss: 0.609046, acc.: 62.50%] [G loss: 0.875453]\n",
            "1972 [D loss: 0.603751, acc.: 62.50%] [G loss: 0.870015]\n",
            "1973 [D loss: 0.604564, acc.: 62.50%] [G loss: 0.892637]\n",
            "1974 [D loss: 0.604539, acc.: 62.50%] [G loss: 0.902752]\n",
            "1975 [D loss: 0.596845, acc.: 62.50%] [G loss: 0.898396]\n",
            "1976 [D loss: 0.598136, acc.: 62.50%] [G loss: 0.900364]\n",
            "1977 [D loss: 0.600440, acc.: 62.50%] [G loss: 0.896863]\n",
            "1978 [D loss: 0.596045, acc.: 62.50%] [G loss: 0.901294]\n",
            "1979 [D loss: 0.597508, acc.: 62.50%] [G loss: 0.906998]\n",
            "1980 [D loss: 0.597094, acc.: 62.50%] [G loss: 0.903227]\n",
            "1981 [D loss: 0.602740, acc.: 62.50%] [G loss: 0.883748]\n",
            "1982 [D loss: 0.593511, acc.: 62.50%] [G loss: 0.892657]\n",
            "1983 [D loss: 0.595507, acc.: 62.50%] [G loss: 0.898311]\n",
            "1984 [D loss: 0.595083, acc.: 62.50%] [G loss: 0.901348]\n",
            "1985 [D loss: 0.600056, acc.: 60.94%] [G loss: 0.888927]\n",
            "1986 [D loss: 0.599523, acc.: 59.38%] [G loss: 0.898933]\n",
            "1987 [D loss: 0.594468, acc.: 62.50%] [G loss: 0.888609]\n",
            "1988 [D loss: 0.595502, acc.: 62.50%] [G loss: 0.895818]\n",
            "1989 [D loss: 0.601285, acc.: 62.50%] [G loss: 0.896236]\n",
            "1990 [D loss: 0.592490, acc.: 62.50%] [G loss: 0.883209]\n",
            "1991 [D loss: 0.602145, acc.: 62.50%] [G loss: 0.887675]\n",
            "1992 [D loss: 0.593622, acc.: 62.50%] [G loss: 0.885936]\n",
            "1993 [D loss: 0.600034, acc.: 60.94%] [G loss: 0.857474]\n",
            "1994 [D loss: 0.607937, acc.: 62.50%] [G loss: 0.873945]\n",
            "1995 [D loss: 0.607674, acc.: 60.94%] [G loss: 0.868055]\n",
            "1996 [D loss: 0.599179, acc.: 62.50%] [G loss: 0.883683]\n",
            "1997 [D loss: 0.606610, acc.: 62.50%] [G loss: 0.894578]\n",
            "1998 [D loss: 0.602922, acc.: 62.50%] [G loss: 0.904944]\n",
            "1999 [D loss: 0.596951, acc.: 62.50%] [G loss: 0.899065]\n",
            "2000 [D loss: 0.597927, acc.: 62.50%] [G loss: 0.904326]\n",
            "generated_data\n",
            "2001 [D loss: 0.601026, acc.: 62.50%] [G loss: 0.916641]\n",
            "2002 [D loss: 0.592716, acc.: 62.50%] [G loss: 0.934945]\n",
            "2003 [D loss: 0.599899, acc.: 62.50%] [G loss: 0.927978]\n",
            "2004 [D loss: 0.599288, acc.: 62.50%] [G loss: 0.929202]\n",
            "2005 [D loss: 0.597489, acc.: 62.50%] [G loss: 0.933439]\n",
            "2006 [D loss: 0.601666, acc.: 62.50%] [G loss: 0.920900]\n",
            "2007 [D loss: 0.593067, acc.: 62.50%] [G loss: 0.925087]\n",
            "2008 [D loss: 0.596188, acc.: 62.50%] [G loss: 0.935512]\n",
            "2009 [D loss: 0.598321, acc.: 62.50%] [G loss: 0.929129]\n",
            "2010 [D loss: 0.607215, acc.: 62.50%] [G loss: 0.931020]\n",
            "2011 [D loss: 0.596877, acc.: 62.50%] [G loss: 0.914382]\n",
            "2012 [D loss: 0.598087, acc.: 62.50%] [G loss: 0.913300]\n",
            "2013 [D loss: 0.599883, acc.: 62.50%] [G loss: 0.931777]\n",
            "2014 [D loss: 0.598158, acc.: 62.50%] [G loss: 0.925595]\n",
            "2015 [D loss: 0.605280, acc.: 62.50%] [G loss: 0.921263]\n",
            "2016 [D loss: 0.601393, acc.: 62.50%] [G loss: 0.904557]\n",
            "2017 [D loss: 0.601684, acc.: 62.50%] [G loss: 0.906888]\n",
            "2018 [D loss: 0.596667, acc.: 62.50%] [G loss: 0.905075]\n",
            "2019 [D loss: 0.598004, acc.: 62.50%] [G loss: 0.910096]\n",
            "2020 [D loss: 0.597308, acc.: 62.50%] [G loss: 0.906120]\n",
            "2021 [D loss: 0.602368, acc.: 62.50%] [G loss: 0.906127]\n",
            "2022 [D loss: 0.599692, acc.: 62.50%] [G loss: 0.902569]\n",
            "2023 [D loss: 0.600457, acc.: 62.50%] [G loss: 0.905390]\n",
            "2024 [D loss: 0.598367, acc.: 62.50%] [G loss: 0.906680]\n",
            "2025 [D loss: 0.600085, acc.: 62.50%] [G loss: 0.909343]\n",
            "2026 [D loss: 0.599818, acc.: 62.50%] [G loss: 0.903442]\n",
            "2027 [D loss: 0.599937, acc.: 62.50%] [G loss: 0.900618]\n",
            "2028 [D loss: 0.601225, acc.: 62.50%] [G loss: 0.905742]\n",
            "2029 [D loss: 0.601473, acc.: 62.50%] [G loss: 0.902682]\n",
            "2030 [D loss: 0.598818, acc.: 62.50%] [G loss: 0.897573]\n",
            "2031 [D loss: 0.598985, acc.: 62.50%] [G loss: 0.896644]\n",
            "2032 [D loss: 0.598430, acc.: 62.50%] [G loss: 0.894065]\n",
            "2033 [D loss: 0.597917, acc.: 62.50%] [G loss: 0.894320]\n",
            "2034 [D loss: 0.598629, acc.: 62.50%] [G loss: 0.895258]\n",
            "2035 [D loss: 0.598545, acc.: 62.50%] [G loss: 0.894930]\n",
            "2036 [D loss: 0.599986, acc.: 62.50%] [G loss: 0.894996]\n",
            "2037 [D loss: 0.600294, acc.: 62.50%] [G loss: 0.894810]\n",
            "2038 [D loss: 0.597111, acc.: 62.50%] [G loss: 0.896939]\n",
            "2039 [D loss: 0.598741, acc.: 62.50%] [G loss: 0.901497]\n",
            "2040 [D loss: 0.600355, acc.: 62.50%] [G loss: 0.896415]\n",
            "2041 [D loss: 0.597578, acc.: 62.50%] [G loss: 0.895942]\n",
            "2042 [D loss: 0.600118, acc.: 62.50%] [G loss: 0.894982]\n",
            "2043 [D loss: 0.598369, acc.: 62.50%] [G loss: 0.891034]\n",
            "2044 [D loss: 0.598496, acc.: 62.50%] [G loss: 0.883047]\n",
            "2045 [D loss: 0.599178, acc.: 62.50%] [G loss: 0.889624]\n",
            "2046 [D loss: 0.599230, acc.: 62.50%] [G loss: 0.889371]\n",
            "2047 [D loss: 0.598922, acc.: 62.50%] [G loss: 0.890168]\n",
            "2048 [D loss: 0.597781, acc.: 62.50%] [G loss: 0.892821]\n",
            "2049 [D loss: 0.598036, acc.: 62.50%] [G loss: 0.894402]\n",
            "2050 [D loss: 0.596505, acc.: 62.50%] [G loss: 0.879264]\n",
            "2051 [D loss: 0.600344, acc.: 62.50%] [G loss: 0.884315]\n",
            "2052 [D loss: 0.597799, acc.: 62.50%] [G loss: 0.886428]\n",
            "2053 [D loss: 0.596742, acc.: 62.50%] [G loss: 0.885799]\n",
            "2054 [D loss: 0.600930, acc.: 62.50%] [G loss: 0.878932]\n",
            "2055 [D loss: 0.596239, acc.: 62.50%] [G loss: 0.886600]\n",
            "2056 [D loss: 0.600377, acc.: 62.50%] [G loss: 0.888537]\n",
            "2057 [D loss: 0.602411, acc.: 62.50%] [G loss: 0.885830]\n",
            "2058 [D loss: 0.598876, acc.: 62.50%] [G loss: 0.891973]\n",
            "2059 [D loss: 0.597023, acc.: 62.50%] [G loss: 0.889815]\n",
            "2060 [D loss: 0.597434, acc.: 62.50%] [G loss: 0.885271]\n",
            "2061 [D loss: 0.599350, acc.: 62.50%] [G loss: 0.887977]\n",
            "2062 [D loss: 0.598126, acc.: 62.50%] [G loss: 0.885378]\n",
            "2063 [D loss: 0.597613, acc.: 62.50%] [G loss: 0.884877]\n",
            "2064 [D loss: 0.600743, acc.: 62.50%] [G loss: 0.889434]\n",
            "2065 [D loss: 0.597780, acc.: 62.50%] [G loss: 0.892290]\n",
            "2066 [D loss: 0.599980, acc.: 62.50%] [G loss: 0.889580]\n",
            "2067 [D loss: 0.599379, acc.: 62.50%] [G loss: 0.891832]\n",
            "2068 [D loss: 0.598404, acc.: 62.50%] [G loss: 0.889111]\n",
            "2069 [D loss: 0.598261, acc.: 62.50%] [G loss: 0.890802]\n",
            "2070 [D loss: 0.598001, acc.: 62.50%] [G loss: 0.891061]\n",
            "2071 [D loss: 0.598631, acc.: 62.50%] [G loss: 0.891749]\n",
            "2072 [D loss: 0.599893, acc.: 62.50%] [G loss: 0.887665]\n",
            "2073 [D loss: 0.598796, acc.: 62.50%] [G loss: 0.890556]\n",
            "2074 [D loss: 0.597306, acc.: 62.50%] [G loss: 0.892047]\n",
            "2075 [D loss: 0.596407, acc.: 62.50%] [G loss: 0.891398]\n",
            "2076 [D loss: 0.598863, acc.: 62.50%] [G loss: 0.895482]\n",
            "2077 [D loss: 0.599435, acc.: 62.50%] [G loss: 0.888445]\n",
            "2078 [D loss: 0.599018, acc.: 62.50%] [G loss: 0.889171]\n",
            "2079 [D loss: 0.599039, acc.: 62.50%] [G loss: 0.890802]\n",
            "2080 [D loss: 0.598211, acc.: 62.50%] [G loss: 0.887049]\n",
            "2081 [D loss: 0.597864, acc.: 62.50%] [G loss: 0.887578]\n",
            "2082 [D loss: 0.597472, acc.: 62.50%] [G loss: 0.885723]\n",
            "2083 [D loss: 0.598105, acc.: 62.50%] [G loss: 0.886210]\n",
            "2084 [D loss: 0.597256, acc.: 62.50%] [G loss: 0.886241]\n",
            "2085 [D loss: 0.599458, acc.: 62.50%] [G loss: 0.886860]\n",
            "2086 [D loss: 0.597228, acc.: 62.50%] [G loss: 0.885886]\n",
            "2087 [D loss: 0.598511, acc.: 62.50%] [G loss: 0.882899]\n",
            "2088 [D loss: 0.600374, acc.: 62.50%] [G loss: 0.884459]\n",
            "2089 [D loss: 0.598076, acc.: 62.50%] [G loss: 0.884706]\n",
            "2090 [D loss: 0.597891, acc.: 62.50%] [G loss: 0.886292]\n",
            "2091 [D loss: 0.598039, acc.: 62.50%] [G loss: 0.883070]\n",
            "2092 [D loss: 0.597662, acc.: 62.50%] [G loss: 0.886230]\n",
            "2093 [D loss: 0.598127, acc.: 62.50%] [G loss: 0.882122]\n",
            "2094 [D loss: 0.599284, acc.: 62.50%] [G loss: 0.881056]\n",
            "2095 [D loss: 0.597486, acc.: 62.50%] [G loss: 0.886242]\n",
            "2096 [D loss: 0.596902, acc.: 62.50%] [G loss: 0.881933]\n",
            "2097 [D loss: 0.599796, acc.: 62.50%] [G loss: 0.885111]\n",
            "2098 [D loss: 0.597721, acc.: 62.50%] [G loss: 0.883437]\n",
            "2099 [D loss: 0.596749, acc.: 62.50%] [G loss: 0.883309]\n",
            "2100 [D loss: 0.600161, acc.: 62.50%] [G loss: 0.884923]\n",
            "generated_data\n",
            "2101 [D loss: 0.597146, acc.: 62.50%] [G loss: 0.883897]\n",
            "2102 [D loss: 0.598894, acc.: 62.50%] [G loss: 0.881591]\n",
            "2103 [D loss: 0.597833, acc.: 62.50%] [G loss: 0.878207]\n",
            "2104 [D loss: 0.598287, acc.: 62.50%] [G loss: 0.881061]\n",
            "2105 [D loss: 0.597206, acc.: 62.50%] [G loss: 0.874391]\n",
            "2106 [D loss: 0.596326, acc.: 62.50%] [G loss: 0.875238]\n",
            "2107 [D loss: 0.597393, acc.: 62.50%] [G loss: 0.889933]\n",
            "2108 [D loss: 0.595602, acc.: 62.50%] [G loss: 0.917254]\n",
            "2109 [D loss: 0.608451, acc.: 62.50%] [G loss: 0.876935]\n",
            "2110 [D loss: 0.597661, acc.: 62.50%] [G loss: 0.876924]\n",
            "2111 [D loss: 0.598836, acc.: 62.50%] [G loss: 0.874983]\n",
            "2112 [D loss: 0.596832, acc.: 62.50%] [G loss: 0.869062]\n",
            "2113 [D loss: 0.599334, acc.: 62.50%] [G loss: 0.885116]\n",
            "2114 [D loss: 0.600865, acc.: 62.50%] [G loss: 0.876098]\n",
            "2115 [D loss: 0.598950, acc.: 62.50%] [G loss: 0.883064]\n",
            "2116 [D loss: 0.595352, acc.: 62.50%] [G loss: 0.881951]\n",
            "2117 [D loss: 0.597382, acc.: 62.50%] [G loss: 0.886002]\n",
            "2118 [D loss: 0.598258, acc.: 62.50%] [G loss: 0.885116]\n",
            "2119 [D loss: 0.599097, acc.: 62.50%] [G loss: 0.886402]\n",
            "2120 [D loss: 0.597820, acc.: 62.50%] [G loss: 0.879331]\n",
            "2121 [D loss: 0.595722, acc.: 62.50%] [G loss: 0.882966]\n",
            "2122 [D loss: 0.598489, acc.: 62.50%] [G loss: 0.878506]\n",
            "2123 [D loss: 0.599882, acc.: 62.50%] [G loss: 0.879261]\n",
            "2124 [D loss: 0.598647, acc.: 62.50%] [G loss: 0.880129]\n",
            "2125 [D loss: 0.596624, acc.: 62.50%] [G loss: 0.880389]\n",
            "2126 [D loss: 0.597276, acc.: 62.50%] [G loss: 0.882931]\n",
            "2127 [D loss: 0.598304, acc.: 62.50%] [G loss: 0.884988]\n",
            "2128 [D loss: 0.599731, acc.: 62.50%] [G loss: 0.881749]\n",
            "2129 [D loss: 0.597894, acc.: 62.50%] [G loss: 0.880053]\n",
            "2130 [D loss: 0.596047, acc.: 62.50%] [G loss: 0.881857]\n",
            "2131 [D loss: 0.596063, acc.: 62.50%] [G loss: 0.881816]\n",
            "2132 [D loss: 0.598929, acc.: 62.50%] [G loss: 0.880338]\n",
            "2133 [D loss: 0.600161, acc.: 62.50%] [G loss: 0.878666]\n",
            "2134 [D loss: 0.597351, acc.: 62.50%] [G loss: 0.880480]\n",
            "2135 [D loss: 0.598156, acc.: 62.50%] [G loss: 0.882745]\n",
            "2136 [D loss: 0.598754, acc.: 62.50%] [G loss: 0.883299]\n",
            "2137 [D loss: 0.598374, acc.: 62.50%] [G loss: 0.880087]\n",
            "2138 [D loss: 0.597321, acc.: 62.50%] [G loss: 0.879916]\n",
            "2139 [D loss: 0.597631, acc.: 62.50%] [G loss: 0.877052]\n",
            "2140 [D loss: 0.597076, acc.: 62.50%] [G loss: 0.876833]\n",
            "2141 [D loss: 0.600058, acc.: 62.50%] [G loss: 0.881320]\n",
            "2142 [D loss: 0.600036, acc.: 62.50%] [G loss: 0.879356]\n",
            "2143 [D loss: 0.597103, acc.: 62.50%] [G loss: 0.876087]\n",
            "2144 [D loss: 0.598626, acc.: 62.50%] [G loss: 0.874682]\n",
            "2145 [D loss: 0.598273, acc.: 62.50%] [G loss: 0.873496]\n",
            "2146 [D loss: 0.597012, acc.: 62.50%] [G loss: 0.874465]\n",
            "2147 [D loss: 0.596710, acc.: 62.50%] [G loss: 0.879289]\n",
            "2148 [D loss: 0.597297, acc.: 62.50%] [G loss: 0.874359]\n",
            "2149 [D loss: 0.597469, acc.: 62.50%] [G loss: 0.876426]\n",
            "2150 [D loss: 0.598179, acc.: 62.50%] [G loss: 0.874732]\n",
            "2151 [D loss: 0.598870, acc.: 62.50%] [G loss: 0.876362]\n",
            "2152 [D loss: 0.597991, acc.: 62.50%] [G loss: 0.878967]\n",
            "2153 [D loss: 0.597656, acc.: 62.50%] [G loss: 0.878645]\n",
            "2154 [D loss: 0.596635, acc.: 62.50%] [G loss: 0.878991]\n",
            "2155 [D loss: 0.600094, acc.: 62.50%] [G loss: 0.877609]\n",
            "2156 [D loss: 0.598390, acc.: 62.50%] [G loss: 0.875672]\n",
            "2157 [D loss: 0.599364, acc.: 62.50%] [G loss: 0.879874]\n",
            "2158 [D loss: 0.599845, acc.: 62.50%] [G loss: 0.884081]\n",
            "2159 [D loss: 0.598677, acc.: 62.50%] [G loss: 0.881097]\n",
            "2160 [D loss: 0.597249, acc.: 62.50%] [G loss: 0.879063]\n",
            "2161 [D loss: 0.598231, acc.: 62.50%] [G loss: 0.887584]\n",
            "2162 [D loss: 0.598882, acc.: 62.50%] [G loss: 0.878873]\n",
            "2163 [D loss: 0.598851, acc.: 62.50%] [G loss: 0.877950]\n",
            "2164 [D loss: 0.598187, acc.: 62.50%] [G loss: 0.878210]\n",
            "2165 [D loss: 0.597423, acc.: 62.50%] [G loss: 0.874811]\n",
            "2166 [D loss: 0.597510, acc.: 62.50%] [G loss: 0.876017]\n",
            "2167 [D loss: 0.598469, acc.: 62.50%] [G loss: 0.875613]\n",
            "2168 [D loss: 0.597933, acc.: 62.50%] [G loss: 0.874464]\n",
            "2169 [D loss: 0.598330, acc.: 62.50%] [G loss: 0.875455]\n",
            "2170 [D loss: 0.597794, acc.: 62.50%] [G loss: 0.870754]\n",
            "2171 [D loss: 0.597612, acc.: 62.50%] [G loss: 0.870547]\n",
            "2172 [D loss: 0.599345, acc.: 62.50%] [G loss: 0.868909]\n",
            "2173 [D loss: 0.597147, acc.: 62.50%] [G loss: 0.871298]\n",
            "2174 [D loss: 0.599766, acc.: 62.50%] [G loss: 0.875640]\n",
            "2175 [D loss: 0.597845, acc.: 62.50%] [G loss: 0.873485]\n",
            "2176 [D loss: 0.599404, acc.: 62.50%] [G loss: 0.878129]\n",
            "2177 [D loss: 0.596764, acc.: 62.50%] [G loss: 0.874773]\n",
            "2178 [D loss: 0.598770, acc.: 62.50%] [G loss: 0.875130]\n",
            "2179 [D loss: 0.598899, acc.: 62.50%] [G loss: 0.871903]\n",
            "2180 [D loss: 0.597564, acc.: 62.50%] [G loss: 0.874304]\n",
            "2181 [D loss: 0.598298, acc.: 62.50%] [G loss: 0.869215]\n",
            "2182 [D loss: 0.598322, acc.: 62.50%] [G loss: 0.867412]\n",
            "2183 [D loss: 0.598002, acc.: 62.50%] [G loss: 0.868163]\n",
            "2184 [D loss: 0.597859, acc.: 62.50%] [G loss: 0.873252]\n",
            "2185 [D loss: 0.597832, acc.: 62.50%] [G loss: 0.862743]\n",
            "2186 [D loss: 0.599461, acc.: 62.50%] [G loss: 0.870703]\n",
            "2187 [D loss: 0.597887, acc.: 62.50%] [G loss: 0.866542]\n",
            "2188 [D loss: 0.598846, acc.: 62.50%] [G loss: 0.868408]\n",
            "2189 [D loss: 0.597279, acc.: 62.50%] [G loss: 0.870809]\n",
            "2190 [D loss: 0.598340, acc.: 62.50%] [G loss: 0.868379]\n",
            "2191 [D loss: 0.598023, acc.: 62.50%] [G loss: 0.868386]\n",
            "2192 [D loss: 0.598940, acc.: 62.50%] [G loss: 0.869126]\n",
            "2193 [D loss: 0.597612, acc.: 62.50%] [G loss: 0.865583]\n",
            "2194 [D loss: 0.598131, acc.: 62.50%] [G loss: 0.862837]\n",
            "2195 [D loss: 0.597939, acc.: 62.50%] [G loss: 0.864259]\n",
            "2196 [D loss: 0.598465, acc.: 62.50%] [G loss: 0.862890]\n",
            "2197 [D loss: 0.597314, acc.: 62.50%] [G loss: 0.863927]\n",
            "2198 [D loss: 0.599988, acc.: 62.50%] [G loss: 0.864958]\n",
            "2199 [D loss: 0.598957, acc.: 62.50%] [G loss: 0.867885]\n",
            "2200 [D loss: 0.598379, acc.: 62.50%] [G loss: 0.867671]\n",
            "generated_data\n",
            "2201 [D loss: 0.599119, acc.: 62.50%] [G loss: 0.868452]\n",
            "2202 [D loss: 0.598056, acc.: 62.50%] [G loss: 0.868861]\n",
            "2203 [D loss: 0.598267, acc.: 62.50%] [G loss: 0.867420]\n",
            "2204 [D loss: 0.596926, acc.: 62.50%] [G loss: 0.869392]\n",
            "2205 [D loss: 0.597145, acc.: 62.50%] [G loss: 0.870246]\n",
            "2206 [D loss: 0.597729, acc.: 62.50%] [G loss: 0.866977]\n",
            "2207 [D loss: 0.598938, acc.: 62.50%] [G loss: 0.865723]\n",
            "2208 [D loss: 0.598900, acc.: 62.50%] [G loss: 0.867778]\n",
            "2209 [D loss: 0.597289, acc.: 62.50%] [G loss: 0.868478]\n",
            "2210 [D loss: 0.595408, acc.: 62.50%] [G loss: 0.870199]\n",
            "2211 [D loss: 0.598321, acc.: 62.50%] [G loss: 0.871838]\n",
            "2212 [D loss: 0.599678, acc.: 62.50%] [G loss: 0.867704]\n",
            "2213 [D loss: 0.597639, acc.: 62.50%] [G loss: 0.867057]\n",
            "2214 [D loss: 0.599535, acc.: 62.50%] [G loss: 0.864931]\n",
            "2215 [D loss: 0.597167, acc.: 62.50%] [G loss: 0.863910]\n",
            "2216 [D loss: 0.596598, acc.: 62.50%] [G loss: 0.858657]\n",
            "2217 [D loss: 0.596482, acc.: 62.50%] [G loss: 0.859266]\n",
            "2218 [D loss: 0.598734, acc.: 62.50%] [G loss: 0.861225]\n",
            "2219 [D loss: 0.597322, acc.: 62.50%] [G loss: 0.856309]\n",
            "2220 [D loss: 0.598966, acc.: 62.50%] [G loss: 0.864747]\n",
            "2221 [D loss: 0.599120, acc.: 62.50%] [G loss: 0.862351]\n",
            "2222 [D loss: 0.597066, acc.: 62.50%] [G loss: 0.870359]\n",
            "2223 [D loss: 0.595544, acc.: 62.50%] [G loss: 0.891280]\n",
            "2224 [D loss: 0.602188, acc.: 62.50%] [G loss: 0.864841]\n",
            "2225 [D loss: 0.599638, acc.: 62.50%] [G loss: 0.860907]\n",
            "2226 [D loss: 0.597317, acc.: 62.50%] [G loss: 0.862249]\n",
            "2227 [D loss: 0.598911, acc.: 62.50%] [G loss: 0.859161]\n",
            "2228 [D loss: 0.597404, acc.: 62.50%] [G loss: 0.863696]\n",
            "2229 [D loss: 0.595650, acc.: 62.50%] [G loss: 0.861328]\n",
            "2230 [D loss: 0.600086, acc.: 62.50%] [G loss: 0.865578]\n",
            "2231 [D loss: 0.598896, acc.: 62.50%] [G loss: 0.864054]\n",
            "2232 [D loss: 0.598354, acc.: 62.50%] [G loss: 0.866024]\n",
            "2233 [D loss: 0.597496, acc.: 62.50%] [G loss: 0.867956]\n",
            "2234 [D loss: 0.596384, acc.: 62.50%] [G loss: 0.880635]\n",
            "2235 [D loss: 0.597833, acc.: 62.50%] [G loss: 0.865889]\n",
            "2236 [D loss: 0.597750, acc.: 62.50%] [G loss: 0.868141]\n",
            "2237 [D loss: 0.596880, acc.: 62.50%] [G loss: 0.865328]\n",
            "2238 [D loss: 0.598861, acc.: 62.50%] [G loss: 0.872578]\n",
            "2239 [D loss: 0.597560, acc.: 62.50%] [G loss: 0.862630]\n",
            "2240 [D loss: 0.599428, acc.: 62.50%] [G loss: 0.863173]\n",
            "2241 [D loss: 0.597652, acc.: 62.50%] [G loss: 0.874297]\n",
            "2242 [D loss: 0.598246, acc.: 62.50%] [G loss: 0.867045]\n",
            "2243 [D loss: 0.600299, acc.: 62.50%] [G loss: 0.866766]\n",
            "2244 [D loss: 0.596545, acc.: 62.50%] [G loss: 0.867371]\n",
            "2245 [D loss: 0.599371, acc.: 62.50%] [G loss: 0.863292]\n",
            "2246 [D loss: 0.597112, acc.: 62.50%] [G loss: 0.864003]\n",
            "2247 [D loss: 0.598299, acc.: 62.50%] [G loss: 0.863616]\n",
            "2248 [D loss: 0.598631, acc.: 62.50%] [G loss: 0.862311]\n",
            "2249 [D loss: 0.598149, acc.: 62.50%] [G loss: 0.861046]\n",
            "2250 [D loss: 0.598245, acc.: 62.50%] [G loss: 0.861704]\n",
            "2251 [D loss: 0.598348, acc.: 62.50%] [G loss: 0.857565]\n",
            "2252 [D loss: 0.597713, acc.: 62.50%] [G loss: 0.860074]\n",
            "2253 [D loss: 0.597200, acc.: 62.50%] [G loss: 0.865368]\n",
            "2254 [D loss: 0.597074, acc.: 62.50%] [G loss: 0.861829]\n",
            "2255 [D loss: 0.597354, acc.: 62.50%] [G loss: 0.859695]\n",
            "2256 [D loss: 0.596049, acc.: 62.50%] [G loss: 0.852707]\n",
            "2257 [D loss: 0.598942, acc.: 62.50%] [G loss: 0.855256]\n",
            "2258 [D loss: 0.597786, acc.: 62.50%] [G loss: 0.859490]\n",
            "2259 [D loss: 0.595544, acc.: 62.50%] [G loss: 0.863303]\n",
            "2260 [D loss: 0.597512, acc.: 62.50%] [G loss: 0.866896]\n",
            "2261 [D loss: 0.597851, acc.: 62.50%] [G loss: 0.863274]\n",
            "2262 [D loss: 0.594970, acc.: 62.50%] [G loss: 0.870881]\n",
            "2263 [D loss: 0.597882, acc.: 62.50%] [G loss: 0.868087]\n",
            "2264 [D loss: 0.598786, acc.: 62.50%] [G loss: 0.863915]\n",
            "2265 [D loss: 0.599796, acc.: 62.50%] [G loss: 0.865275]\n",
            "2266 [D loss: 0.599798, acc.: 62.50%] [G loss: 0.860352]\n",
            "2267 [D loss: 0.597676, acc.: 62.50%] [G loss: 0.864168]\n",
            "2268 [D loss: 0.598560, acc.: 62.50%] [G loss: 0.858033]\n",
            "2269 [D loss: 0.599151, acc.: 62.50%] [G loss: 0.852432]\n",
            "2270 [D loss: 0.598652, acc.: 62.50%] [G loss: 0.859517]\n",
            "2271 [D loss: 0.600895, acc.: 62.50%] [G loss: 0.859997]\n",
            "2272 [D loss: 0.598778, acc.: 62.50%] [G loss: 0.853236]\n",
            "2273 [D loss: 0.599051, acc.: 62.50%] [G loss: 0.861812]\n",
            "2274 [D loss: 0.596393, acc.: 62.50%] [G loss: 0.863101]\n",
            "2275 [D loss: 0.598931, acc.: 62.50%] [G loss: 0.860719]\n",
            "2276 [D loss: 0.596321, acc.: 62.50%] [G loss: 0.862103]\n",
            "2277 [D loss: 0.598514, acc.: 62.50%] [G loss: 0.863601]\n",
            "2278 [D loss: 0.599376, acc.: 62.50%] [G loss: 0.862916]\n",
            "2279 [D loss: 0.596991, acc.: 62.50%] [G loss: 0.855539]\n",
            "2280 [D loss: 0.597299, acc.: 62.50%] [G loss: 0.861739]\n",
            "2281 [D loss: 0.598518, acc.: 62.50%] [G loss: 0.852799]\n",
            "2282 [D loss: 0.600435, acc.: 62.50%] [G loss: 0.861774]\n",
            "2283 [D loss: 0.596835, acc.: 62.50%] [G loss: 0.850690]\n",
            "2284 [D loss: 0.600960, acc.: 62.50%] [G loss: 0.851933]\n",
            "2285 [D loss: 0.597759, acc.: 62.50%] [G loss: 0.859572]\n",
            "2286 [D loss: 0.600633, acc.: 62.50%] [G loss: 0.854851]\n",
            "2287 [D loss: 0.597969, acc.: 62.50%] [G loss: 0.858613]\n",
            "2288 [D loss: 0.597870, acc.: 62.50%] [G loss: 0.861503]\n",
            "2289 [D loss: 0.599256, acc.: 62.50%] [G loss: 0.862819]\n",
            "2290 [D loss: 0.596044, acc.: 62.50%] [G loss: 0.865375]\n",
            "2291 [D loss: 0.595051, acc.: 62.50%] [G loss: 0.860739]\n",
            "2292 [D loss: 0.598600, acc.: 62.50%] [G loss: 0.856672]\n",
            "2293 [D loss: 0.600927, acc.: 62.50%] [G loss: 0.857559]\n",
            "2294 [D loss: 0.598084, acc.: 62.50%] [G loss: 0.861271]\n",
            "2295 [D loss: 0.598918, acc.: 62.50%] [G loss: 0.862798]\n",
            "2296 [D loss: 0.597015, acc.: 62.50%] [G loss: 0.866361]\n",
            "2297 [D loss: 0.598764, acc.: 62.50%] [G loss: 0.859454]\n",
            "2298 [D loss: 0.597297, acc.: 62.50%] [G loss: 0.863250]\n",
            "2299 [D loss: 0.596823, acc.: 62.50%] [G loss: 0.869887]\n",
            "2300 [D loss: 0.594290, acc.: 62.50%] [G loss: 0.866527]\n",
            "generated_data\n",
            "2301 [D loss: 0.597159, acc.: 62.50%] [G loss: 0.875530]\n",
            "2302 [D loss: 0.597844, acc.: 62.50%] [G loss: 0.865256]\n",
            "2303 [D loss: 0.597388, acc.: 62.50%] [G loss: 0.872273]\n",
            "2304 [D loss: 0.596504, acc.: 62.50%] [G loss: 0.869835]\n",
            "2305 [D loss: 0.598860, acc.: 62.50%] [G loss: 0.876053]\n",
            "2306 [D loss: 0.598594, acc.: 62.50%] [G loss: 0.868816]\n",
            "2307 [D loss: 0.596043, acc.: 62.50%] [G loss: 0.864543]\n",
            "2308 [D loss: 0.598869, acc.: 62.50%] [G loss: 0.864705]\n",
            "2309 [D loss: 0.595134, acc.: 62.50%] [G loss: 0.862244]\n",
            "2310 [D loss: 0.596428, acc.: 62.50%] [G loss: 0.868671]\n",
            "2311 [D loss: 0.597129, acc.: 62.50%] [G loss: 0.862325]\n",
            "2312 [D loss: 0.594863, acc.: 62.50%] [G loss: 0.864230]\n",
            "2313 [D loss: 0.596049, acc.: 62.50%] [G loss: 0.860828]\n",
            "2314 [D loss: 0.597020, acc.: 62.50%] [G loss: 0.861814]\n",
            "2315 [D loss: 0.598848, acc.: 62.50%] [G loss: 0.862806]\n",
            "2316 [D loss: 0.599445, acc.: 62.50%] [G loss: 0.857559]\n",
            "2317 [D loss: 0.596069, acc.: 62.50%] [G loss: 0.861474]\n",
            "2318 [D loss: 0.598029, acc.: 62.50%] [G loss: 0.860657]\n",
            "2319 [D loss: 0.597612, acc.: 62.50%] [G loss: 0.857544]\n",
            "2320 [D loss: 0.595730, acc.: 62.50%] [G loss: 0.865769]\n",
            "2321 [D loss: 0.596284, acc.: 62.50%] [G loss: 0.852790]\n",
            "2322 [D loss: 0.604228, acc.: 62.50%] [G loss: 0.853730]\n",
            "2323 [D loss: 0.598936, acc.: 62.50%] [G loss: 0.860934]\n",
            "2324 [D loss: 0.595972, acc.: 62.50%] [G loss: 0.863052]\n",
            "2325 [D loss: 0.595949, acc.: 62.50%] [G loss: 0.863515]\n",
            "2326 [D loss: 0.599500, acc.: 62.50%] [G loss: 0.861900]\n",
            "2327 [D loss: 0.596600, acc.: 62.50%] [G loss: 0.853089]\n",
            "2328 [D loss: 0.597811, acc.: 62.50%] [G loss: 0.867295]\n",
            "2329 [D loss: 0.599734, acc.: 62.50%] [G loss: 0.868030]\n",
            "2330 [D loss: 0.596356, acc.: 62.50%] [G loss: 0.863925]\n",
            "2331 [D loss: 0.597546, acc.: 62.50%] [G loss: 0.868574]\n",
            "2332 [D loss: 0.595535, acc.: 62.50%] [G loss: 0.870998]\n",
            "2333 [D loss: 0.601972, acc.: 60.94%] [G loss: 0.860398]\n",
            "2334 [D loss: 0.600742, acc.: 62.50%] [G loss: 0.860320]\n",
            "2335 [D loss: 0.597748, acc.: 62.50%] [G loss: 0.861699]\n",
            "2336 [D loss: 0.597469, acc.: 62.50%] [G loss: 0.861086]\n",
            "2337 [D loss: 0.596428, acc.: 62.50%] [G loss: 0.858713]\n",
            "2338 [D loss: 0.599214, acc.: 62.50%] [G loss: 0.862586]\n",
            "2339 [D loss: 0.596104, acc.: 62.50%] [G loss: 0.864181]\n",
            "2340 [D loss: 0.598422, acc.: 62.50%] [G loss: 0.861344]\n",
            "2341 [D loss: 0.599462, acc.: 62.50%] [G loss: 0.866855]\n",
            "2342 [D loss: 0.598176, acc.: 62.50%] [G loss: 0.868141]\n",
            "2343 [D loss: 0.597933, acc.: 62.50%] [G loss: 0.877849]\n",
            "2344 [D loss: 0.599060, acc.: 62.50%] [G loss: 0.862249]\n",
            "2345 [D loss: 0.597613, acc.: 62.50%] [G loss: 0.869715]\n",
            "2346 [D loss: 0.603712, acc.: 62.50%] [G loss: 0.861509]\n",
            "2347 [D loss: 0.596886, acc.: 62.50%] [G loss: 0.862262]\n",
            "2348 [D loss: 0.596420, acc.: 62.50%] [G loss: 0.859220]\n",
            "2349 [D loss: 0.595949, acc.: 62.50%] [G loss: 0.857467]\n",
            "2350 [D loss: 0.597266, acc.: 62.50%] [G loss: 0.856727]\n",
            "2351 [D loss: 0.598710, acc.: 62.50%] [G loss: 0.856287]\n",
            "2352 [D loss: 0.594844, acc.: 62.50%] [G loss: 0.860997]\n",
            "2353 [D loss: 0.596761, acc.: 62.50%] [G loss: 0.858273]\n",
            "2354 [D loss: 0.598029, acc.: 62.50%] [G loss: 0.858468]\n",
            "2355 [D loss: 0.598411, acc.: 62.50%] [G loss: 0.858036]\n",
            "2356 [D loss: 0.596056, acc.: 62.50%] [G loss: 0.853566]\n",
            "2357 [D loss: 0.595419, acc.: 62.50%] [G loss: 0.855868]\n",
            "2358 [D loss: 0.596606, acc.: 62.50%] [G loss: 0.860727]\n",
            "2359 [D loss: 0.597064, acc.: 62.50%] [G loss: 0.854120]\n",
            "2360 [D loss: 0.599446, acc.: 62.50%] [G loss: 0.860713]\n",
            "2361 [D loss: 0.597712, acc.: 62.50%] [G loss: 0.857179]\n",
            "2362 [D loss: 0.596491, acc.: 62.50%] [G loss: 0.856592]\n",
            "2363 [D loss: 0.597861, acc.: 62.50%] [G loss: 0.866585]\n",
            "2364 [D loss: 0.592640, acc.: 62.50%] [G loss: 0.881171]\n",
            "2365 [D loss: 0.601321, acc.: 62.50%] [G loss: 0.854513]\n",
            "2366 [D loss: 0.614078, acc.: 60.94%] [G loss: 0.863340]\n",
            "2367 [D loss: 0.597308, acc.: 62.50%] [G loss: 0.867426]\n",
            "2368 [D loss: 0.598400, acc.: 62.50%] [G loss: 0.870681]\n",
            "2369 [D loss: 0.597731, acc.: 62.50%] [G loss: 0.867097]\n",
            "2370 [D loss: 0.597686, acc.: 62.50%] [G loss: 0.869090]\n",
            "2371 [D loss: 0.596117, acc.: 62.50%] [G loss: 0.879960]\n",
            "2372 [D loss: 0.594803, acc.: 62.50%] [G loss: 0.882876]\n",
            "2373 [D loss: 0.595249, acc.: 62.50%] [G loss: 0.879607]\n",
            "2374 [D loss: 0.598908, acc.: 62.50%] [G loss: 0.866481]\n",
            "2375 [D loss: 0.596707, acc.: 62.50%] [G loss: 0.867964]\n",
            "2376 [D loss: 0.599201, acc.: 62.50%] [G loss: 0.896589]\n",
            "2377 [D loss: 0.612575, acc.: 62.50%] [G loss: 0.861508]\n",
            "2378 [D loss: 0.601437, acc.: 62.50%] [G loss: 0.867277]\n",
            "2379 [D loss: 0.593617, acc.: 62.50%] [G loss: 0.866134]\n",
            "2380 [D loss: 0.598175, acc.: 62.50%] [G loss: 0.876966]\n",
            "2381 [D loss: 0.593828, acc.: 62.50%] [G loss: 0.870999]\n",
            "2382 [D loss: 0.598940, acc.: 62.50%] [G loss: 0.890105]\n",
            "2383 [D loss: 0.596892, acc.: 62.50%] [G loss: 0.868772]\n",
            "2384 [D loss: 0.587271, acc.: 62.50%] [G loss: 0.886562]\n",
            "2385 [D loss: 0.605113, acc.: 62.50%] [G loss: 0.874914]\n",
            "2386 [D loss: 0.596059, acc.: 62.50%] [G loss: 0.864345]\n",
            "2387 [D loss: 0.598913, acc.: 62.50%] [G loss: 0.885268]\n",
            "2388 [D loss: 0.596383, acc.: 62.50%] [G loss: 0.878015]\n",
            "2389 [D loss: 0.603555, acc.: 62.50%] [G loss: 0.863274]\n",
            "2390 [D loss: 0.594926, acc.: 62.50%] [G loss: 0.850575]\n",
            "2391 [D loss: 0.623866, acc.: 53.12%] [G loss: 0.876167]\n",
            "2392 [D loss: 0.598519, acc.: 62.50%] [G loss: 0.858663]\n",
            "2393 [D loss: 0.595079, acc.: 62.50%] [G loss: 0.857415]\n",
            "2394 [D loss: 0.596144, acc.: 62.50%] [G loss: 0.857184]\n",
            "2395 [D loss: 0.595798, acc.: 62.50%] [G loss: 0.857879]\n",
            "2396 [D loss: 0.595888, acc.: 62.50%] [G loss: 0.854518]\n",
            "2397 [D loss: 0.596666, acc.: 62.50%] [G loss: 0.846589]\n",
            "2398 [D loss: 0.598144, acc.: 62.50%] [G loss: 0.820267]\n",
            "2399 [D loss: 0.599410, acc.: 62.50%] [G loss: 0.859149]\n",
            "2400 [D loss: 0.596078, acc.: 62.50%] [G loss: 0.854229]\n",
            "generated_data\n",
            "2401 [D loss: 0.594194, acc.: 62.50%] [G loss: 0.848971]\n",
            "2402 [D loss: 0.594460, acc.: 62.50%] [G loss: 0.847886]\n",
            "2403 [D loss: 0.598667, acc.: 62.50%] [G loss: 0.863878]\n",
            "2404 [D loss: 0.591143, acc.: 62.50%] [G loss: 0.855001]\n",
            "2405 [D loss: 0.606459, acc.: 60.94%] [G loss: 0.853345]\n",
            "2406 [D loss: 0.603508, acc.: 62.50%] [G loss: 0.844442]\n",
            "2407 [D loss: 0.599462, acc.: 62.50%] [G loss: 0.852882]\n",
            "2408 [D loss: 0.596477, acc.: 62.50%] [G loss: 0.853778]\n",
            "2409 [D loss: 0.597898, acc.: 62.50%] [G loss: 0.851840]\n",
            "2410 [D loss: 0.603353, acc.: 62.50%] [G loss: 0.855587]\n",
            "2411 [D loss: 0.596424, acc.: 62.50%] [G loss: 0.860901]\n",
            "2412 [D loss: 0.598505, acc.: 62.50%] [G loss: 0.860787]\n",
            "2413 [D loss: 0.595777, acc.: 62.50%] [G loss: 0.877138]\n",
            "2414 [D loss: 0.603525, acc.: 62.50%] [G loss: 0.864130]\n",
            "2415 [D loss: 0.599271, acc.: 62.50%] [G loss: 0.859561]\n",
            "2416 [D loss: 0.595235, acc.: 62.50%] [G loss: 0.856355]\n",
            "2417 [D loss: 0.597922, acc.: 62.50%] [G loss: 0.856553]\n",
            "2418 [D loss: 0.598052, acc.: 62.50%] [G loss: 0.856551]\n",
            "2419 [D loss: 0.597762, acc.: 62.50%] [G loss: 0.857627]\n",
            "2420 [D loss: 0.598232, acc.: 62.50%] [G loss: 0.844088]\n",
            "2421 [D loss: 0.600607, acc.: 60.94%] [G loss: 0.837904]\n",
            "2422 [D loss: 0.606774, acc.: 60.94%] [G loss: 0.830645]\n",
            "2423 [D loss: 0.593313, acc.: 62.50%] [G loss: 0.829815]\n",
            "2424 [D loss: 0.609541, acc.: 59.38%] [G loss: 0.843161]\n",
            "2425 [D loss: 0.599629, acc.: 62.50%] [G loss: 0.845273]\n",
            "2426 [D loss: 0.598984, acc.: 62.50%] [G loss: 0.840659]\n",
            "2427 [D loss: 0.599485, acc.: 62.50%] [G loss: 0.849155]\n",
            "2428 [D loss: 0.595594, acc.: 62.50%] [G loss: 0.853382]\n",
            "2429 [D loss: 0.598091, acc.: 62.50%] [G loss: 0.861372]\n",
            "2430 [D loss: 0.593175, acc.: 62.50%] [G loss: 1.257764]\n",
            "2431 [D loss: 0.782758, acc.: 62.50%] [G loss: 0.856463]\n",
            "2432 [D loss: 0.601708, acc.: 62.50%] [G loss: 0.857558]\n",
            "2433 [D loss: 0.600039, acc.: 62.50%] [G loss: 0.850749]\n",
            "2434 [D loss: 0.597211, acc.: 62.50%] [G loss: 0.859061]\n",
            "2435 [D loss: 0.599144, acc.: 62.50%] [G loss: 0.853677]\n",
            "2436 [D loss: 0.596290, acc.: 62.50%] [G loss: 0.845234]\n",
            "2437 [D loss: 0.599253, acc.: 62.50%] [G loss: 0.853875]\n",
            "2438 [D loss: 0.600740, acc.: 62.50%] [G loss: 0.850240]\n",
            "2439 [D loss: 0.596429, acc.: 62.50%] [G loss: 0.852949]\n",
            "2440 [D loss: 0.597621, acc.: 62.50%] [G loss: 0.856021]\n",
            "2441 [D loss: 0.598062, acc.: 62.50%] [G loss: 0.846203]\n",
            "2442 [D loss: 0.598012, acc.: 62.50%] [G loss: 0.847872]\n",
            "2443 [D loss: 0.603095, acc.: 60.94%] [G loss: 0.834213]\n",
            "2444 [D loss: 0.601050, acc.: 62.50%] [G loss: 0.867712]\n",
            "2445 [D loss: 0.601883, acc.: 62.50%] [G loss: 0.827112]\n",
            "2446 [D loss: 0.599293, acc.: 60.94%] [G loss: 0.820610]\n",
            "2447 [D loss: 0.605340, acc.: 62.50%] [G loss: 0.822627]\n",
            "2448 [D loss: 0.599079, acc.: 60.94%] [G loss: 0.835393]\n",
            "2449 [D loss: 0.593746, acc.: 62.50%] [G loss: 0.847943]\n",
            "2450 [D loss: 0.598720, acc.: 62.50%] [G loss: 0.857759]\n",
            "2451 [D loss: 0.592729, acc.: 62.50%] [G loss: 0.852509]\n",
            "2452 [D loss: 0.609402, acc.: 62.50%] [G loss: 0.828305]\n",
            "2453 [D loss: 0.612644, acc.: 60.94%] [G loss: 0.834854]\n",
            "2454 [D loss: 0.599875, acc.: 62.50%] [G loss: 0.840235]\n",
            "2455 [D loss: 0.597050, acc.: 62.50%] [G loss: 0.833412]\n",
            "2456 [D loss: 0.602440, acc.: 60.94%] [G loss: 0.831057]\n",
            "2457 [D loss: 0.605646, acc.: 60.94%] [G loss: 0.844640]\n",
            "2458 [D loss: 0.600874, acc.: 62.50%] [G loss: 0.842162]\n",
            "2459 [D loss: 0.601615, acc.: 62.50%] [G loss: 0.844631]\n",
            "2460 [D loss: 0.594470, acc.: 62.50%] [G loss: 0.850917]\n",
            "2461 [D loss: 0.598810, acc.: 62.50%] [G loss: 0.850739]\n",
            "2462 [D loss: 0.597522, acc.: 62.50%] [G loss: 0.845424]\n",
            "2463 [D loss: 0.599262, acc.: 62.50%] [G loss: 0.845316]\n",
            "2464 [D loss: 0.596184, acc.: 62.50%] [G loss: 0.851169]\n",
            "2465 [D loss: 0.595437, acc.: 62.50%] [G loss: 0.842692]\n",
            "2466 [D loss: 0.598487, acc.: 62.50%] [G loss: 0.844076]\n",
            "2467 [D loss: 0.599711, acc.: 62.50%] [G loss: 0.838259]\n",
            "2468 [D loss: 0.601840, acc.: 62.50%] [G loss: 0.847120]\n",
            "2469 [D loss: 0.598062, acc.: 62.50%] [G loss: 0.844999]\n",
            "2470 [D loss: 0.604896, acc.: 62.50%] [G loss: 0.853537]\n",
            "2471 [D loss: 0.593476, acc.: 62.50%] [G loss: 0.858736]\n",
            "2472 [D loss: 0.600347, acc.: 62.50%] [G loss: 0.873194]\n",
            "2473 [D loss: 0.599502, acc.: 62.50%] [G loss: 0.853378]\n",
            "2474 [D loss: 0.597457, acc.: 62.50%] [G loss: 0.846952]\n",
            "2475 [D loss: 0.598567, acc.: 62.50%] [G loss: 0.848252]\n",
            "2476 [D loss: 0.595029, acc.: 62.50%] [G loss: 0.846943]\n",
            "2477 [D loss: 0.597163, acc.: 62.50%] [G loss: 0.850692]\n",
            "2478 [D loss: 0.597590, acc.: 62.50%] [G loss: 0.851861]\n",
            "2479 [D loss: 0.599405, acc.: 62.50%] [G loss: 0.856659]\n",
            "2480 [D loss: 0.597811, acc.: 62.50%] [G loss: 0.860911]\n",
            "2481 [D loss: 0.597023, acc.: 62.50%] [G loss: 0.848549]\n",
            "2482 [D loss: 0.597957, acc.: 62.50%] [G loss: 0.854183]\n",
            "2483 [D loss: 0.598006, acc.: 62.50%] [G loss: 0.852681]\n",
            "2484 [D loss: 0.595253, acc.: 62.50%] [G loss: 0.850116]\n",
            "2485 [D loss: 0.598343, acc.: 62.50%] [G loss: 0.843737]\n",
            "2486 [D loss: 0.595996, acc.: 62.50%] [G loss: 0.845510]\n",
            "2487 [D loss: 0.600843, acc.: 60.94%] [G loss: 0.845031]\n",
            "2488 [D loss: 0.596676, acc.: 62.50%] [G loss: 0.844836]\n",
            "2489 [D loss: 0.596627, acc.: 62.50%] [G loss: 0.846945]\n",
            "2490 [D loss: 0.595191, acc.: 62.50%] [G loss: 0.842846]\n",
            "2491 [D loss: 0.601623, acc.: 60.94%] [G loss: 0.851277]\n",
            "2492 [D loss: 0.595969, acc.: 62.50%] [G loss: 0.851330]\n",
            "2493 [D loss: 0.598871, acc.: 59.38%] [G loss: 0.854840]\n",
            "2494 [D loss: 0.598600, acc.: 62.50%] [G loss: 0.852241]\n",
            "2495 [D loss: 0.599841, acc.: 62.50%] [G loss: 0.858482]\n",
            "2496 [D loss: 0.597018, acc.: 62.50%] [G loss: 0.856043]\n",
            "2497 [D loss: 0.601636, acc.: 62.50%] [G loss: 0.853367]\n",
            "2498 [D loss: 0.596871, acc.: 62.50%] [G loss: 0.856441]\n",
            "2499 [D loss: 0.598311, acc.: 62.50%] [G loss: 0.857887]\n",
            "2500 [D loss: 0.597923, acc.: 62.50%] [G loss: 0.861347]\n",
            "generated_data\n",
            "2501 [D loss: 0.597549, acc.: 62.50%] [G loss: 0.862506]\n",
            "2502 [D loss: 0.595584, acc.: 62.50%] [G loss: 0.863712]\n",
            "2503 [D loss: 0.594883, acc.: 62.50%] [G loss: 0.867348]\n",
            "2504 [D loss: 0.598832, acc.: 62.50%] [G loss: 0.860665]\n",
            "2505 [D loss: 0.596300, acc.: 62.50%] [G loss: 0.858846]\n",
            "2506 [D loss: 0.598832, acc.: 62.50%] [G loss: 0.854617]\n",
            "2507 [D loss: 0.597839, acc.: 62.50%] [G loss: 0.861172]\n",
            "2508 [D loss: 0.593443, acc.: 62.50%] [G loss: 0.857566]\n",
            "2509 [D loss: 0.598483, acc.: 62.50%] [G loss: 0.860503]\n",
            "2510 [D loss: 0.601368, acc.: 62.50%] [G loss: 0.856787]\n",
            "2511 [D loss: 0.597316, acc.: 62.50%] [G loss: 0.855033]\n",
            "2512 [D loss: 0.597909, acc.: 62.50%] [G loss: 0.858094]\n",
            "2513 [D loss: 0.594914, acc.: 62.50%] [G loss: 0.856219]\n",
            "2514 [D loss: 0.596589, acc.: 62.50%] [G loss: 0.857518]\n",
            "2515 [D loss: 0.598483, acc.: 62.50%] [G loss: 0.852005]\n",
            "2516 [D loss: 0.601686, acc.: 62.50%] [G loss: 0.858000]\n",
            "2517 [D loss: 0.597716, acc.: 62.50%] [G loss: 0.858826]\n",
            "2518 [D loss: 0.600138, acc.: 62.50%] [G loss: 0.853701]\n",
            "2519 [D loss: 0.599264, acc.: 62.50%] [G loss: 0.854215]\n",
            "2520 [D loss: 0.598207, acc.: 62.50%] [G loss: 0.857031]\n",
            "2521 [D loss: 0.598263, acc.: 62.50%] [G loss: 0.858236]\n",
            "2522 [D loss: 0.596372, acc.: 62.50%] [G loss: 0.857430]\n",
            "2523 [D loss: 0.596326, acc.: 62.50%] [G loss: 0.863156]\n",
            "2524 [D loss: 0.595037, acc.: 62.50%] [G loss: 0.861614]\n",
            "2525 [D loss: 0.598078, acc.: 62.50%] [G loss: 0.863080]\n",
            "2526 [D loss: 0.599559, acc.: 62.50%] [G loss: 0.863860]\n",
            "2527 [D loss: 0.596822, acc.: 62.50%] [G loss: 0.858155]\n",
            "2528 [D loss: 0.598321, acc.: 62.50%] [G loss: 0.852071]\n",
            "2529 [D loss: 0.598554, acc.: 62.50%] [G loss: 0.856282]\n",
            "2530 [D loss: 0.596797, acc.: 62.50%] [G loss: 0.856695]\n",
            "2531 [D loss: 0.597295, acc.: 62.50%] [G loss: 0.856917]\n",
            "2532 [D loss: 0.595714, acc.: 62.50%] [G loss: 0.854042]\n",
            "2533 [D loss: 0.598508, acc.: 62.50%] [G loss: 0.852751]\n",
            "2534 [D loss: 0.596783, acc.: 62.50%] [G loss: 0.850867]\n",
            "2535 [D loss: 0.592810, acc.: 62.50%] [G loss: 0.859073]\n",
            "2536 [D loss: 0.596078, acc.: 62.50%] [G loss: 0.861284]\n",
            "2537 [D loss: 0.594001, acc.: 62.50%] [G loss: 0.862525]\n",
            "2538 [D loss: 0.596669, acc.: 62.50%] [G loss: 0.876276]\n",
            "2539 [D loss: 0.600496, acc.: 62.50%] [G loss: 0.863321]\n",
            "2540 [D loss: 0.596765, acc.: 62.50%] [G loss: 0.854923]\n",
            "2541 [D loss: 0.594377, acc.: 62.50%] [G loss: 0.857764]\n",
            "2542 [D loss: 0.597956, acc.: 62.50%] [G loss: 0.858664]\n",
            "2543 [D loss: 0.598578, acc.: 62.50%] [G loss: 0.853284]\n",
            "2544 [D loss: 0.597858, acc.: 62.50%] [G loss: 0.856190]\n",
            "2545 [D loss: 0.598035, acc.: 62.50%] [G loss: 0.855076]\n",
            "2546 [D loss: 0.596841, acc.: 62.50%] [G loss: 0.853255]\n",
            "2547 [D loss: 0.596739, acc.: 62.50%] [G loss: 0.850585]\n",
            "2548 [D loss: 0.596959, acc.: 62.50%] [G loss: 0.864797]\n",
            "2549 [D loss: 0.597674, acc.: 62.50%] [G loss: 0.872875]\n",
            "2550 [D loss: 0.599135, acc.: 62.50%] [G loss: 0.864391]\n",
            "2551 [D loss: 0.599714, acc.: 62.50%] [G loss: 0.850338]\n",
            "2552 [D loss: 0.595498, acc.: 62.50%] [G loss: 0.847937]\n",
            "2553 [D loss: 0.597192, acc.: 62.50%] [G loss: 0.856301]\n",
            "2554 [D loss: 0.597665, acc.: 62.50%] [G loss: 0.854946]\n",
            "2555 [D loss: 0.598714, acc.: 62.50%] [G loss: 0.849873]\n",
            "2556 [D loss: 0.596580, acc.: 62.50%] [G loss: 0.845520]\n",
            "2557 [D loss: 0.596452, acc.: 62.50%] [G loss: 0.847223]\n",
            "2558 [D loss: 0.599990, acc.: 62.50%] [G loss: 0.838545]\n",
            "2559 [D loss: 0.598411, acc.: 62.50%] [G loss: 0.856024]\n",
            "2560 [D loss: 0.595322, acc.: 62.50%] [G loss: 0.843409]\n",
            "2561 [D loss: 0.595021, acc.: 62.50%] [G loss: 0.852431]\n",
            "2562 [D loss: 0.595041, acc.: 62.50%] [G loss: 0.834479]\n",
            "2563 [D loss: 0.600394, acc.: 62.50%] [G loss: 0.847380]\n",
            "2564 [D loss: 0.594934, acc.: 64.06%] [G loss: 0.837144]\n",
            "2565 [D loss: 0.596706, acc.: 60.94%] [G loss: 0.832869]\n",
            "2566 [D loss: 0.601866, acc.: 62.50%] [G loss: 0.833246]\n",
            "2567 [D loss: 0.609932, acc.: 59.38%] [G loss: 0.832292]\n",
            "2568 [D loss: 0.598723, acc.: 62.50%] [G loss: 0.848103]\n",
            "2569 [D loss: 0.595098, acc.: 62.50%] [G loss: 0.837978]\n",
            "2570 [D loss: 0.601837, acc.: 62.50%] [G loss: 0.849740]\n",
            "2571 [D loss: 0.597248, acc.: 62.50%] [G loss: 0.841583]\n",
            "2572 [D loss: 0.604592, acc.: 60.94%] [G loss: 0.849805]\n",
            "2573 [D loss: 0.595566, acc.: 62.50%] [G loss: 0.843869]\n",
            "2574 [D loss: 0.596262, acc.: 62.50%] [G loss: 0.849193]\n",
            "2575 [D loss: 0.596664, acc.: 62.50%] [G loss: 0.836758]\n",
            "2576 [D loss: 0.604118, acc.: 60.94%] [G loss: 0.839653]\n",
            "2577 [D loss: 0.597782, acc.: 62.50%] [G loss: 0.847208]\n",
            "2578 [D loss: 0.595648, acc.: 62.50%] [G loss: 0.850597]\n",
            "2579 [D loss: 0.597487, acc.: 62.50%] [G loss: 0.856773]\n",
            "2580 [D loss: 0.597336, acc.: 62.50%] [G loss: 0.846576]\n",
            "2581 [D loss: 0.598429, acc.: 62.50%] [G loss: 0.855195]\n",
            "2582 [D loss: 0.599529, acc.: 62.50%] [G loss: 0.854874]\n",
            "2583 [D loss: 0.597760, acc.: 62.50%] [G loss: 0.855674]\n",
            "2584 [D loss: 0.597206, acc.: 62.50%] [G loss: 0.852038]\n",
            "2585 [D loss: 0.600827, acc.: 62.50%] [G loss: 0.851590]\n",
            "2586 [D loss: 0.598411, acc.: 62.50%] [G loss: 0.855874]\n",
            "2587 [D loss: 0.598050, acc.: 62.50%] [G loss: 0.851115]\n",
            "2588 [D loss: 0.598941, acc.: 62.50%] [G loss: 0.853411]\n",
            "2589 [D loss: 0.597940, acc.: 62.50%] [G loss: 0.850722]\n",
            "2590 [D loss: 0.596591, acc.: 62.50%] [G loss: 0.854913]\n",
            "2591 [D loss: 0.598949, acc.: 62.50%] [G loss: 0.859407]\n",
            "2592 [D loss: 0.597953, acc.: 62.50%] [G loss: 0.855085]\n",
            "2593 [D loss: 0.598285, acc.: 62.50%] [G loss: 0.855903]\n",
            "2594 [D loss: 0.595619, acc.: 62.50%] [G loss: 0.862433]\n",
            "2595 [D loss: 0.596781, acc.: 62.50%] [G loss: 0.856173]\n",
            "2596 [D loss: 0.599935, acc.: 62.50%] [G loss: 0.858705]\n",
            "2597 [D loss: 0.596124, acc.: 62.50%] [G loss: 0.860847]\n",
            "2598 [D loss: 0.594183, acc.: 62.50%] [G loss: 0.863385]\n",
            "2599 [D loss: 0.596176, acc.: 62.50%] [G loss: 0.867535]\n",
            "2600 [D loss: 0.601379, acc.: 62.50%] [G loss: 0.852131]\n",
            "generated_data\n",
            "2601 [D loss: 0.596502, acc.: 64.06%] [G loss: 0.866441]\n",
            "2602 [D loss: 0.597200, acc.: 62.50%] [G loss: 0.857361]\n",
            "2603 [D loss: 0.600292, acc.: 62.50%] [G loss: 0.858664]\n",
            "2604 [D loss: 0.599641, acc.: 62.50%] [G loss: 0.850671]\n",
            "2605 [D loss: 0.599776, acc.: 62.50%] [G loss: 0.854934]\n",
            "2606 [D loss: 0.599781, acc.: 62.50%] [G loss: 0.858607]\n",
            "2607 [D loss: 0.596997, acc.: 62.50%] [G loss: 0.854885]\n",
            "2608 [D loss: 0.597238, acc.: 62.50%] [G loss: 0.856507]\n",
            "2609 [D loss: 0.596652, acc.: 62.50%] [G loss: 0.854221]\n",
            "2610 [D loss: 0.597258, acc.: 62.50%] [G loss: 0.859043]\n",
            "2611 [D loss: 0.599812, acc.: 60.94%] [G loss: 0.852619]\n",
            "2612 [D loss: 0.595678, acc.: 62.50%] [G loss: 0.853531]\n",
            "2613 [D loss: 0.596909, acc.: 62.50%] [G loss: 0.854257]\n",
            "2614 [D loss: 0.597207, acc.: 62.50%] [G loss: 0.856834]\n",
            "2615 [D loss: 0.597719, acc.: 62.50%] [G loss: 0.851459]\n",
            "2616 [D loss: 0.594229, acc.: 62.50%] [G loss: 0.851884]\n",
            "2617 [D loss: 0.599579, acc.: 62.50%] [G loss: 0.854766]\n",
            "2618 [D loss: 0.595773, acc.: 62.50%] [G loss: 0.862747]\n",
            "2619 [D loss: 0.598151, acc.: 62.50%] [G loss: 0.858392]\n",
            "2620 [D loss: 0.597112, acc.: 62.50%] [G loss: 0.852512]\n",
            "2621 [D loss: 0.597367, acc.: 64.06%] [G loss: 0.858535]\n",
            "2622 [D loss: 0.600587, acc.: 62.50%] [G loss: 0.871376]\n",
            "2623 [D loss: 0.595885, acc.: 64.06%] [G loss: 0.873794]\n",
            "2624 [D loss: 0.593554, acc.: 62.50%] [G loss: 0.884331]\n",
            "2625 [D loss: 0.593277, acc.: 62.50%] [G loss: 0.879332]\n",
            "2626 [D loss: 0.595100, acc.: 62.50%] [G loss: 0.883822]\n",
            "2627 [D loss: 0.593655, acc.: 62.50%] [G loss: 0.888636]\n",
            "2628 [D loss: 0.596922, acc.: 62.50%] [G loss: 0.891756]\n",
            "2629 [D loss: 0.596764, acc.: 62.50%] [G loss: 0.872043]\n",
            "2630 [D loss: 0.593933, acc.: 64.06%] [G loss: 0.864819]\n",
            "2631 [D loss: 0.596854, acc.: 62.50%] [G loss: 0.842269]\n",
            "2632 [D loss: 0.630259, acc.: 60.94%] [G loss: 0.876868]\n",
            "2633 [D loss: 0.597843, acc.: 62.50%] [G loss: 0.864438]\n",
            "2634 [D loss: 0.600078, acc.: 62.50%] [G loss: 0.861187]\n",
            "2635 [D loss: 0.597311, acc.: 62.50%] [G loss: 0.862221]\n",
            "2636 [D loss: 0.595997, acc.: 62.50%] [G loss: 0.860613]\n",
            "2637 [D loss: 0.597891, acc.: 62.50%] [G loss: 0.864807]\n",
            "2638 [D loss: 0.599110, acc.: 62.50%] [G loss: 0.863826]\n",
            "2639 [D loss: 0.597220, acc.: 62.50%] [G loss: 0.881211]\n",
            "2640 [D loss: 0.595438, acc.: 62.50%] [G loss: 0.873762]\n",
            "2641 [D loss: 0.594698, acc.: 62.50%] [G loss: 0.860698]\n",
            "2642 [D loss: 0.599392, acc.: 62.50%] [G loss: 0.860602]\n",
            "2643 [D loss: 0.598966, acc.: 62.50%] [G loss: 0.856038]\n",
            "2644 [D loss: 0.602779, acc.: 62.50%] [G loss: 0.863134]\n",
            "2645 [D loss: 0.596542, acc.: 62.50%] [G loss: 0.868442]\n",
            "2646 [D loss: 0.596635, acc.: 62.50%] [G loss: 0.865449]\n",
            "2647 [D loss: 0.600147, acc.: 62.50%] [G loss: 0.872146]\n",
            "2648 [D loss: 0.598407, acc.: 62.50%] [G loss: 0.858541]\n",
            "2649 [D loss: 0.598570, acc.: 62.50%] [G loss: 0.860001]\n",
            "2650 [D loss: 0.595425, acc.: 62.50%] [G loss: 0.861694]\n",
            "2651 [D loss: 0.605472, acc.: 60.94%] [G loss: 0.859136]\n",
            "2652 [D loss: 0.596708, acc.: 62.50%] [G loss: 0.852322]\n",
            "2653 [D loss: 0.602071, acc.: 62.50%] [G loss: 0.849887]\n",
            "2654 [D loss: 0.597043, acc.: 62.50%] [G loss: 0.849932]\n",
            "2655 [D loss: 0.601690, acc.: 59.38%] [G loss: 0.857162]\n",
            "2656 [D loss: 0.598693, acc.: 60.94%] [G loss: 0.856226]\n",
            "2657 [D loss: 0.600726, acc.: 62.50%] [G loss: 0.866309]\n",
            "2658 [D loss: 0.596160, acc.: 60.94%] [G loss: 0.863798]\n",
            "2659 [D loss: 0.592253, acc.: 62.50%] [G loss: 0.891034]\n",
            "2660 [D loss: 0.600726, acc.: 62.50%] [G loss: 0.866071]\n",
            "2661 [D loss: 0.598716, acc.: 60.94%] [G loss: 0.875393]\n",
            "2662 [D loss: 0.597258, acc.: 62.50%] [G loss: 0.879405]\n",
            "2663 [D loss: 0.605736, acc.: 62.50%] [G loss: 0.851483]\n",
            "2664 [D loss: 0.599553, acc.: 62.50%] [G loss: 0.875259]\n",
            "2665 [D loss: 0.598492, acc.: 62.50%] [G loss: 0.854883]\n",
            "2666 [D loss: 0.589493, acc.: 62.50%] [G loss: 0.877099]\n",
            "2667 [D loss: 0.607508, acc.: 62.50%] [G loss: 0.852256]\n",
            "2668 [D loss: 0.597011, acc.: 62.50%] [G loss: 0.848343]\n",
            "2669 [D loss: 0.596753, acc.: 62.50%] [G loss: 0.847532]\n",
            "2670 [D loss: 0.598004, acc.: 62.50%] [G loss: 0.850995]\n",
            "2671 [D loss: 0.596434, acc.: 62.50%] [G loss: 0.838657]\n",
            "2672 [D loss: 0.600663, acc.: 62.50%] [G loss: 0.861488]\n",
            "2673 [D loss: 0.595407, acc.: 62.50%] [G loss: 0.856091]\n",
            "2674 [D loss: 0.600840, acc.: 62.50%] [G loss: 0.836813]\n",
            "2675 [D loss: 0.595918, acc.: 62.50%] [G loss: 0.832377]\n",
            "2676 [D loss: 0.598569, acc.: 62.50%] [G loss: 0.830264]\n",
            "2677 [D loss: 0.599316, acc.: 60.94%] [G loss: 0.847311]\n",
            "2678 [D loss: 0.602060, acc.: 60.94%] [G loss: 0.848681]\n",
            "2679 [D loss: 0.596157, acc.: 62.50%] [G loss: 0.851349]\n",
            "2680 [D loss: 0.596568, acc.: 62.50%] [G loss: 0.850891]\n",
            "2681 [D loss: 0.599739, acc.: 62.50%] [G loss: 0.874901]\n",
            "2682 [D loss: 0.595454, acc.: 62.50%] [G loss: 0.872749]\n",
            "2683 [D loss: 0.596292, acc.: 62.50%] [G loss: 0.891180]\n",
            "2684 [D loss: 0.596725, acc.: 62.50%] [G loss: 0.893265]\n",
            "2685 [D loss: 0.598787, acc.: 62.50%] [G loss: 0.877657]\n",
            "2686 [D loss: 0.601012, acc.: 62.50%] [G loss: 0.870700]\n",
            "2687 [D loss: 0.609072, acc.: 62.50%] [G loss: 0.859451]\n",
            "2688 [D loss: 0.602959, acc.: 62.50%] [G loss: 0.855476]\n",
            "2689 [D loss: 0.597766, acc.: 62.50%] [G loss: 0.860213]\n",
            "2690 [D loss: 0.599736, acc.: 62.50%] [G loss: 0.854842]\n",
            "2691 [D loss: 0.595126, acc.: 62.50%] [G loss: 0.869068]\n",
            "2692 [D loss: 0.596028, acc.: 62.50%] [G loss: 0.854057]\n",
            "2693 [D loss: 0.597845, acc.: 62.50%] [G loss: 0.849371]\n",
            "2694 [D loss: 0.593581, acc.: 62.50%] [G loss: 0.857594]\n",
            "2695 [D loss: 0.595110, acc.: 62.50%] [G loss: 0.839988]\n",
            "2696 [D loss: 0.600903, acc.: 62.50%] [G loss: 0.856406]\n",
            "2697 [D loss: 0.602186, acc.: 62.50%] [G loss: 0.846300]\n",
            "2698 [D loss: 0.600576, acc.: 60.94%] [G loss: 0.851034]\n",
            "2699 [D loss: 0.602736, acc.: 62.50%] [G loss: 0.858696]\n",
            "2700 [D loss: 0.604245, acc.: 60.94%] [G loss: 0.856956]\n",
            "generated_data\n",
            "2701 [D loss: 0.600049, acc.: 62.50%] [G loss: 0.836887]\n",
            "2702 [D loss: 0.595171, acc.: 62.50%] [G loss: 0.853299]\n",
            "2703 [D loss: 0.597629, acc.: 62.50%] [G loss: 0.844073]\n",
            "2704 [D loss: 0.593302, acc.: 62.50%] [G loss: 0.851962]\n",
            "2705 [D loss: 0.599335, acc.: 62.50%] [G loss: 0.846627]\n",
            "2706 [D loss: 0.597487, acc.: 62.50%] [G loss: 0.851232]\n",
            "2707 [D loss: 0.598902, acc.: 62.50%] [G loss: 0.859049]\n",
            "2708 [D loss: 0.599641, acc.: 62.50%] [G loss: 0.859814]\n",
            "2709 [D loss: 0.597921, acc.: 62.50%] [G loss: 0.858799]\n",
            "2710 [D loss: 0.600145, acc.: 62.50%] [G loss: 0.865084]\n",
            "2711 [D loss: 0.604085, acc.: 60.94%] [G loss: 0.862292]\n",
            "2712 [D loss: 0.597622, acc.: 62.50%] [G loss: 0.856547]\n",
            "2713 [D loss: 0.598132, acc.: 62.50%] [G loss: 0.867921]\n",
            "2714 [D loss: 0.598730, acc.: 62.50%] [G loss: 0.856061]\n",
            "2715 [D loss: 0.595152, acc.: 62.50%] [G loss: 0.851603]\n",
            "2716 [D loss: 0.598838, acc.: 60.94%] [G loss: 0.854438]\n",
            "2717 [D loss: 0.595983, acc.: 62.50%] [G loss: 0.857834]\n",
            "2718 [D loss: 0.598000, acc.: 60.94%] [G loss: 0.872127]\n",
            "2719 [D loss: 0.599767, acc.: 62.50%] [G loss: 0.859818]\n",
            "2720 [D loss: 0.601267, acc.: 62.50%] [G loss: 0.868974]\n",
            "2721 [D loss: 0.593106, acc.: 62.50%] [G loss: 0.896896]\n",
            "2722 [D loss: 0.600943, acc.: 62.50%] [G loss: 0.870758]\n",
            "2723 [D loss: 0.601243, acc.: 62.50%] [G loss: 0.865332]\n",
            "2724 [D loss: 0.597352, acc.: 62.50%] [G loss: 0.854104]\n",
            "2725 [D loss: 0.595616, acc.: 62.50%] [G loss: 0.858611]\n",
            "2726 [D loss: 0.598169, acc.: 62.50%] [G loss: 0.854688]\n",
            "2727 [D loss: 0.595769, acc.: 62.50%] [G loss: 0.855740]\n",
            "2728 [D loss: 0.599572, acc.: 62.50%] [G loss: 0.877176]\n",
            "2729 [D loss: 0.598135, acc.: 62.50%] [G loss: 0.859571]\n",
            "2730 [D loss: 0.599332, acc.: 62.50%] [G loss: 0.866558]\n",
            "2731 [D loss: 0.602244, acc.: 62.50%] [G loss: 0.859570]\n",
            "2732 [D loss: 0.600543, acc.: 62.50%] [G loss: 0.858465]\n",
            "2733 [D loss: 0.596992, acc.: 62.50%] [G loss: 0.905775]\n",
            "2734 [D loss: 0.593893, acc.: 62.50%] [G loss: 0.895446]\n",
            "2735 [D loss: 0.597971, acc.: 62.50%] [G loss: 0.934568]\n",
            "2736 [D loss: 0.617533, acc.: 59.38%] [G loss: 0.863467]\n",
            "2737 [D loss: 0.609061, acc.: 64.06%] [G loss: 0.804386]\n",
            "2738 [D loss: 0.594225, acc.: 62.50%] [G loss: 0.811712]\n",
            "2739 [D loss: 0.608695, acc.: 59.38%] [G loss: 0.757376]\n",
            "2740 [D loss: 0.616722, acc.: 56.25%] [G loss: 0.798643]\n",
            "2741 [D loss: 0.622458, acc.: 59.38%] [G loss: 0.812221]\n",
            "2742 [D loss: 0.608118, acc.: 62.50%] [G loss: 0.858695]\n",
            "2743 [D loss: 0.600166, acc.: 62.50%] [G loss: 0.898847]\n",
            "2744 [D loss: 0.592232, acc.: 62.50%] [G loss: 0.959077]\n",
            "2745 [D loss: 0.585082, acc.: 62.50%] [G loss: 1.041007]\n",
            "2746 [D loss: 0.598183, acc.: 62.50%] [G loss: 1.000864]\n",
            "2747 [D loss: 0.607542, acc.: 62.50%] [G loss: 0.968642]\n",
            "2748 [D loss: 0.615700, acc.: 62.50%] [G loss: 0.914937]\n",
            "2749 [D loss: 0.601453, acc.: 62.50%] [G loss: 0.892390]\n",
            "2750 [D loss: 0.602650, acc.: 62.50%] [G loss: 0.887717]\n",
            "2751 [D loss: 0.603424, acc.: 62.50%] [G loss: 0.877433]\n",
            "2752 [D loss: 0.598425, acc.: 62.50%] [G loss: 0.882708]\n",
            "2753 [D loss: 0.604200, acc.: 62.50%] [G loss: 0.866104]\n",
            "2754 [D loss: 0.601821, acc.: 62.50%] [G loss: 0.857104]\n",
            "2755 [D loss: 0.598848, acc.: 62.50%] [G loss: 0.887241]\n",
            "2756 [D loss: 0.600501, acc.: 62.50%] [G loss: 0.879585]\n",
            "2757 [D loss: 0.601823, acc.: 62.50%] [G loss: 0.869423]\n",
            "2758 [D loss: 0.605168, acc.: 62.50%] [G loss: 0.861420]\n",
            "2759 [D loss: 0.603872, acc.: 62.50%] [G loss: 0.858600]\n",
            "2760 [D loss: 0.604111, acc.: 62.50%] [G loss: 0.852003]\n",
            "2761 [D loss: 0.602477, acc.: 62.50%] [G loss: 0.848218]\n",
            "2762 [D loss: 0.601463, acc.: 62.50%] [G loss: 0.846939]\n",
            "2763 [D loss: 0.600020, acc.: 62.50%] [G loss: 0.847560]\n",
            "2764 [D loss: 0.602167, acc.: 62.50%] [G loss: 0.849770]\n",
            "2765 [D loss: 0.603360, acc.: 62.50%] [G loss: 0.844980]\n",
            "2766 [D loss: 0.602850, acc.: 62.50%] [G loss: 0.848139]\n",
            "2767 [D loss: 0.600907, acc.: 62.50%] [G loss: 0.864884]\n",
            "2768 [D loss: 0.603082, acc.: 62.50%] [G loss: 0.865702]\n",
            "2769 [D loss: 0.600093, acc.: 62.50%] [G loss: 0.874240]\n",
            "2770 [D loss: 0.601141, acc.: 62.50%] [G loss: 0.860454]\n",
            "2771 [D loss: 0.603870, acc.: 62.50%] [G loss: 0.855831]\n",
            "2772 [D loss: 0.602558, acc.: 62.50%] [G loss: 0.853264]\n",
            "2773 [D loss: 0.600045, acc.: 62.50%] [G loss: 0.849080]\n",
            "2774 [D loss: 0.600781, acc.: 62.50%] [G loss: 0.846830]\n",
            "2775 [D loss: 0.603259, acc.: 62.50%] [G loss: 0.843931]\n",
            "2776 [D loss: 0.601955, acc.: 62.50%] [G loss: 0.844027]\n",
            "2777 [D loss: 0.603712, acc.: 62.50%] [G loss: 0.845842]\n",
            "2778 [D loss: 0.602887, acc.: 62.50%] [G loss: 0.849285]\n",
            "2779 [D loss: 0.602414, acc.: 62.50%] [G loss: 0.859139]\n",
            "2780 [D loss: 0.602827, acc.: 62.50%] [G loss: 0.864491]\n",
            "2781 [D loss: 0.602159, acc.: 62.50%] [G loss: 0.855140]\n",
            "2782 [D loss: 0.601571, acc.: 62.50%] [G loss: 0.848019]\n",
            "2783 [D loss: 0.601851, acc.: 62.50%] [G loss: 0.847138]\n",
            "2784 [D loss: 0.602587, acc.: 62.50%] [G loss: 0.839408]\n",
            "2785 [D loss: 0.601863, acc.: 62.50%] [G loss: 0.838564]\n",
            "2786 [D loss: 0.599487, acc.: 62.50%] [G loss: 0.832606]\n",
            "2787 [D loss: 0.601277, acc.: 62.50%] [G loss: 0.832650]\n",
            "2788 [D loss: 0.601237, acc.: 62.50%] [G loss: 0.835082]\n",
            "2789 [D loss: 0.600199, acc.: 62.50%] [G loss: 0.833935]\n",
            "2790 [D loss: 0.604297, acc.: 62.50%] [G loss: 0.830434]\n",
            "2791 [D loss: 0.600420, acc.: 62.50%] [G loss: 0.831054]\n",
            "2792 [D loss: 0.602342, acc.: 62.50%] [G loss: 0.834658]\n",
            "2793 [D loss: 0.602798, acc.: 62.50%] [G loss: 0.838508]\n",
            "2794 [D loss: 0.601609, acc.: 62.50%] [G loss: 0.847594]\n",
            "2795 [D loss: 0.602204, acc.: 62.50%] [G loss: 0.848592]\n",
            "2796 [D loss: 0.601556, acc.: 62.50%] [G loss: 0.852806]\n",
            "2797 [D loss: 0.601564, acc.: 62.50%] [G loss: 0.852426]\n",
            "2798 [D loss: 0.601413, acc.: 62.50%] [G loss: 0.846056]\n",
            "2799 [D loss: 0.601515, acc.: 62.50%] [G loss: 0.843489]\n",
            "2800 [D loss: 0.600467, acc.: 62.50%] [G loss: 0.842309]\n",
            "generated_data\n",
            "2801 [D loss: 0.599344, acc.: 62.50%] [G loss: 0.837805]\n",
            "2802 [D loss: 0.600056, acc.: 62.50%] [G loss: 0.831818]\n",
            "2803 [D loss: 0.599193, acc.: 62.50%] [G loss: 0.835950]\n",
            "2804 [D loss: 0.606852, acc.: 62.50%] [G loss: 0.835365]\n",
            "2805 [D loss: 0.600765, acc.: 62.50%] [G loss: 0.835414]\n",
            "2806 [D loss: 0.601754, acc.: 62.50%] [G loss: 0.839161]\n",
            "2807 [D loss: 0.601496, acc.: 62.50%] [G loss: 0.837840]\n",
            "2808 [D loss: 0.599773, acc.: 62.50%] [G loss: 0.839679]\n",
            "2809 [D loss: 0.601847, acc.: 62.50%] [G loss: 0.844818]\n",
            "2810 [D loss: 0.597315, acc.: 62.50%] [G loss: 0.837234]\n",
            "2811 [D loss: 0.601950, acc.: 62.50%] [G loss: 0.838258]\n",
            "2812 [D loss: 0.601652, acc.: 62.50%] [G loss: 0.843237]\n",
            "2813 [D loss: 0.601648, acc.: 62.50%] [G loss: 0.837935]\n",
            "2814 [D loss: 0.603553, acc.: 62.50%] [G loss: 0.839585]\n",
            "2815 [D loss: 0.603184, acc.: 62.50%] [G loss: 0.842738]\n",
            "2816 [D loss: 0.601138, acc.: 62.50%] [G loss: 0.840793]\n",
            "2817 [D loss: 0.600172, acc.: 62.50%] [G loss: 0.843206]\n",
            "2818 [D loss: 0.600907, acc.: 62.50%] [G loss: 0.841931]\n",
            "2819 [D loss: 0.599385, acc.: 62.50%] [G loss: 0.841798]\n",
            "2820 [D loss: 0.599605, acc.: 62.50%] [G loss: 0.840726]\n",
            "2821 [D loss: 0.600427, acc.: 62.50%] [G loss: 0.845269]\n",
            "2822 [D loss: 0.601107, acc.: 62.50%] [G loss: 0.847525]\n",
            "2823 [D loss: 0.599238, acc.: 62.50%] [G loss: 0.842079]\n",
            "2824 [D loss: 0.600267, acc.: 62.50%] [G loss: 0.839688]\n",
            "2825 [D loss: 0.599922, acc.: 62.50%] [G loss: 0.835462]\n",
            "2826 [D loss: 0.601544, acc.: 62.50%] [G loss: 0.840982]\n",
            "2827 [D loss: 0.600848, acc.: 62.50%] [G loss: 0.831238]\n",
            "2828 [D loss: 0.599458, acc.: 62.50%] [G loss: 0.828028]\n",
            "2829 [D loss: 0.600687, acc.: 62.50%] [G loss: 0.833889]\n",
            "2830 [D loss: 0.604031, acc.: 62.50%] [G loss: 0.837504]\n",
            "2831 [D loss: 0.600234, acc.: 62.50%] [G loss: 0.839202]\n",
            "2832 [D loss: 0.602781, acc.: 62.50%] [G loss: 0.842117]\n",
            "2833 [D loss: 0.602631, acc.: 62.50%] [G loss: 0.839819]\n",
            "2834 [D loss: 0.599677, acc.: 62.50%] [G loss: 0.839839]\n",
            "2835 [D loss: 0.598935, acc.: 62.50%] [G loss: 0.836023]\n",
            "2836 [D loss: 0.599342, acc.: 62.50%] [G loss: 0.842451]\n",
            "2837 [D loss: 0.600040, acc.: 62.50%] [G loss: 0.841211]\n",
            "2838 [D loss: 0.601884, acc.: 62.50%] [G loss: 0.840606]\n",
            "2839 [D loss: 0.598795, acc.: 62.50%] [G loss: 0.843883]\n",
            "2840 [D loss: 0.598771, acc.: 62.50%] [G loss: 0.843965]\n",
            "2841 [D loss: 0.602251, acc.: 62.50%] [G loss: 0.848407]\n",
            "2842 [D loss: 0.599646, acc.: 62.50%] [G loss: 0.853171]\n",
            "2843 [D loss: 0.601400, acc.: 62.50%] [G loss: 0.857335]\n",
            "2844 [D loss: 0.603092, acc.: 62.50%] [G loss: 0.857309]\n",
            "2845 [D loss: 0.599486, acc.: 62.50%] [G loss: 0.851658]\n",
            "2846 [D loss: 0.601091, acc.: 62.50%] [G loss: 0.848652]\n",
            "2847 [D loss: 0.602252, acc.: 62.50%] [G loss: 0.849778]\n",
            "2848 [D loss: 0.601417, acc.: 62.50%] [G loss: 0.844593]\n",
            "2849 [D loss: 0.606698, acc.: 62.50%] [G loss: 0.846120]\n",
            "2850 [D loss: 0.597911, acc.: 62.50%] [G loss: 0.847675]\n",
            "2851 [D loss: 0.599134, acc.: 62.50%] [G loss: 0.849335]\n",
            "2852 [D loss: 0.601773, acc.: 62.50%] [G loss: 0.846766]\n",
            "2853 [D loss: 0.600544, acc.: 62.50%] [G loss: 0.849805]\n",
            "2854 [D loss: 0.598819, acc.: 62.50%] [G loss: 0.855019]\n",
            "2855 [D loss: 0.601329, acc.: 62.50%] [G loss: 0.851017]\n",
            "2856 [D loss: 0.598192, acc.: 62.50%] [G loss: 0.906446]\n",
            "2857 [D loss: 0.630183, acc.: 62.50%] [G loss: 0.851900]\n",
            "2858 [D loss: 0.602851, acc.: 62.50%] [G loss: 0.848139]\n",
            "2859 [D loss: 0.599954, acc.: 62.50%] [G loss: 0.853950]\n",
            "2860 [D loss: 0.599214, acc.: 62.50%] [G loss: 0.855388]\n",
            "2861 [D loss: 0.600055, acc.: 62.50%] [G loss: 0.850571]\n",
            "2862 [D loss: 0.600973, acc.: 62.50%] [G loss: 0.852887]\n",
            "2863 [D loss: 0.601202, acc.: 62.50%] [G loss: 0.851410]\n",
            "2864 [D loss: 0.601683, acc.: 62.50%] [G loss: 0.849562]\n",
            "2865 [D loss: 0.600246, acc.: 62.50%] [G loss: 0.846880]\n",
            "2866 [D loss: 0.601989, acc.: 62.50%] [G loss: 0.850376]\n",
            "2867 [D loss: 0.602059, acc.: 62.50%] [G loss: 0.850188]\n",
            "2868 [D loss: 0.598062, acc.: 62.50%] [G loss: 0.847197]\n",
            "2869 [D loss: 0.601265, acc.: 62.50%] [G loss: 0.843597]\n",
            "2870 [D loss: 0.601716, acc.: 62.50%] [G loss: 0.845306]\n",
            "2871 [D loss: 0.600464, acc.: 62.50%] [G loss: 0.842244]\n",
            "2872 [D loss: 0.601206, acc.: 60.94%] [G loss: 0.842917]\n",
            "2873 [D loss: 0.600905, acc.: 62.50%] [G loss: 0.848613]\n",
            "2874 [D loss: 0.600683, acc.: 62.50%] [G loss: 0.860249]\n",
            "2875 [D loss: 0.601878, acc.: 62.50%] [G loss: 0.845885]\n",
            "2876 [D loss: 0.599063, acc.: 62.50%] [G loss: 0.850764]\n",
            "2877 [D loss: 0.596128, acc.: 62.50%] [G loss: 0.843499]\n",
            "2878 [D loss: 0.600026, acc.: 62.50%] [G loss: 0.853897]\n",
            "2879 [D loss: 0.592955, acc.: 62.50%] [G loss: 0.846124]\n",
            "2880 [D loss: 0.600014, acc.: 62.50%] [G loss: 0.854950]\n",
            "2881 [D loss: 0.600394, acc.: 62.50%] [G loss: 0.838307]\n",
            "2882 [D loss: 0.603004, acc.: 62.50%] [G loss: 0.833168]\n",
            "2883 [D loss: 0.603001, acc.: 62.50%] [G loss: 0.832493]\n",
            "2884 [D loss: 0.599459, acc.: 62.50%] [G loss: 0.834788]\n",
            "2885 [D loss: 0.602922, acc.: 62.50%] [G loss: 0.831068]\n",
            "2886 [D loss: 0.601344, acc.: 62.50%] [G loss: 0.837566]\n",
            "2887 [D loss: 0.600234, acc.: 62.50%] [G loss: 0.828640]\n",
            "2888 [D loss: 0.601969, acc.: 62.50%] [G loss: 0.837890]\n",
            "2889 [D loss: 0.599040, acc.: 62.50%] [G loss: 0.838114]\n",
            "2890 [D loss: 0.600879, acc.: 62.50%] [G loss: 0.840160]\n",
            "2891 [D loss: 0.600598, acc.: 62.50%] [G loss: 0.836509]\n",
            "2892 [D loss: 0.603475, acc.: 62.50%] [G loss: 0.834716]\n",
            "2893 [D loss: 0.600829, acc.: 62.50%] [G loss: 0.843874]\n",
            "2894 [D loss: 0.599689, acc.: 62.50%] [G loss: 0.845710]\n",
            "2895 [D loss: 0.597184, acc.: 62.50%] [G loss: 0.850206]\n",
            "2896 [D loss: 0.601650, acc.: 62.50%] [G loss: 0.850527]\n",
            "2897 [D loss: 0.599166, acc.: 62.50%] [G loss: 0.850841]\n",
            "2898 [D loss: 0.597369, acc.: 62.50%] [G loss: 0.839660]\n",
            "2899 [D loss: 0.603096, acc.: 60.94%] [G loss: 0.845739]\n",
            "2900 [D loss: 0.605005, acc.: 62.50%] [G loss: 0.855402]\n",
            "generated_data\n",
            "2901 [D loss: 0.601528, acc.: 62.50%] [G loss: 0.848018]\n",
            "2902 [D loss: 0.600091, acc.: 62.50%] [G loss: 0.842659]\n",
            "2903 [D loss: 0.599232, acc.: 62.50%] [G loss: 0.845797]\n",
            "2904 [D loss: 0.593467, acc.: 62.50%] [G loss: 0.879559]\n",
            "2905 [D loss: 0.597054, acc.: 62.50%] [G loss: 0.910076]\n",
            "2906 [D loss: 0.620814, acc.: 62.50%] [G loss: 0.860628]\n",
            "2907 [D loss: 0.606161, acc.: 62.50%] [G loss: 0.847023]\n",
            "2908 [D loss: 0.605713, acc.: 62.50%] [G loss: 0.844365]\n",
            "2909 [D loss: 0.600522, acc.: 62.50%] [G loss: 0.844748]\n",
            "2910 [D loss: 0.600897, acc.: 62.50%] [G loss: 0.844098]\n",
            "2911 [D loss: 0.600488, acc.: 62.50%] [G loss: 0.842202]\n",
            "2912 [D loss: 0.601235, acc.: 62.50%] [G loss: 0.841318]\n",
            "2913 [D loss: 0.602926, acc.: 62.50%] [G loss: 0.841886]\n",
            "2914 [D loss: 0.602691, acc.: 62.50%] [G loss: 0.838380]\n",
            "2915 [D loss: 0.604713, acc.: 62.50%] [G loss: 0.839860]\n",
            "2916 [D loss: 0.601221, acc.: 62.50%] [G loss: 0.839757]\n",
            "2917 [D loss: 0.597977, acc.: 62.50%] [G loss: 0.839174]\n",
            "2918 [D loss: 0.600838, acc.: 62.50%] [G loss: 0.830887]\n",
            "2919 [D loss: 0.598542, acc.: 62.50%] [G loss: 0.833932]\n",
            "2920 [D loss: 0.602970, acc.: 62.50%] [G loss: 0.838610]\n",
            "2921 [D loss: 0.599281, acc.: 62.50%] [G loss: 0.838386]\n",
            "2922 [D loss: 0.601890, acc.: 62.50%] [G loss: 0.841975]\n",
            "2923 [D loss: 0.599492, acc.: 62.50%] [G loss: 0.841546]\n",
            "2924 [D loss: 0.600991, acc.: 62.50%] [G loss: 0.843364]\n",
            "2925 [D loss: 0.598954, acc.: 62.50%] [G loss: 0.840284]\n",
            "2926 [D loss: 0.600742, acc.: 62.50%] [G loss: 0.846259]\n",
            "2927 [D loss: 0.598470, acc.: 62.50%] [G loss: 0.845490]\n",
            "2928 [D loss: 0.600298, acc.: 62.50%] [G loss: 0.848134]\n",
            "2929 [D loss: 0.601054, acc.: 62.50%] [G loss: 0.850181]\n",
            "2930 [D loss: 0.601994, acc.: 62.50%] [G loss: 0.845974]\n",
            "2931 [D loss: 0.600123, acc.: 62.50%] [G loss: 0.851953]\n",
            "2932 [D loss: 0.599894, acc.: 62.50%] [G loss: 0.850340]\n",
            "2933 [D loss: 0.600345, acc.: 62.50%] [G loss: 0.853270]\n",
            "2934 [D loss: 0.599370, acc.: 62.50%] [G loss: 0.852474]\n",
            "2935 [D loss: 0.601070, acc.: 62.50%] [G loss: 0.857684]\n",
            "2936 [D loss: 0.600411, acc.: 62.50%] [G loss: 0.855788]\n",
            "2937 [D loss: 0.599812, acc.: 62.50%] [G loss: 0.852801]\n",
            "2938 [D loss: 0.598637, acc.: 62.50%] [G loss: 0.852367]\n",
            "2939 [D loss: 0.599922, acc.: 62.50%] [G loss: 0.848482]\n",
            "2940 [D loss: 0.601371, acc.: 62.50%] [G loss: 0.848581]\n",
            "2941 [D loss: 0.599741, acc.: 62.50%] [G loss: 0.851898]\n",
            "2942 [D loss: 0.600307, acc.: 62.50%] [G loss: 0.854146]\n",
            "2943 [D loss: 0.597933, acc.: 62.50%] [G loss: 0.865079]\n",
            "2944 [D loss: 0.596533, acc.: 62.50%] [G loss: 0.868871]\n",
            "2945 [D loss: 0.597387, acc.: 62.50%] [G loss: 0.864222]\n",
            "2946 [D loss: 0.598029, acc.: 62.50%] [G loss: 0.859032]\n",
            "2947 [D loss: 0.602249, acc.: 62.50%] [G loss: 0.866209]\n",
            "2948 [D loss: 0.598948, acc.: 62.50%] [G loss: 0.858240]\n",
            "2949 [D loss: 0.602087, acc.: 62.50%] [G loss: 0.861380]\n",
            "2950 [D loss: 0.601692, acc.: 62.50%] [G loss: 0.870816]\n",
            "2951 [D loss: 0.600475, acc.: 62.50%] [G loss: 0.853553]\n",
            "2952 [D loss: 0.600363, acc.: 62.50%] [G loss: 0.854805]\n",
            "2953 [D loss: 0.598310, acc.: 62.50%] [G loss: 0.856226]\n",
            "2954 [D loss: 0.598767, acc.: 62.50%] [G loss: 0.855226]\n",
            "2955 [D loss: 0.597462, acc.: 62.50%] [G loss: 0.858768]\n",
            "2956 [D loss: 0.601091, acc.: 62.50%] [G loss: 0.863100]\n",
            "2957 [D loss: 0.599916, acc.: 62.50%] [G loss: 0.854293]\n",
            "2958 [D loss: 0.600505, acc.: 62.50%] [G loss: 0.853147]\n",
            "2959 [D loss: 0.600735, acc.: 62.50%] [G loss: 0.849009]\n",
            "2960 [D loss: 0.600065, acc.: 62.50%] [G loss: 0.848795]\n",
            "2961 [D loss: 0.598291, acc.: 62.50%] [G loss: 0.845096]\n",
            "2962 [D loss: 0.600072, acc.: 62.50%] [G loss: 0.840123]\n",
            "2963 [D loss: 0.600087, acc.: 62.50%] [G loss: 0.834868]\n",
            "2964 [D loss: 0.600725, acc.: 62.50%] [G loss: 0.836857]\n",
            "2965 [D loss: 0.601636, acc.: 62.50%] [G loss: 0.836464]\n",
            "2966 [D loss: 0.599313, acc.: 62.50%] [G loss: 0.835833]\n",
            "2967 [D loss: 0.599803, acc.: 62.50%] [G loss: 0.837423]\n",
            "2968 [D loss: 0.597400, acc.: 62.50%] [G loss: 0.832940]\n",
            "2969 [D loss: 0.600334, acc.: 62.50%] [G loss: 0.840523]\n",
            "2970 [D loss: 0.604154, acc.: 62.50%] [G loss: 0.844684]\n",
            "2971 [D loss: 0.601580, acc.: 62.50%] [G loss: 0.847013]\n",
            "2972 [D loss: 0.598130, acc.: 62.50%] [G loss: 0.849947]\n",
            "2973 [D loss: 0.598040, acc.: 62.50%] [G loss: 0.848673]\n",
            "2974 [D loss: 0.602110, acc.: 62.50%] [G loss: 0.849128]\n",
            "2975 [D loss: 0.601768, acc.: 62.50%] [G loss: 0.850459]\n",
            "2976 [D loss: 0.598817, acc.: 62.50%] [G loss: 0.851279]\n",
            "2977 [D loss: 0.600435, acc.: 62.50%] [G loss: 0.848342]\n",
            "2978 [D loss: 0.598134, acc.: 62.50%] [G loss: 0.845815]\n",
            "2979 [D loss: 0.600365, acc.: 62.50%] [G loss: 0.842510]\n",
            "2980 [D loss: 0.598619, acc.: 62.50%] [G loss: 0.833979]\n",
            "2981 [D loss: 0.600563, acc.: 62.50%] [G loss: 0.840282]\n",
            "2982 [D loss: 0.603938, acc.: 62.50%] [G loss: 0.842566]\n",
            "2983 [D loss: 0.601586, acc.: 62.50%] [G loss: 0.841520]\n",
            "2984 [D loss: 0.598658, acc.: 62.50%] [G loss: 0.845731]\n",
            "2985 [D loss: 0.599331, acc.: 62.50%] [G loss: 0.846806]\n",
            "2986 [D loss: 0.599353, acc.: 62.50%] [G loss: 0.838950]\n",
            "2987 [D loss: 0.600217, acc.: 62.50%] [G loss: 0.838825]\n",
            "2988 [D loss: 0.599760, acc.: 62.50%] [G loss: 0.843476]\n",
            "2989 [D loss: 0.598732, acc.: 62.50%] [G loss: 0.838571]\n",
            "2990 [D loss: 0.599153, acc.: 62.50%] [G loss: 0.842108]\n",
            "2991 [D loss: 0.601837, acc.: 62.50%] [G loss: 0.840418]\n",
            "2992 [D loss: 0.597049, acc.: 62.50%] [G loss: 0.840042]\n",
            "2993 [D loss: 0.598748, acc.: 62.50%] [G loss: 0.835321]\n",
            "2994 [D loss: 0.598754, acc.: 62.50%] [G loss: 0.835557]\n",
            "2995 [D loss: 0.598055, acc.: 62.50%] [G loss: 0.829615]\n",
            "2996 [D loss: 0.609087, acc.: 59.38%] [G loss: 0.837708]\n",
            "2997 [D loss: 0.599750, acc.: 62.50%] [G loss: 0.838094]\n",
            "2998 [D loss: 0.599061, acc.: 62.50%] [G loss: 0.840864]\n",
            "2999 [D loss: 0.601755, acc.: 62.50%] [G loss: 0.840193]\n",
            "3000 [D loss: 0.600388, acc.: 62.50%] [G loss: 0.850372]\n",
            "generated_data\n",
            "3001 [D loss: 0.603209, acc.: 62.50%] [G loss: 0.840721]\n",
            "3002 [D loss: 0.601142, acc.: 62.50%] [G loss: 0.842144]\n",
            "3003 [D loss: 0.602455, acc.: 62.50%] [G loss: 0.842857]\n",
            "3004 [D loss: 0.600473, acc.: 62.50%] [G loss: 0.843430]\n",
            "3005 [D loss: 0.599100, acc.: 62.50%] [G loss: 0.844376]\n",
            "3006 [D loss: 0.599035, acc.: 62.50%] [G loss: 0.845182]\n",
            "3007 [D loss: 0.600337, acc.: 62.50%] [G loss: 0.844812]\n",
            "3008 [D loss: 0.600253, acc.: 62.50%] [G loss: 0.846410]\n",
            "3009 [D loss: 0.598959, acc.: 62.50%] [G loss: 0.843304]\n",
            "3010 [D loss: 0.599069, acc.: 62.50%] [G loss: 0.846067]\n",
            "3011 [D loss: 0.598925, acc.: 62.50%] [G loss: 0.844421]\n",
            "3012 [D loss: 0.600412, acc.: 62.50%] [G loss: 0.843799]\n",
            "3013 [D loss: 0.600006, acc.: 62.50%] [G loss: 0.837852]\n",
            "3014 [D loss: 0.599885, acc.: 62.50%] [G loss: 0.839089]\n",
            "3015 [D loss: 0.598642, acc.: 62.50%] [G loss: 0.838408]\n",
            "3016 [D loss: 0.599818, acc.: 62.50%] [G loss: 0.838347]\n",
            "3017 [D loss: 0.600777, acc.: 62.50%] [G loss: 0.840385]\n",
            "3018 [D loss: 0.599366, acc.: 62.50%] [G loss: 0.839398]\n",
            "3019 [D loss: 0.600398, acc.: 62.50%] [G loss: 0.841142]\n",
            "3020 [D loss: 0.599569, acc.: 62.50%] [G loss: 0.842214]\n",
            "3021 [D loss: 0.599995, acc.: 62.50%] [G loss: 0.843751]\n",
            "3022 [D loss: 0.600048, acc.: 62.50%] [G loss: 0.843424]\n",
            "3023 [D loss: 0.599450, acc.: 62.50%] [G loss: 0.841665]\n",
            "3024 [D loss: 0.599379, acc.: 62.50%] [G loss: 0.838641]\n",
            "3025 [D loss: 0.600181, acc.: 62.50%] [G loss: 0.841537]\n",
            "3026 [D loss: 0.599917, acc.: 62.50%] [G loss: 0.840525]\n",
            "3027 [D loss: 0.599820, acc.: 62.50%] [G loss: 0.841947]\n",
            "3028 [D loss: 0.599357, acc.: 62.50%] [G loss: 0.844360]\n",
            "3029 [D loss: 0.599833, acc.: 62.50%] [G loss: 0.846036]\n",
            "3030 [D loss: 0.600690, acc.: 62.50%] [G loss: 0.842683]\n",
            "3031 [D loss: 0.599916, acc.: 62.50%] [G loss: 0.843084]\n",
            "3032 [D loss: 0.600764, acc.: 62.50%] [G loss: 0.842602]\n",
            "3033 [D loss: 0.599684, acc.: 62.50%] [G loss: 0.842634]\n",
            "3034 [D loss: 0.599187, acc.: 62.50%] [G loss: 0.843307]\n",
            "3035 [D loss: 0.599761, acc.: 62.50%] [G loss: 0.844787]\n",
            "3036 [D loss: 0.599982, acc.: 62.50%] [G loss: 0.847250]\n",
            "3037 [D loss: 0.599629, acc.: 62.50%] [G loss: 0.846913]\n",
            "3038 [D loss: 0.599634, acc.: 62.50%] [G loss: 0.845122]\n",
            "3039 [D loss: 0.599186, acc.: 62.50%] [G loss: 0.846396]\n",
            "3040 [D loss: 0.598886, acc.: 62.50%] [G loss: 0.845769]\n",
            "3041 [D loss: 0.599015, acc.: 62.50%] [G loss: 0.847707]\n",
            "3042 [D loss: 0.599265, acc.: 62.50%] [G loss: 0.848558]\n",
            "3043 [D loss: 0.600364, acc.: 62.50%] [G loss: 0.845366]\n",
            "3044 [D loss: 0.599500, acc.: 62.50%] [G loss: 0.845059]\n",
            "3045 [D loss: 0.599767, acc.: 62.50%] [G loss: 0.843756]\n",
            "3046 [D loss: 0.599832, acc.: 62.50%] [G loss: 0.842592]\n",
            "3047 [D loss: 0.600526, acc.: 62.50%] [G loss: 0.845317]\n",
            "3048 [D loss: 0.598755, acc.: 62.50%] [G loss: 0.848951]\n",
            "3049 [D loss: 0.599038, acc.: 62.50%] [G loss: 0.850542]\n",
            "3050 [D loss: 0.599174, acc.: 62.50%] [G loss: 0.855699]\n",
            "3051 [D loss: 0.600110, acc.: 62.50%] [G loss: 0.851714]\n",
            "3052 [D loss: 0.598580, acc.: 62.50%] [G loss: 0.852905]\n",
            "3053 [D loss: 0.600856, acc.: 62.50%] [G loss: 0.853494]\n",
            "3054 [D loss: 0.602567, acc.: 62.50%] [G loss: 0.845262]\n",
            "3055 [D loss: 0.598620, acc.: 62.50%] [G loss: 0.847021]\n",
            "3056 [D loss: 0.599832, acc.: 62.50%] [G loss: 0.846826]\n",
            "3057 [D loss: 0.598190, acc.: 62.50%] [G loss: 0.843216]\n",
            "3058 [D loss: 0.600789, acc.: 62.50%] [G loss: 0.842496]\n",
            "3059 [D loss: 0.599753, acc.: 62.50%] [G loss: 0.843340]\n",
            "3060 [D loss: 0.598001, acc.: 62.50%] [G loss: 0.845486]\n",
            "3061 [D loss: 0.599437, acc.: 62.50%] [G loss: 0.843871]\n",
            "3062 [D loss: 0.599108, acc.: 62.50%] [G loss: 0.844192]\n",
            "3063 [D loss: 0.599262, acc.: 62.50%] [G loss: 0.840388]\n",
            "3064 [D loss: 0.599167, acc.: 62.50%] [G loss: 0.842478]\n",
            "3065 [D loss: 0.598251, acc.: 62.50%] [G loss: 0.840611]\n",
            "3066 [D loss: 0.601300, acc.: 62.50%] [G loss: 0.843422]\n",
            "3067 [D loss: 0.598826, acc.: 62.50%] [G loss: 0.844106]\n",
            "3068 [D loss: 0.599214, acc.: 62.50%] [G loss: 0.846848]\n",
            "3069 [D loss: 0.599489, acc.: 62.50%] [G loss: 0.845744]\n",
            "3070 [D loss: 0.600335, acc.: 62.50%] [G loss: 0.847390]\n",
            "3071 [D loss: 0.599447, acc.: 62.50%] [G loss: 0.845187]\n",
            "3072 [D loss: 0.598837, acc.: 62.50%] [G loss: 0.844265]\n",
            "3073 [D loss: 0.599700, acc.: 62.50%] [G loss: 0.845782]\n",
            "3074 [D loss: 0.599818, acc.: 62.50%] [G loss: 0.845078]\n",
            "3075 [D loss: 0.600188, acc.: 62.50%] [G loss: 0.851646]\n",
            "3076 [D loss: 0.599491, acc.: 62.50%] [G loss: 0.851434]\n",
            "3077 [D loss: 0.599429, acc.: 62.50%] [G loss: 0.851735]\n",
            "3078 [D loss: 0.597997, acc.: 62.50%] [G loss: 0.850074]\n",
            "3079 [D loss: 0.599307, acc.: 62.50%] [G loss: 0.851304]\n",
            "3080 [D loss: 0.601098, acc.: 62.50%] [G loss: 0.852399]\n",
            "3081 [D loss: 0.599896, acc.: 62.50%] [G loss: 0.853952]\n",
            "3082 [D loss: 0.599260, acc.: 62.50%] [G loss: 0.849262]\n",
            "3083 [D loss: 0.599275, acc.: 62.50%] [G loss: 0.846370]\n",
            "3084 [D loss: 0.600384, acc.: 62.50%] [G loss: 0.845703]\n",
            "3085 [D loss: 0.598284, acc.: 62.50%] [G loss: 0.848676]\n",
            "3086 [D loss: 0.598562, acc.: 62.50%] [G loss: 0.848346]\n",
            "3087 [D loss: 0.599383, acc.: 62.50%] [G loss: 0.848920]\n",
            "3088 [D loss: 0.599032, acc.: 62.50%] [G loss: 0.848081]\n",
            "3089 [D loss: 0.600088, acc.: 62.50%] [G loss: 0.846473]\n",
            "3090 [D loss: 0.599005, acc.: 62.50%] [G loss: 0.848714]\n",
            "3091 [D loss: 0.598242, acc.: 62.50%] [G loss: 0.846744]\n",
            "3092 [D loss: 0.598641, acc.: 62.50%] [G loss: 0.846180]\n",
            "3093 [D loss: 0.600089, acc.: 62.50%] [G loss: 0.920732]\n",
            "3094 [D loss: 0.631523, acc.: 62.50%] [G loss: 0.843946]\n",
            "3095 [D loss: 0.598009, acc.: 62.50%] [G loss: 0.840277]\n",
            "3096 [D loss: 0.600540, acc.: 62.50%] [G loss: 0.843281]\n",
            "3097 [D loss: 0.600265, acc.: 62.50%] [G loss: 0.841319]\n",
            "3098 [D loss: 0.599031, acc.: 62.50%] [G loss: 0.840349]\n",
            "3099 [D loss: 0.600748, acc.: 62.50%] [G loss: 0.842437]\n",
            "3100 [D loss: 0.599514, acc.: 62.50%] [G loss: 0.841031]\n",
            "generated_data\n",
            "3101 [D loss: 0.600426, acc.: 62.50%] [G loss: 0.843803]\n",
            "3102 [D loss: 0.597951, acc.: 62.50%] [G loss: 0.845138]\n",
            "3103 [D loss: 0.600609, acc.: 62.50%] [G loss: 0.856300]\n",
            "3104 [D loss: 0.599309, acc.: 62.50%] [G loss: 0.857069]\n",
            "3105 [D loss: 0.600354, acc.: 62.50%] [G loss: 0.860740]\n",
            "3106 [D loss: 0.599174, acc.: 62.50%] [G loss: 0.862661]\n",
            "3107 [D loss: 0.600467, acc.: 62.50%] [G loss: 0.858564]\n",
            "3108 [D loss: 0.601789, acc.: 62.50%] [G loss: 0.850852]\n",
            "3109 [D loss: 0.599820, acc.: 62.50%] [G loss: 0.852556]\n",
            "3110 [D loss: 0.599948, acc.: 62.50%] [G loss: 0.854265]\n",
            "3111 [D loss: 0.598479, acc.: 62.50%] [G loss: 0.850416]\n",
            "3112 [D loss: 0.599230, acc.: 62.50%] [G loss: 0.847582]\n",
            "3113 [D loss: 0.598389, acc.: 62.50%] [G loss: 0.846955]\n",
            "3114 [D loss: 0.599060, acc.: 62.50%] [G loss: 0.848420]\n",
            "3115 [D loss: 0.600353, acc.: 62.50%] [G loss: 0.856179]\n",
            "3116 [D loss: 0.598909, acc.: 62.50%] [G loss: 0.846759]\n",
            "3117 [D loss: 0.598626, acc.: 62.50%] [G loss: 0.843682]\n",
            "3118 [D loss: 0.598745, acc.: 62.50%] [G loss: 0.845355]\n",
            "3119 [D loss: 0.598814, acc.: 62.50%] [G loss: 0.840809]\n",
            "3120 [D loss: 0.599802, acc.: 62.50%] [G loss: 0.839292]\n",
            "3121 [D loss: 0.599751, acc.: 62.50%] [G loss: 0.839287]\n",
            "3122 [D loss: 0.600155, acc.: 62.50%] [G loss: 0.838925]\n",
            "3123 [D loss: 0.599326, acc.: 62.50%] [G loss: 0.836650]\n",
            "3124 [D loss: 0.599427, acc.: 62.50%] [G loss: 0.837494]\n",
            "3125 [D loss: 0.598350, acc.: 62.50%] [G loss: 0.834953]\n",
            "3126 [D loss: 0.598650, acc.: 62.50%] [G loss: 0.824798]\n",
            "3127 [D loss: 0.644691, acc.: 57.81%] [G loss: 0.834140]\n",
            "3128 [D loss: 0.599397, acc.: 62.50%] [G loss: 0.847411]\n",
            "3129 [D loss: 0.597630, acc.: 62.50%] [G loss: 0.845447]\n",
            "3130 [D loss: 0.596369, acc.: 62.50%] [G loss: 0.845453]\n",
            "3131 [D loss: 0.606439, acc.: 60.94%] [G loss: 0.829625]\n",
            "3132 [D loss: 0.602908, acc.: 62.50%] [G loss: 0.844825]\n",
            "3133 [D loss: 0.594593, acc.: 62.50%] [G loss: 0.841255]\n",
            "3134 [D loss: 0.602659, acc.: 62.50%] [G loss: 0.843814]\n",
            "3135 [D loss: 0.597240, acc.: 62.50%] [G loss: 0.838995]\n",
            "3136 [D loss: 0.600312, acc.: 62.50%] [G loss: 0.843489]\n",
            "3137 [D loss: 0.600167, acc.: 62.50%] [G loss: 0.841302]\n",
            "3138 [D loss: 0.599743, acc.: 62.50%] [G loss: 0.841041]\n",
            "3139 [D loss: 0.598393, acc.: 62.50%] [G loss: 0.844429]\n",
            "3140 [D loss: 0.598152, acc.: 62.50%] [G loss: 0.844657]\n",
            "3141 [D loss: 0.599963, acc.: 62.50%] [G loss: 0.843022]\n",
            "3142 [D loss: 0.599224, acc.: 62.50%] [G loss: 0.842970]\n",
            "3143 [D loss: 0.598210, acc.: 62.50%] [G loss: 0.843654]\n",
            "3144 [D loss: 0.599643, acc.: 62.50%] [G loss: 0.840338]\n",
            "3145 [D loss: 0.599744, acc.: 62.50%] [G loss: 0.837217]\n",
            "3146 [D loss: 0.602336, acc.: 62.50%] [G loss: 0.841219]\n",
            "3147 [D loss: 0.596607, acc.: 62.50%] [G loss: 0.838346]\n",
            "3148 [D loss: 0.601219, acc.: 62.50%] [G loss: 0.838767]\n",
            "3149 [D loss: 0.597216, acc.: 62.50%] [G loss: 0.837378]\n",
            "3150 [D loss: 0.599359, acc.: 62.50%] [G loss: 0.836651]\n",
            "3151 [D loss: 0.600703, acc.: 62.50%] [G loss: 0.839779]\n",
            "3152 [D loss: 0.596960, acc.: 62.50%] [G loss: 0.837566]\n",
            "3153 [D loss: 0.599759, acc.: 62.50%] [G loss: 0.839569]\n",
            "3154 [D loss: 0.599546, acc.: 62.50%] [G loss: 0.838787]\n",
            "3155 [D loss: 0.598595, acc.: 62.50%] [G loss: 0.838283]\n",
            "3156 [D loss: 0.597697, acc.: 62.50%] [G loss: 0.834017]\n",
            "3157 [D loss: 0.598888, acc.: 62.50%] [G loss: 0.829189]\n",
            "3158 [D loss: 0.598857, acc.: 62.50%] [G loss: 0.830258]\n",
            "3159 [D loss: 0.599559, acc.: 62.50%] [G loss: 0.831957]\n",
            "3160 [D loss: 0.599271, acc.: 62.50%] [G loss: 0.827713]\n",
            "3161 [D loss: 0.599666, acc.: 62.50%] [G loss: 0.830609]\n",
            "3162 [D loss: 0.601014, acc.: 62.50%] [G loss: 0.833346]\n",
            "3163 [D loss: 0.600143, acc.: 62.50%] [G loss: 0.837059]\n",
            "3164 [D loss: 0.597760, acc.: 62.50%] [G loss: 0.838099]\n",
            "3165 [D loss: 0.598884, acc.: 62.50%] [G loss: 0.840983]\n",
            "3166 [D loss: 0.598453, acc.: 62.50%] [G loss: 0.848234]\n",
            "3167 [D loss: 0.598920, acc.: 62.50%] [G loss: 0.848404]\n",
            "3168 [D loss: 0.599487, acc.: 62.50%] [G loss: 0.849218]\n",
            "3169 [D loss: 0.597951, acc.: 62.50%] [G loss: 0.850137]\n",
            "3170 [D loss: 0.598608, acc.: 62.50%] [G loss: 0.849756]\n",
            "3171 [D loss: 0.597964, acc.: 62.50%] [G loss: 0.860194]\n",
            "3172 [D loss: 0.596339, acc.: 62.50%] [G loss: 0.871973]\n",
            "3173 [D loss: 0.599551, acc.: 62.50%] [G loss: 0.869103]\n",
            "3174 [D loss: 0.598745, acc.: 62.50%] [G loss: 0.857052]\n",
            "3175 [D loss: 0.601436, acc.: 62.50%] [G loss: 0.861549]\n",
            "3176 [D loss: 0.602081, acc.: 62.50%] [G loss: 0.853299]\n",
            "3177 [D loss: 0.599306, acc.: 62.50%] [G loss: 0.850200]\n",
            "3178 [D loss: 0.598469, acc.: 62.50%] [G loss: 0.846581]\n",
            "3179 [D loss: 0.598472, acc.: 62.50%] [G loss: 0.846240]\n",
            "3180 [D loss: 0.598183, acc.: 62.50%] [G loss: 0.844145]\n",
            "3181 [D loss: 0.599211, acc.: 62.50%] [G loss: 0.843116]\n",
            "3182 [D loss: 0.598822, acc.: 62.50%] [G loss: 0.844702]\n",
            "3183 [D loss: 0.599461, acc.: 62.50%] [G loss: 0.844173]\n",
            "3184 [D loss: 0.598964, acc.: 62.50%] [G loss: 0.841943]\n",
            "3185 [D loss: 0.598382, acc.: 62.50%] [G loss: 0.842334]\n",
            "3186 [D loss: 0.598735, acc.: 62.50%] [G loss: 0.841678]\n",
            "3187 [D loss: 0.598012, acc.: 62.50%] [G loss: 0.842169]\n",
            "3188 [D loss: 0.598032, acc.: 62.50%] [G loss: 0.841321]\n",
            "3189 [D loss: 0.598736, acc.: 62.50%] [G loss: 0.843485]\n",
            "3190 [D loss: 0.599674, acc.: 62.50%] [G loss: 0.844805]\n",
            "3191 [D loss: 0.598840, acc.: 62.50%] [G loss: 0.838028]\n",
            "3192 [D loss: 0.599287, acc.: 62.50%] [G loss: 0.846115]\n",
            "3193 [D loss: 0.600624, acc.: 62.50%] [G loss: 0.844339]\n",
            "3194 [D loss: 0.598998, acc.: 62.50%] [G loss: 0.846786]\n",
            "3195 [D loss: 0.601582, acc.: 62.50%] [G loss: 0.845518]\n",
            "3196 [D loss: 0.595930, acc.: 62.50%] [G loss: 0.854945]\n",
            "3197 [D loss: 0.595938, acc.: 62.50%] [G loss: 0.869111]\n",
            "3198 [D loss: 0.604694, acc.: 62.50%] [G loss: 0.856465]\n",
            "3199 [D loss: 0.598997, acc.: 62.50%] [G loss: 0.855082]\n",
            "3200 [D loss: 0.598405, acc.: 62.50%] [G loss: 0.854616]\n",
            "generated_data\n",
            "3201 [D loss: 0.597706, acc.: 62.50%] [G loss: 0.853223]\n",
            "3202 [D loss: 0.599666, acc.: 62.50%] [G loss: 0.852302]\n",
            "3203 [D loss: 0.601446, acc.: 62.50%] [G loss: 0.851175]\n",
            "3204 [D loss: 0.598164, acc.: 62.50%] [G loss: 0.853640]\n",
            "3205 [D loss: 0.598090, acc.: 62.50%] [G loss: 0.855694]\n",
            "3206 [D loss: 0.599679, acc.: 62.50%] [G loss: 0.851589]\n",
            "3207 [D loss: 0.598917, acc.: 62.50%] [G loss: 0.850840]\n",
            "3208 [D loss: 0.597920, acc.: 62.50%] [G loss: 0.848805]\n",
            "3209 [D loss: 0.597029, acc.: 62.50%] [G loss: 0.841464]\n",
            "3210 [D loss: 0.606776, acc.: 57.81%] [G loss: 0.830455]\n",
            "3211 [D loss: 0.600244, acc.: 62.50%] [G loss: 0.833599]\n",
            "3212 [D loss: 0.598602, acc.: 62.50%] [G loss: 0.824161]\n",
            "3213 [D loss: 0.603272, acc.: 62.50%] [G loss: 0.814865]\n",
            "3214 [D loss: 0.621725, acc.: 56.25%] [G loss: 0.833749]\n",
            "3215 [D loss: 0.595967, acc.: 62.50%] [G loss: 0.836734]\n",
            "3216 [D loss: 0.598541, acc.: 62.50%] [G loss: 0.832592]\n",
            "3217 [D loss: 0.600078, acc.: 62.50%] [G loss: 0.838857]\n",
            "3218 [D loss: 0.597885, acc.: 62.50%] [G loss: 0.852432]\n",
            "3219 [D loss: 0.597780, acc.: 62.50%] [G loss: 0.852642]\n",
            "3220 [D loss: 0.595518, acc.: 62.50%] [G loss: 0.846748]\n",
            "3221 [D loss: 0.597365, acc.: 62.50%] [G loss: 0.842987]\n",
            "3222 [D loss: 0.600037, acc.: 62.50%] [G loss: 0.846355]\n",
            "3223 [D loss: 0.598528, acc.: 62.50%] [G loss: 0.842491]\n",
            "3224 [D loss: 0.600649, acc.: 62.50%] [G loss: 0.844587]\n",
            "3225 [D loss: 0.599002, acc.: 62.50%] [G loss: 0.846722]\n",
            "3226 [D loss: 0.600654, acc.: 62.50%] [G loss: 0.847380]\n",
            "3227 [D loss: 0.599400, acc.: 62.50%] [G loss: 0.845946]\n",
            "3228 [D loss: 0.597333, acc.: 62.50%] [G loss: 0.847042]\n",
            "3229 [D loss: 0.617175, acc.: 54.69%] [G loss: 0.844842]\n",
            "3230 [D loss: 0.599673, acc.: 62.50%] [G loss: 0.845323]\n",
            "3231 [D loss: 0.595620, acc.: 62.50%] [G loss: 0.838149]\n",
            "3232 [D loss: 0.618783, acc.: 56.25%] [G loss: 0.834636]\n",
            "3233 [D loss: 0.599307, acc.: 59.38%] [G loss: 0.841734]\n",
            "3234 [D loss: 0.601094, acc.: 60.94%] [G loss: 0.840981]\n",
            "3235 [D loss: 0.599402, acc.: 60.94%] [G loss: 0.855330]\n",
            "3236 [D loss: 0.596998, acc.: 59.38%] [G loss: 0.855114]\n",
            "3237 [D loss: 0.605625, acc.: 60.94%] [G loss: 0.852233]\n",
            "3238 [D loss: 0.596754, acc.: 62.50%] [G loss: 0.853560]\n",
            "3239 [D loss: 0.598524, acc.: 62.50%] [G loss: 0.858909]\n",
            "3240 [D loss: 0.604933, acc.: 62.50%] [G loss: 0.855046]\n",
            "3241 [D loss: 0.600501, acc.: 62.50%] [G loss: 0.854742]\n",
            "3242 [D loss: 0.596707, acc.: 62.50%] [G loss: 0.858894]\n",
            "3243 [D loss: 0.598997, acc.: 62.50%] [G loss: 0.860662]\n",
            "3244 [D loss: 0.598469, acc.: 62.50%] [G loss: 0.853631]\n",
            "3245 [D loss: 0.600530, acc.: 62.50%] [G loss: 0.857446]\n",
            "3246 [D loss: 0.595323, acc.: 62.50%] [G loss: 0.866032]\n",
            "3247 [D loss: 0.599460, acc.: 62.50%] [G loss: 0.865930]\n",
            "3248 [D loss: 0.598478, acc.: 62.50%] [G loss: 0.855247]\n",
            "3249 [D loss: 0.598343, acc.: 62.50%] [G loss: 0.860356]\n",
            "3250 [D loss: 0.597891, acc.: 62.50%] [G loss: 0.861985]\n",
            "3251 [D loss: 0.599316, acc.: 62.50%] [G loss: 0.857997]\n",
            "3252 [D loss: 0.602045, acc.: 62.50%] [G loss: 0.852024]\n",
            "3253 [D loss: 0.599002, acc.: 62.50%] [G loss: 0.849789]\n",
            "3254 [D loss: 0.597362, acc.: 62.50%] [G loss: 0.852100]\n",
            "3255 [D loss: 0.598577, acc.: 62.50%] [G loss: 0.852840]\n",
            "3256 [D loss: 0.598160, acc.: 62.50%] [G loss: 0.854742]\n",
            "3257 [D loss: 0.595578, acc.: 62.50%] [G loss: 0.851652]\n",
            "3258 [D loss: 0.598347, acc.: 62.50%] [G loss: 0.853505]\n",
            "3259 [D loss: 0.597957, acc.: 62.50%] [G loss: 0.854713]\n",
            "3260 [D loss: 0.598502, acc.: 62.50%] [G loss: 0.856535]\n",
            "3261 [D loss: 0.597761, acc.: 62.50%] [G loss: 0.852865]\n",
            "3262 [D loss: 0.596694, acc.: 62.50%] [G loss: 0.855761]\n",
            "3263 [D loss: 0.595931, acc.: 62.50%] [G loss: 0.850221]\n",
            "3264 [D loss: 0.598341, acc.: 62.50%] [G loss: 0.855898]\n",
            "3265 [D loss: 0.594782, acc.: 62.50%] [G loss: 0.848172]\n",
            "3266 [D loss: 0.595257, acc.: 62.50%] [G loss: 0.858121]\n",
            "3267 [D loss: 0.595128, acc.: 62.50%] [G loss: 0.989488]\n",
            "3268 [D loss: 0.632533, acc.: 62.50%] [G loss: 0.854296]\n",
            "3269 [D loss: 0.597278, acc.: 62.50%] [G loss: 0.859784]\n",
            "3270 [D loss: 0.598211, acc.: 62.50%] [G loss: 0.866485]\n",
            "3271 [D loss: 0.600031, acc.: 62.50%] [G loss: 0.859906]\n",
            "3272 [D loss: 0.595507, acc.: 62.50%] [G loss: 0.859782]\n",
            "3273 [D loss: 0.597907, acc.: 62.50%] [G loss: 0.859298]\n",
            "3274 [D loss: 0.605148, acc.: 62.50%] [G loss: 0.855809]\n",
            "3275 [D loss: 0.596956, acc.: 62.50%] [G loss: 0.860447]\n",
            "3276 [D loss: 0.599202, acc.: 62.50%] [G loss: 0.856216]\n",
            "3277 [D loss: 0.599699, acc.: 62.50%] [G loss: 0.855575]\n",
            "3278 [D loss: 0.600051, acc.: 62.50%] [G loss: 0.858309]\n",
            "3279 [D loss: 0.598861, acc.: 62.50%] [G loss: 0.863829]\n",
            "3280 [D loss: 0.598765, acc.: 62.50%] [G loss: 0.866948]\n",
            "3281 [D loss: 0.599487, acc.: 62.50%] [G loss: 0.867677]\n",
            "3282 [D loss: 0.602350, acc.: 62.50%] [G loss: 0.869400]\n",
            "3283 [D loss: 0.603278, acc.: 62.50%] [G loss: 0.860521]\n",
            "3284 [D loss: 0.599267, acc.: 62.50%] [G loss: 0.872075]\n",
            "3285 [D loss: 0.603393, acc.: 62.50%] [G loss: 0.847566]\n",
            "3286 [D loss: 0.596161, acc.: 62.50%] [G loss: 0.862849]\n",
            "3287 [D loss: 0.600160, acc.: 62.50%] [G loss: 0.848346]\n",
            "3288 [D loss: 0.601425, acc.: 62.50%] [G loss: 0.850503]\n",
            "3289 [D loss: 0.597918, acc.: 62.50%] [G loss: 0.849988]\n",
            "3290 [D loss: 0.599077, acc.: 62.50%] [G loss: 0.849129]\n",
            "3291 [D loss: 0.603701, acc.: 60.94%] [G loss: 0.864346]\n",
            "3292 [D loss: 0.598262, acc.: 62.50%] [G loss: 0.858104]\n",
            "3293 [D loss: 0.597843, acc.: 62.50%] [G loss: 0.857024]\n",
            "3294 [D loss: 0.598351, acc.: 62.50%] [G loss: 0.850389]\n",
            "3295 [D loss: 0.601354, acc.: 62.50%] [G loss: 0.863186]\n",
            "3296 [D loss: 0.599317, acc.: 62.50%] [G loss: 0.862564]\n",
            "3297 [D loss: 0.591601, acc.: 62.50%] [G loss: 0.888203]\n",
            "3298 [D loss: 0.595203, acc.: 62.50%] [G loss: 0.957428]\n",
            "3299 [D loss: 0.599392, acc.: 62.50%] [G loss: 0.940118]\n",
            "3300 [D loss: 0.620928, acc.: 62.50%] [G loss: 0.865723]\n",
            "generated_data\n",
            "3301 [D loss: 0.600043, acc.: 62.50%] [G loss: 0.857335]\n",
            "3302 [D loss: 0.600629, acc.: 62.50%] [G loss: 0.855531]\n",
            "3303 [D loss: 0.600434, acc.: 62.50%] [G loss: 0.845607]\n",
            "3304 [D loss: 0.604546, acc.: 62.50%] [G loss: 0.855591]\n",
            "3305 [D loss: 0.598505, acc.: 62.50%] [G loss: 0.853108]\n",
            "3306 [D loss: 0.600611, acc.: 62.50%] [G loss: 0.854427]\n",
            "3307 [D loss: 0.599624, acc.: 62.50%] [G loss: 0.849966]\n",
            "3308 [D loss: 0.596790, acc.: 62.50%] [G loss: 0.853768]\n",
            "3309 [D loss: 0.596385, acc.: 62.50%] [G loss: 0.855954]\n",
            "3310 [D loss: 0.598390, acc.: 62.50%] [G loss: 0.859036]\n",
            "3311 [D loss: 0.598814, acc.: 62.50%] [G loss: 0.848348]\n",
            "3312 [D loss: 0.598604, acc.: 62.50%] [G loss: 0.854679]\n",
            "3313 [D loss: 0.598122, acc.: 62.50%] [G loss: 0.855520]\n",
            "3314 [D loss: 0.599768, acc.: 62.50%] [G loss: 0.849076]\n",
            "3315 [D loss: 0.595607, acc.: 62.50%] [G loss: 0.852988]\n",
            "3316 [D loss: 0.596033, acc.: 62.50%] [G loss: 0.854550]\n",
            "3317 [D loss: 0.595751, acc.: 62.50%] [G loss: 1.142402]\n",
            "3318 [D loss: 0.694371, acc.: 62.50%] [G loss: 0.861813]\n",
            "3319 [D loss: 0.599865, acc.: 62.50%] [G loss: 0.853086]\n",
            "3320 [D loss: 0.607129, acc.: 62.50%] [G loss: 0.857177]\n",
            "3321 [D loss: 0.600501, acc.: 62.50%] [G loss: 0.852912]\n",
            "3322 [D loss: 0.595020, acc.: 62.50%] [G loss: 0.866070]\n",
            "3323 [D loss: 0.599124, acc.: 62.50%] [G loss: 0.870681]\n",
            "3324 [D loss: 0.600900, acc.: 62.50%] [G loss: 0.859857]\n",
            "3325 [D loss: 0.606339, acc.: 62.50%] [G loss: 0.852216]\n",
            "3326 [D loss: 0.601012, acc.: 62.50%] [G loss: 0.871952]\n",
            "3327 [D loss: 0.596801, acc.: 62.50%] [G loss: 0.873283]\n",
            "3328 [D loss: 0.607705, acc.: 62.50%] [G loss: 0.854239]\n",
            "3329 [D loss: 0.601360, acc.: 62.50%] [G loss: 0.847512]\n",
            "3330 [D loss: 0.598434, acc.: 62.50%] [G loss: 0.849144]\n",
            "3331 [D loss: 0.598249, acc.: 62.50%] [G loss: 0.849084]\n",
            "3332 [D loss: 0.596757, acc.: 62.50%] [G loss: 0.850501]\n",
            "3333 [D loss: 0.597735, acc.: 62.50%] [G loss: 0.846138]\n",
            "3334 [D loss: 0.595210, acc.: 62.50%] [G loss: 0.854341]\n",
            "3335 [D loss: 0.596265, acc.: 62.50%] [G loss: 0.859098]\n",
            "3336 [D loss: 0.596485, acc.: 62.50%] [G loss: 0.861714]\n",
            "3337 [D loss: 0.600297, acc.: 62.50%] [G loss: 0.862298]\n",
            "3338 [D loss: 0.595974, acc.: 62.50%] [G loss: 0.860215]\n",
            "3339 [D loss: 0.595417, acc.: 62.50%] [G loss: 0.854724]\n",
            "3340 [D loss: 0.595067, acc.: 62.50%] [G loss: 0.871763]\n",
            "3341 [D loss: 0.595036, acc.: 64.06%] [G loss: 0.861519]\n",
            "3342 [D loss: 0.600592, acc.: 62.50%] [G loss: 0.846136]\n",
            "3343 [D loss: 0.598111, acc.: 62.50%] [G loss: 0.849962]\n",
            "3344 [D loss: 0.598875, acc.: 62.50%] [G loss: 0.841642]\n",
            "3345 [D loss: 0.599558, acc.: 62.50%] [G loss: 0.842294]\n",
            "3346 [D loss: 0.598745, acc.: 62.50%] [G loss: 0.844003]\n",
            "3347 [D loss: 0.598517, acc.: 62.50%] [G loss: 0.837951]\n",
            "3348 [D loss: 0.597360, acc.: 62.50%] [G loss: 0.833708]\n",
            "3349 [D loss: 0.600414, acc.: 62.50%] [G loss: 0.838941]\n",
            "3350 [D loss: 0.600222, acc.: 62.50%] [G loss: 0.840190]\n",
            "3351 [D loss: 0.599186, acc.: 62.50%] [G loss: 0.837940]\n",
            "3352 [D loss: 0.597880, acc.: 62.50%] [G loss: 0.840663]\n",
            "3353 [D loss: 0.598624, acc.: 62.50%] [G loss: 0.848978]\n",
            "3354 [D loss: 0.600418, acc.: 62.50%] [G loss: 0.848259]\n",
            "3355 [D loss: 0.596619, acc.: 62.50%] [G loss: 0.849737]\n",
            "3356 [D loss: 0.598568, acc.: 62.50%] [G loss: 0.846389]\n",
            "3357 [D loss: 0.599175, acc.: 62.50%] [G loss: 0.853163]\n",
            "3358 [D loss: 0.597973, acc.: 62.50%] [G loss: 0.852552]\n",
            "3359 [D loss: 0.597795, acc.: 62.50%] [G loss: 0.857575]\n",
            "3360 [D loss: 0.598062, acc.: 62.50%] [G loss: 0.866674]\n",
            "3361 [D loss: 0.596666, acc.: 62.50%] [G loss: 0.865654]\n",
            "3362 [D loss: 0.601596, acc.: 62.50%] [G loss: 0.863875]\n",
            "3363 [D loss: 0.599815, acc.: 62.50%] [G loss: 0.858122]\n",
            "3364 [D loss: 0.599060, acc.: 62.50%] [G loss: 0.862351]\n",
            "3365 [D loss: 0.599529, acc.: 62.50%] [G loss: 0.870098]\n",
            "3366 [D loss: 0.599084, acc.: 62.50%] [G loss: 0.867986]\n",
            "3367 [D loss: 0.600252, acc.: 62.50%] [G loss: 0.864200]\n",
            "3368 [D loss: 0.598987, acc.: 62.50%] [G loss: 0.866231]\n",
            "3369 [D loss: 0.599067, acc.: 62.50%] [G loss: 0.865850]\n",
            "3370 [D loss: 0.600148, acc.: 62.50%] [G loss: 0.860347]\n",
            "3371 [D loss: 0.598448, acc.: 62.50%] [G loss: 0.862786]\n",
            "3372 [D loss: 0.597947, acc.: 62.50%] [G loss: 0.851437]\n",
            "3373 [D loss: 0.596816, acc.: 62.50%] [G loss: 0.857808]\n",
            "3374 [D loss: 0.600860, acc.: 62.50%] [G loss: 0.860512]\n",
            "3375 [D loss: 0.596397, acc.: 62.50%] [G loss: 0.863988]\n",
            "3376 [D loss: 0.602420, acc.: 60.94%] [G loss: 0.856394]\n",
            "3377 [D loss: 0.600439, acc.: 62.50%] [G loss: 0.858115]\n",
            "3378 [D loss: 0.601197, acc.: 62.50%] [G loss: 0.856894]\n",
            "3379 [D loss: 0.599015, acc.: 62.50%] [G loss: 0.854401]\n",
            "3380 [D loss: 0.596137, acc.: 62.50%] [G loss: 0.853219]\n",
            "3381 [D loss: 0.599831, acc.: 62.50%] [G loss: 0.853578]\n",
            "3382 [D loss: 0.596877, acc.: 62.50%] [G loss: 0.861205]\n",
            "3383 [D loss: 0.599358, acc.: 62.50%] [G loss: 0.859632]\n",
            "3384 [D loss: 0.597334, acc.: 62.50%] [G loss: 0.854610]\n",
            "3385 [D loss: 0.598320, acc.: 62.50%] [G loss: 0.859117]\n",
            "3386 [D loss: 0.604877, acc.: 60.94%] [G loss: 0.863947]\n",
            "3387 [D loss: 0.595883, acc.: 62.50%] [G loss: 0.850071]\n",
            "3388 [D loss: 0.599786, acc.: 62.50%] [G loss: 0.864529]\n",
            "3389 [D loss: 0.596868, acc.: 62.50%] [G loss: 0.868903]\n",
            "3390 [D loss: 0.601615, acc.: 62.50%] [G loss: 0.862364]\n",
            "3391 [D loss: 0.597472, acc.: 62.50%] [G loss: 0.858395]\n",
            "3392 [D loss: 0.598475, acc.: 62.50%] [G loss: 0.861244]\n",
            "3393 [D loss: 0.595963, acc.: 62.50%] [G loss: 0.866395]\n",
            "3394 [D loss: 0.600536, acc.: 62.50%] [G loss: 0.859691]\n",
            "3395 [D loss: 0.597623, acc.: 62.50%] [G loss: 0.861787]\n",
            "3396 [D loss: 0.600165, acc.: 62.50%] [G loss: 0.859845]\n",
            "3397 [D loss: 0.598284, acc.: 62.50%] [G loss: 0.870930]\n",
            "3398 [D loss: 0.604512, acc.: 62.50%] [G loss: 0.858349]\n",
            "3399 [D loss: 0.599633, acc.: 62.50%] [G loss: 0.859305]\n",
            "3400 [D loss: 0.597500, acc.: 62.50%] [G loss: 0.865516]\n",
            "generated_data\n",
            "3401 [D loss: 0.596621, acc.: 62.50%] [G loss: 0.870217]\n",
            "3402 [D loss: 0.595432, acc.: 62.50%] [G loss: 0.864576]\n",
            "3403 [D loss: 0.602045, acc.: 62.50%] [G loss: 0.872477]\n",
            "3404 [D loss: 0.600630, acc.: 62.50%] [G loss: 0.856850]\n",
            "3405 [D loss: 0.596855, acc.: 62.50%] [G loss: 0.855256]\n",
            "3406 [D loss: 0.603364, acc.: 62.50%] [G loss: 0.858467]\n",
            "3407 [D loss: 0.598627, acc.: 62.50%] [G loss: 0.857334]\n",
            "3408 [D loss: 0.596896, acc.: 62.50%] [G loss: 0.860015]\n",
            "3409 [D loss: 0.597625, acc.: 62.50%] [G loss: 0.861137]\n",
            "3410 [D loss: 0.599863, acc.: 62.50%] [G loss: 0.870444]\n",
            "3411 [D loss: 0.601824, acc.: 62.50%] [G loss: 0.860767]\n",
            "3412 [D loss: 0.595430, acc.: 62.50%] [G loss: 0.858670]\n",
            "3413 [D loss: 0.601675, acc.: 60.94%] [G loss: 0.863128]\n",
            "3414 [D loss: 0.598788, acc.: 62.50%] [G loss: 0.864543]\n",
            "3415 [D loss: 0.596645, acc.: 62.50%] [G loss: 0.865042]\n",
            "3416 [D loss: 0.600716, acc.: 62.50%] [G loss: 0.867381]\n",
            "3417 [D loss: 0.599432, acc.: 62.50%] [G loss: 0.867928]\n",
            "3418 [D loss: 0.597709, acc.: 62.50%] [G loss: 0.865286]\n",
            "3419 [D loss: 0.597524, acc.: 62.50%] [G loss: 0.861995]\n",
            "3420 [D loss: 0.597769, acc.: 62.50%] [G loss: 0.872005]\n",
            "3421 [D loss: 0.592001, acc.: 62.50%] [G loss: 0.863345]\n",
            "3422 [D loss: 0.635351, acc.: 59.38%] [G loss: 0.870945]\n",
            "3423 [D loss: 0.597939, acc.: 62.50%] [G loss: 0.875362]\n",
            "3424 [D loss: 0.597240, acc.: 62.50%] [G loss: 0.871825]\n",
            "3425 [D loss: 0.600249, acc.: 62.50%] [G loss: 0.880987]\n",
            "3426 [D loss: 0.608411, acc.: 62.50%] [G loss: 0.854582]\n",
            "3427 [D loss: 0.600694, acc.: 62.50%] [G loss: 0.848809]\n",
            "3428 [D loss: 0.604717, acc.: 62.50%] [G loss: 0.840321]\n",
            "3429 [D loss: 0.598737, acc.: 62.50%] [G loss: 0.857621]\n",
            "3430 [D loss: 0.597760, acc.: 62.50%] [G loss: 0.908576]\n",
            "3431 [D loss: 0.600145, acc.: 62.50%] [G loss: 0.895733]\n",
            "3432 [D loss: 0.597790, acc.: 62.50%] [G loss: 0.879528]\n",
            "3433 [D loss: 0.601377, acc.: 62.50%] [G loss: 0.867552]\n",
            "3434 [D loss: 0.598375, acc.: 62.50%] [G loss: 0.868899]\n",
            "3435 [D loss: 0.596392, acc.: 62.50%] [G loss: 0.861960]\n",
            "3436 [D loss: 0.596518, acc.: 62.50%] [G loss: 0.863949]\n",
            "3437 [D loss: 0.593456, acc.: 62.50%] [G loss: 0.861810]\n",
            "3438 [D loss: 0.597379, acc.: 62.50%] [G loss: 0.860891]\n",
            "3439 [D loss: 0.600658, acc.: 62.50%] [G loss: 0.862839]\n",
            "3440 [D loss: 0.595836, acc.: 62.50%] [G loss: 0.860931]\n",
            "3441 [D loss: 0.596917, acc.: 62.50%] [G loss: 0.857585]\n",
            "3442 [D loss: 0.595616, acc.: 62.50%] [G loss: 0.857080]\n",
            "3443 [D loss: 0.596571, acc.: 62.50%] [G loss: 0.861158]\n",
            "3444 [D loss: 0.597896, acc.: 62.50%] [G loss: 0.860883]\n",
            "3445 [D loss: 0.598984, acc.: 62.50%] [G loss: 0.859443]\n",
            "3446 [D loss: 0.597616, acc.: 62.50%] [G loss: 0.860988]\n",
            "3447 [D loss: 0.597462, acc.: 62.50%] [G loss: 0.863549]\n",
            "3448 [D loss: 0.596891, acc.: 62.50%] [G loss: 0.860299]\n",
            "3449 [D loss: 0.600402, acc.: 62.50%] [G loss: 0.862542]\n",
            "3450 [D loss: 0.598423, acc.: 62.50%] [G loss: 0.857874]\n",
            "3451 [D loss: 0.595478, acc.: 62.50%] [G loss: 0.862762]\n",
            "3452 [D loss: 0.595831, acc.: 62.50%] [G loss: 0.866241]\n",
            "3453 [D loss: 0.595935, acc.: 62.50%] [G loss: 0.861788]\n",
            "3454 [D loss: 0.595366, acc.: 62.50%] [G loss: 0.860564]\n",
            "3455 [D loss: 0.597073, acc.: 62.50%] [G loss: 0.868921]\n",
            "3456 [D loss: 0.594732, acc.: 62.50%] [G loss: 0.865103]\n",
            "3457 [D loss: 0.594765, acc.: 62.50%] [G loss: 0.866097]\n",
            "3458 [D loss: 0.596653, acc.: 62.50%] [G loss: 0.865016]\n",
            "3459 [D loss: 0.596210, acc.: 62.50%] [G loss: 0.863432]\n",
            "3460 [D loss: 0.596428, acc.: 62.50%] [G loss: 0.864877]\n",
            "3461 [D loss: 0.596015, acc.: 62.50%] [G loss: 0.858002]\n",
            "3462 [D loss: 0.595710, acc.: 62.50%] [G loss: 0.859539]\n",
            "3463 [D loss: 0.593506, acc.: 62.50%] [G loss: 0.858843]\n",
            "3464 [D loss: 0.595030, acc.: 62.50%] [G loss: 0.859263]\n",
            "3465 [D loss: 0.595537, acc.: 62.50%] [G loss: 0.864960]\n",
            "3466 [D loss: 0.591691, acc.: 62.50%] [G loss: 0.882814]\n",
            "3467 [D loss: 0.593867, acc.: 60.94%] [G loss: 0.877095]\n",
            "3468 [D loss: 0.597490, acc.: 64.06%] [G loss: 0.857750]\n",
            "3469 [D loss: 0.601725, acc.: 60.94%] [G loss: 0.855601]\n",
            "3470 [D loss: 0.590871, acc.: 62.50%] [G loss: 0.854955]\n",
            "3471 [D loss: 0.588903, acc.: 65.62%] [G loss: 0.864730]\n",
            "3472 [D loss: 0.595228, acc.: 62.50%] [G loss: 0.852370]\n",
            "3473 [D loss: 0.594779, acc.: 64.06%] [G loss: 0.860772]\n",
            "3474 [D loss: 0.593206, acc.: 64.06%] [G loss: 0.860771]\n",
            "3475 [D loss: 0.605005, acc.: 56.25%] [G loss: 0.848695]\n",
            "3476 [D loss: 0.592806, acc.: 60.94%] [G loss: 0.833244]\n",
            "3477 [D loss: 0.610074, acc.: 56.25%] [G loss: 0.838475]\n",
            "3478 [D loss: 0.597552, acc.: 62.50%] [G loss: 0.855541]\n",
            "3479 [D loss: 0.593901, acc.: 62.50%] [G loss: 0.855807]\n",
            "3480 [D loss: 0.595570, acc.: 62.50%] [G loss: 0.855791]\n",
            "3481 [D loss: 0.598833, acc.: 62.50%] [G loss: 0.846415]\n",
            "3482 [D loss: 0.592686, acc.: 62.50%] [G loss: 0.848406]\n",
            "3483 [D loss: 0.594771, acc.: 62.50%] [G loss: 0.850046]\n",
            "3484 [D loss: 0.591602, acc.: 62.50%] [G loss: 0.854222]\n",
            "3485 [D loss: 0.599037, acc.: 62.50%] [G loss: 0.854193]\n",
            "3486 [D loss: 0.602675, acc.: 62.50%] [G loss: 0.837349]\n",
            "3487 [D loss: 0.598892, acc.: 62.50%] [G loss: 0.844189]\n",
            "3488 [D loss: 0.602594, acc.: 62.50%] [G loss: 0.836334]\n",
            "3489 [D loss: 0.599842, acc.: 62.50%] [G loss: 0.853022]\n",
            "3490 [D loss: 0.601487, acc.: 62.50%] [G loss: 0.848342]\n",
            "3491 [D loss: 0.597820, acc.: 62.50%] [G loss: 0.844860]\n",
            "3492 [D loss: 0.594299, acc.: 62.50%] [G loss: 0.847994]\n",
            "3493 [D loss: 0.596638, acc.: 62.50%] [G loss: 0.840807]\n",
            "3494 [D loss: 0.597400, acc.: 62.50%] [G loss: 0.854116]\n",
            "3495 [D loss: 0.591660, acc.: 62.50%] [G loss: 0.859685]\n",
            "3496 [D loss: 0.598635, acc.: 62.50%] [G loss: 0.849408]\n",
            "3497 [D loss: 0.598538, acc.: 62.50%] [G loss: 0.848350]\n",
            "3498 [D loss: 0.599061, acc.: 62.50%] [G loss: 0.848316]\n",
            "3499 [D loss: 0.596788, acc.: 62.50%] [G loss: 0.850763]\n",
            "3500 [D loss: 0.595704, acc.: 62.50%] [G loss: 0.848998]\n",
            "generated_data\n",
            "3501 [D loss: 0.596450, acc.: 62.50%] [G loss: 0.844683]\n",
            "3502 [D loss: 0.598038, acc.: 62.50%] [G loss: 0.848356]\n",
            "3503 [D loss: 0.599799, acc.: 62.50%] [G loss: 0.857944]\n",
            "3504 [D loss: 0.595417, acc.: 62.50%] [G loss: 0.898338]\n",
            "3505 [D loss: 0.596059, acc.: 62.50%] [G loss: 0.886158]\n",
            "3506 [D loss: 0.603425, acc.: 62.50%] [G loss: 0.851376]\n",
            "3507 [D loss: 0.593828, acc.: 62.50%] [G loss: 0.861260]\n",
            "3508 [D loss: 0.593059, acc.: 62.50%] [G loss: 0.854728]\n",
            "3509 [D loss: 0.598072, acc.: 62.50%] [G loss: 0.854677]\n",
            "3510 [D loss: 0.602469, acc.: 62.50%] [G loss: 0.860660]\n",
            "3511 [D loss: 0.606704, acc.: 62.50%] [G loss: 0.849495]\n",
            "3512 [D loss: 0.601470, acc.: 62.50%] [G loss: 0.852789]\n",
            "3513 [D loss: 0.600971, acc.: 62.50%] [G loss: 0.851395]\n",
            "3514 [D loss: 0.599858, acc.: 62.50%] [G loss: 0.858085]\n",
            "3515 [D loss: 0.599433, acc.: 62.50%] [G loss: 0.862637]\n",
            "3516 [D loss: 0.598790, acc.: 62.50%] [G loss: 0.863049]\n",
            "3517 [D loss: 0.597833, acc.: 62.50%] [G loss: 0.854812]\n",
            "3518 [D loss: 0.597212, acc.: 62.50%] [G loss: 0.859714]\n",
            "3519 [D loss: 0.598259, acc.: 62.50%] [G loss: 0.860296]\n",
            "3520 [D loss: 0.597098, acc.: 62.50%] [G loss: 0.858824]\n",
            "3521 [D loss: 0.599688, acc.: 62.50%] [G loss: 0.858375]\n",
            "3522 [D loss: 0.598930, acc.: 62.50%] [G loss: 0.855775]\n",
            "3523 [D loss: 0.597525, acc.: 62.50%] [G loss: 0.859090]\n",
            "3524 [D loss: 0.596658, acc.: 62.50%] [G loss: 0.859582]\n",
            "3525 [D loss: 0.597879, acc.: 62.50%] [G loss: 0.862142]\n",
            "3526 [D loss: 0.597579, acc.: 62.50%] [G loss: 0.856591]\n",
            "3527 [D loss: 0.597315, acc.: 62.50%] [G loss: 0.862824]\n",
            "3528 [D loss: 0.599590, acc.: 62.50%] [G loss: 0.844517]\n",
            "3529 [D loss: 0.595259, acc.: 62.50%] [G loss: 0.843183]\n",
            "3530 [D loss: 0.595973, acc.: 62.50%] [G loss: 0.847495]\n",
            "3531 [D loss: 0.595008, acc.: 62.50%] [G loss: 0.852878]\n",
            "3532 [D loss: 0.597995, acc.: 62.50%] [G loss: 0.867425]\n",
            "3533 [D loss: 0.596769, acc.: 62.50%] [G loss: 0.847623]\n",
            "3534 [D loss: 0.599581, acc.: 62.50%] [G loss: 0.858610]\n",
            "3535 [D loss: 0.597930, acc.: 62.50%] [G loss: 0.866990]\n",
            "3536 [D loss: 0.597311, acc.: 62.50%] [G loss: 0.858551]\n",
            "3537 [D loss: 0.598280, acc.: 62.50%] [G loss: 0.863910]\n",
            "3538 [D loss: 0.600386, acc.: 62.50%] [G loss: 0.856913]\n",
            "3539 [D loss: 0.596782, acc.: 62.50%] [G loss: 0.852165]\n",
            "3540 [D loss: 0.598524, acc.: 62.50%] [G loss: 0.859807]\n",
            "3541 [D loss: 0.598732, acc.: 62.50%] [G loss: 0.856735]\n",
            "3542 [D loss: 0.598771, acc.: 62.50%] [G loss: 0.849826]\n",
            "3543 [D loss: 0.599804, acc.: 62.50%] [G loss: 0.864014]\n",
            "3544 [D loss: 0.595410, acc.: 62.50%] [G loss: 0.857769]\n",
            "3545 [D loss: 0.599477, acc.: 62.50%] [G loss: 0.859351]\n",
            "3546 [D loss: 0.599568, acc.: 62.50%] [G loss: 0.857152]\n",
            "3547 [D loss: 0.599589, acc.: 62.50%] [G loss: 0.854554]\n",
            "3548 [D loss: 0.594634, acc.: 62.50%] [G loss: 0.861777]\n",
            "3549 [D loss: 0.597453, acc.: 62.50%] [G loss: 0.850137]\n",
            "3550 [D loss: 0.595267, acc.: 62.50%] [G loss: 0.856264]\n",
            "3551 [D loss: 0.597252, acc.: 62.50%] [G loss: 0.853406]\n",
            "3552 [D loss: 0.597504, acc.: 62.50%] [G loss: 0.847185]\n",
            "3553 [D loss: 0.597758, acc.: 62.50%] [G loss: 0.851249]\n",
            "3554 [D loss: 0.596306, acc.: 62.50%] [G loss: 0.854578]\n",
            "3555 [D loss: 0.604991, acc.: 60.94%] [G loss: 0.839562]\n",
            "3556 [D loss: 0.596506, acc.: 62.50%] [G loss: 0.852427]\n",
            "3557 [D loss: 0.602503, acc.: 62.50%] [G loss: 0.845842]\n",
            "3558 [D loss: 0.602565, acc.: 62.50%] [G loss: 0.851759]\n",
            "3559 [D loss: 0.600891, acc.: 62.50%] [G loss: 0.860854]\n",
            "3560 [D loss: 0.595407, acc.: 62.50%] [G loss: 0.867476]\n",
            "3561 [D loss: 0.595191, acc.: 62.50%] [G loss: 0.860028]\n",
            "3562 [D loss: 0.597466, acc.: 62.50%] [G loss: 0.872038]\n",
            "3563 [D loss: 0.594880, acc.: 62.50%] [G loss: 0.868702]\n",
            "3564 [D loss: 0.595735, acc.: 62.50%] [G loss: 0.868651]\n",
            "3565 [D loss: 0.596622, acc.: 62.50%] [G loss: 0.871210]\n",
            "3566 [D loss: 0.594943, acc.: 62.50%] [G loss: 0.869457]\n",
            "3567 [D loss: 0.596577, acc.: 62.50%] [G loss: 0.871875]\n",
            "3568 [D loss: 0.594531, acc.: 62.50%] [G loss: 0.877379]\n",
            "3569 [D loss: 0.595627, acc.: 62.50%] [G loss: 0.885294]\n",
            "3570 [D loss: 0.604092, acc.: 62.50%] [G loss: 0.875673]\n",
            "3571 [D loss: 0.599527, acc.: 62.50%] [G loss: 0.903522]\n",
            "3572 [D loss: 0.610975, acc.: 62.50%] [G loss: 0.875592]\n",
            "3573 [D loss: 0.600215, acc.: 62.50%] [G loss: 0.873600]\n",
            "3574 [D loss: 0.598140, acc.: 62.50%] [G loss: 0.868751]\n",
            "3575 [D loss: 0.595326, acc.: 62.50%] [G loss: 0.869662]\n",
            "3576 [D loss: 0.597171, acc.: 62.50%] [G loss: 0.870372]\n",
            "3577 [D loss: 0.596823, acc.: 62.50%] [G loss: 0.870142]\n",
            "3578 [D loss: 0.601886, acc.: 62.50%] [G loss: 0.869542]\n",
            "3579 [D loss: 0.600103, acc.: 62.50%] [G loss: 0.865712]\n",
            "3580 [D loss: 0.595436, acc.: 62.50%] [G loss: 0.886699]\n",
            "3581 [D loss: 0.598495, acc.: 62.50%] [G loss: 0.869707]\n",
            "3582 [D loss: 0.599486, acc.: 62.50%] [G loss: 0.872489]\n",
            "3583 [D loss: 0.596845, acc.: 62.50%] [G loss: 0.868092]\n",
            "3584 [D loss: 0.600623, acc.: 62.50%] [G loss: 0.867653]\n",
            "3585 [D loss: 0.597188, acc.: 62.50%] [G loss: 0.870155]\n",
            "3586 [D loss: 0.597024, acc.: 62.50%] [G loss: 0.862688]\n",
            "3587 [D loss: 0.597851, acc.: 62.50%] [G loss: 0.868791]\n",
            "3588 [D loss: 0.597576, acc.: 62.50%] [G loss: 0.863886]\n",
            "3589 [D loss: 0.597260, acc.: 62.50%] [G loss: 0.868681]\n",
            "3590 [D loss: 0.597285, acc.: 62.50%] [G loss: 0.867371]\n",
            "3591 [D loss: 0.596774, acc.: 62.50%] [G loss: 0.874297]\n",
            "3592 [D loss: 0.597820, acc.: 62.50%] [G loss: 0.866789]\n",
            "3593 [D loss: 0.596494, acc.: 62.50%] [G loss: 0.870404]\n",
            "3594 [D loss: 0.597169, acc.: 62.50%] [G loss: 0.879432]\n",
            "3595 [D loss: 0.599657, acc.: 62.50%] [G loss: 0.876456]\n",
            "3596 [D loss: 0.600050, acc.: 62.50%] [G loss: 0.876564]\n",
            "3597 [D loss: 0.602302, acc.: 62.50%] [G loss: 0.876423]\n",
            "3598 [D loss: 0.598394, acc.: 62.50%] [G loss: 0.874184]\n",
            "3599 [D loss: 0.598637, acc.: 62.50%] [G loss: 0.883055]\n",
            "3600 [D loss: 0.600795, acc.: 62.50%] [G loss: 0.870844]\n",
            "generated_data\n",
            "3601 [D loss: 0.598936, acc.: 62.50%] [G loss: 0.870691]\n",
            "3602 [D loss: 0.598297, acc.: 62.50%] [G loss: 0.862921]\n",
            "3603 [D loss: 0.598674, acc.: 62.50%] [G loss: 0.861265]\n",
            "3604 [D loss: 0.597000, acc.: 62.50%] [G loss: 0.867701]\n",
            "3605 [D loss: 0.599372, acc.: 62.50%] [G loss: 0.861509]\n",
            "3606 [D loss: 0.599214, acc.: 62.50%] [G loss: 0.858505]\n",
            "3607 [D loss: 0.597525, acc.: 62.50%] [G loss: 0.862437]\n",
            "3608 [D loss: 0.596163, acc.: 62.50%] [G loss: 0.864869]\n",
            "3609 [D loss: 0.599983, acc.: 62.50%] [G loss: 0.862138]\n",
            "3610 [D loss: 0.599154, acc.: 62.50%] [G loss: 0.865992]\n",
            "3611 [D loss: 0.596225, acc.: 62.50%] [G loss: 0.865975]\n",
            "3612 [D loss: 0.599987, acc.: 62.50%] [G loss: 0.862651]\n",
            "3613 [D loss: 0.597392, acc.: 62.50%] [G loss: 0.862258]\n",
            "3614 [D loss: 0.600043, acc.: 62.50%] [G loss: 0.860310]\n",
            "3615 [D loss: 0.596981, acc.: 62.50%] [G loss: 0.864358]\n",
            "3616 [D loss: 0.598058, acc.: 62.50%] [G loss: 0.861022]\n",
            "3617 [D loss: 0.597734, acc.: 62.50%] [G loss: 0.874218]\n",
            "3618 [D loss: 0.599814, acc.: 62.50%] [G loss: 0.861761]\n",
            "3619 [D loss: 0.597734, acc.: 62.50%] [G loss: 0.863891]\n",
            "3620 [D loss: 0.598955, acc.: 62.50%] [G loss: 0.865923]\n",
            "3621 [D loss: 0.596909, acc.: 62.50%] [G loss: 0.861666]\n",
            "3622 [D loss: 0.599086, acc.: 62.50%] [G loss: 0.863978]\n",
            "3623 [D loss: 0.595951, acc.: 62.50%] [G loss: 0.864217]\n",
            "3624 [D loss: 0.596857, acc.: 62.50%] [G loss: 0.866490]\n",
            "3625 [D loss: 0.598402, acc.: 62.50%] [G loss: 0.868061]\n",
            "3626 [D loss: 0.597954, acc.: 62.50%] [G loss: 0.866532]\n",
            "3627 [D loss: 0.599401, acc.: 62.50%] [G loss: 0.864893]\n",
            "3628 [D loss: 0.599613, acc.: 62.50%] [G loss: 0.860637]\n",
            "3629 [D loss: 0.633137, acc.: 51.56%] [G loss: 0.866488]\n",
            "3630 [D loss: 0.597928, acc.: 62.50%] [G loss: 0.863397]\n",
            "3631 [D loss: 0.598334, acc.: 62.50%] [G loss: 0.863178]\n",
            "3632 [D loss: 0.596741, acc.: 62.50%] [G loss: 0.864586]\n",
            "3633 [D loss: 0.596295, acc.: 62.50%] [G loss: 0.868026]\n",
            "3634 [D loss: 0.598332, acc.: 62.50%] [G loss: 0.870387]\n",
            "3635 [D loss: 0.595623, acc.: 62.50%] [G loss: 0.872340]\n",
            "3636 [D loss: 0.598399, acc.: 62.50%] [G loss: 0.873732]\n",
            "3637 [D loss: 0.596358, acc.: 62.50%] [G loss: 0.866878]\n",
            "3638 [D loss: 0.613573, acc.: 62.50%] [G loss: 0.882097]\n",
            "3639 [D loss: 0.597060, acc.: 62.50%] [G loss: 0.875689]\n",
            "3640 [D loss: 0.591876, acc.: 62.50%] [G loss: 0.887368]\n",
            "3641 [D loss: 0.596647, acc.: 62.50%] [G loss: 0.882872]\n",
            "3642 [D loss: 0.598453, acc.: 62.50%] [G loss: 0.887597]\n",
            "3643 [D loss: 0.599434, acc.: 62.50%] [G loss: 0.884983]\n",
            "3644 [D loss: 0.600062, acc.: 62.50%] [G loss: 0.878421]\n",
            "3645 [D loss: 0.603095, acc.: 62.50%] [G loss: 0.877587]\n",
            "3646 [D loss: 0.596840, acc.: 62.50%] [G loss: 0.877133]\n",
            "3647 [D loss: 0.599943, acc.: 62.50%] [G loss: 0.875638]\n",
            "3648 [D loss: 0.601129, acc.: 62.50%] [G loss: 0.878266]\n",
            "3649 [D loss: 0.600208, acc.: 62.50%] [G loss: 0.870512]\n",
            "3650 [D loss: 0.599075, acc.: 62.50%] [G loss: 0.880009]\n",
            "3651 [D loss: 0.599952, acc.: 62.50%] [G loss: 0.876456]\n",
            "3652 [D loss: 0.600534, acc.: 62.50%] [G loss: 0.871295]\n",
            "3653 [D loss: 0.601128, acc.: 62.50%] [G loss: 0.870518]\n",
            "3654 [D loss: 0.601157, acc.: 62.50%] [G loss: 0.868198]\n",
            "3655 [D loss: 0.600379, acc.: 62.50%] [G loss: 0.859169]\n",
            "3656 [D loss: 0.600972, acc.: 62.50%] [G loss: 0.860478]\n",
            "3657 [D loss: 0.599099, acc.: 62.50%] [G loss: 0.861509]\n",
            "3658 [D loss: 0.599330, acc.: 62.50%] [G loss: 0.861620]\n",
            "3659 [D loss: 0.598815, acc.: 62.50%] [G loss: 0.860804]\n",
            "3660 [D loss: 0.600262, acc.: 62.50%] [G loss: 0.855521]\n",
            "3661 [D loss: 0.599334, acc.: 62.50%] [G loss: 0.858324]\n",
            "3662 [D loss: 0.601261, acc.: 62.50%] [G loss: 0.851153]\n",
            "3663 [D loss: 0.599940, acc.: 62.50%] [G loss: 0.854296]\n",
            "3664 [D loss: 0.599712, acc.: 62.50%] [G loss: 0.853363]\n",
            "3665 [D loss: 0.600227, acc.: 62.50%] [G loss: 0.854528]\n",
            "3666 [D loss: 0.599663, acc.: 62.50%] [G loss: 0.852948]\n",
            "3667 [D loss: 0.599127, acc.: 62.50%] [G loss: 0.853612]\n",
            "3668 [D loss: 0.599569, acc.: 62.50%] [G loss: 0.848388]\n",
            "3669 [D loss: 0.599694, acc.: 62.50%] [G loss: 0.847947]\n",
            "3670 [D loss: 0.599942, acc.: 62.50%] [G loss: 0.850199]\n",
            "3671 [D loss: 0.599797, acc.: 62.50%] [G loss: 0.851520]\n",
            "3672 [D loss: 0.600010, acc.: 62.50%] [G loss: 0.849870]\n",
            "3673 [D loss: 0.599443, acc.: 62.50%] [G loss: 0.846906]\n",
            "3674 [D loss: 0.600122, acc.: 62.50%] [G loss: 0.850747]\n",
            "3675 [D loss: 0.599625, acc.: 62.50%] [G loss: 0.850165]\n",
            "3676 [D loss: 0.598883, acc.: 62.50%] [G loss: 0.850300]\n",
            "3677 [D loss: 0.598967, acc.: 62.50%] [G loss: 0.850833]\n",
            "3678 [D loss: 0.598502, acc.: 62.50%] [G loss: 0.849816]\n",
            "3679 [D loss: 0.597930, acc.: 62.50%] [G loss: 0.844323]\n",
            "3680 [D loss: 0.598763, acc.: 62.50%] [G loss: 0.843340]\n",
            "3681 [D loss: 0.598206, acc.: 62.50%] [G loss: 0.845685]\n",
            "3682 [D loss: 0.599856, acc.: 62.50%] [G loss: 0.843697]\n",
            "3683 [D loss: 0.600226, acc.: 62.50%] [G loss: 0.843534]\n",
            "3684 [D loss: 0.599693, acc.: 62.50%] [G loss: 0.845591]\n",
            "3685 [D loss: 0.598244, acc.: 62.50%] [G loss: 0.844312]\n",
            "3686 [D loss: 0.598132, acc.: 62.50%] [G loss: 0.842372]\n",
            "3687 [D loss: 0.598725, acc.: 62.50%] [G loss: 0.847931]\n",
            "3688 [D loss: 0.598724, acc.: 62.50%] [G loss: 0.850000]\n",
            "3689 [D loss: 0.599431, acc.: 62.50%] [G loss: 0.850068]\n",
            "3690 [D loss: 0.598300, acc.: 62.50%] [G loss: 0.847577]\n",
            "3691 [D loss: 0.597830, acc.: 62.50%] [G loss: 0.851242]\n",
            "3692 [D loss: 0.600308, acc.: 62.50%] [G loss: 0.847135]\n",
            "3693 [D loss: 0.599128, acc.: 62.50%] [G loss: 0.849099]\n",
            "3694 [D loss: 0.599402, acc.: 62.50%] [G loss: 0.849192]\n",
            "3695 [D loss: 0.598545, acc.: 62.50%] [G loss: 0.853569]\n",
            "3696 [D loss: 0.598606, acc.: 62.50%] [G loss: 0.857563]\n",
            "3697 [D loss: 0.597691, acc.: 62.50%] [G loss: 0.850152]\n",
            "3698 [D loss: 0.598766, acc.: 62.50%] [G loss: 0.857183]\n",
            "3699 [D loss: 0.596489, acc.: 62.50%] [G loss: 0.856818]\n",
            "3700 [D loss: 0.596387, acc.: 62.50%] [G loss: 0.856044]\n",
            "generated_data\n",
            "3701 [D loss: 0.597608, acc.: 62.50%] [G loss: 0.857253]\n",
            "3702 [D loss: 0.595263, acc.: 62.50%] [G loss: 0.861668]\n",
            "3703 [D loss: 0.598175, acc.: 62.50%] [G loss: 0.873371]\n",
            "3704 [D loss: 0.595153, acc.: 62.50%] [G loss: 0.857080]\n",
            "3705 [D loss: 0.597338, acc.: 62.50%] [G loss: 0.857722]\n",
            "3706 [D loss: 0.595182, acc.: 62.50%] [G loss: 0.865778]\n",
            "3707 [D loss: 0.598977, acc.: 62.50%] [G loss: 0.854645]\n",
            "3708 [D loss: 0.596142, acc.: 62.50%] [G loss: 0.859123]\n",
            "3709 [D loss: 0.597687, acc.: 62.50%] [G loss: 0.863758]\n",
            "3710 [D loss: 0.595265, acc.: 62.50%] [G loss: 0.866700]\n",
            "3711 [D loss: 0.598062, acc.: 62.50%] [G loss: 0.871689]\n",
            "3712 [D loss: 0.595002, acc.: 62.50%] [G loss: 0.873207]\n",
            "3713 [D loss: 0.604259, acc.: 62.50%] [G loss: 0.876703]\n",
            "3714 [D loss: 0.600155, acc.: 62.50%] [G loss: 0.863141]\n",
            "3715 [D loss: 0.596762, acc.: 62.50%] [G loss: 0.855542]\n",
            "3716 [D loss: 0.595681, acc.: 62.50%] [G loss: 0.867364]\n",
            "3717 [D loss: 0.596748, acc.: 62.50%] [G loss: 1.104966]\n",
            "3718 [D loss: 0.626320, acc.: 62.50%] [G loss: 0.871557]\n",
            "3719 [D loss: 0.595113, acc.: 62.50%] [G loss: 0.851323]\n",
            "3720 [D loss: 0.598868, acc.: 62.50%] [G loss: 0.859397]\n",
            "3721 [D loss: 0.594936, acc.: 62.50%] [G loss: 0.858637]\n",
            "3722 [D loss: 0.596905, acc.: 62.50%] [G loss: 0.853705]\n",
            "3723 [D loss: 0.594807, acc.: 62.50%] [G loss: 0.854084]\n",
            "3724 [D loss: 0.592237, acc.: 62.50%] [G loss: 0.856707]\n",
            "3725 [D loss: 0.594590, acc.: 62.50%] [G loss: 0.854159]\n",
            "3726 [D loss: 0.592837, acc.: 62.50%] [G loss: 0.856129]\n",
            "3727 [D loss: 0.592103, acc.: 62.50%] [G loss: 0.855790]\n",
            "3728 [D loss: 0.595488, acc.: 62.50%] [G loss: 0.856519]\n",
            "3729 [D loss: 0.603952, acc.: 62.50%] [G loss: 0.868885]\n",
            "3730 [D loss: 0.599489, acc.: 62.50%] [G loss: 0.873428]\n",
            "3731 [D loss: 0.600064, acc.: 62.50%] [G loss: 0.872143]\n",
            "3732 [D loss: 0.598111, acc.: 62.50%] [G loss: 0.879031]\n",
            "3733 [D loss: 0.599150, acc.: 62.50%] [G loss: 0.897125]\n",
            "3734 [D loss: 0.594106, acc.: 62.50%] [G loss: 0.965156]\n",
            "3735 [D loss: 0.612986, acc.: 62.50%] [G loss: 0.870236]\n",
            "3736 [D loss: 0.594756, acc.: 62.50%] [G loss: 0.887191]\n",
            "3737 [D loss: 0.592632, acc.: 62.50%] [G loss: 0.885801]\n",
            "3738 [D loss: 0.596697, acc.: 62.50%] [G loss: 0.873853]\n",
            "3739 [D loss: 0.592865, acc.: 62.50%] [G loss: 0.880685]\n",
            "3740 [D loss: 0.591232, acc.: 62.50%] [G loss: 0.885221]\n",
            "3741 [D loss: 0.600795, acc.: 62.50%] [G loss: 0.887893]\n",
            "3742 [D loss: 0.596609, acc.: 62.50%] [G loss: 0.879368]\n",
            "3743 [D loss: 0.599543, acc.: 62.50%] [G loss: 0.864204]\n",
            "3744 [D loss: 0.600591, acc.: 62.50%] [G loss: 0.855709]\n",
            "3745 [D loss: 0.599965, acc.: 62.50%] [G loss: 0.858969]\n",
            "3746 [D loss: 0.600277, acc.: 62.50%] [G loss: 0.858059]\n",
            "3747 [D loss: 0.598360, acc.: 62.50%] [G loss: 0.855965]\n",
            "3748 [D loss: 0.598990, acc.: 62.50%] [G loss: 0.858768]\n",
            "3749 [D loss: 0.597641, acc.: 62.50%] [G loss: 0.861359]\n",
            "3750 [D loss: 0.598539, acc.: 62.50%] [G loss: 0.867252]\n",
            "3751 [D loss: 0.598685, acc.: 62.50%] [G loss: 0.863558]\n",
            "3752 [D loss: 0.596855, acc.: 62.50%] [G loss: 0.862214]\n",
            "3753 [D loss: 0.598063, acc.: 62.50%] [G loss: 0.864292]\n",
            "3754 [D loss: 0.598065, acc.: 62.50%] [G loss: 0.960396]\n",
            "3755 [D loss: 0.649207, acc.: 62.50%] [G loss: 0.865575]\n",
            "3756 [D loss: 0.599888, acc.: 62.50%] [G loss: 0.861400]\n",
            "3757 [D loss: 0.597183, acc.: 62.50%] [G loss: 0.863808]\n",
            "3758 [D loss: 0.599156, acc.: 62.50%] [G loss: 0.869066]\n",
            "3759 [D loss: 0.598976, acc.: 62.50%] [G loss: 0.862357]\n",
            "3760 [D loss: 0.598274, acc.: 62.50%] [G loss: 0.864280]\n",
            "3761 [D loss: 0.598087, acc.: 62.50%] [G loss: 0.860721]\n",
            "3762 [D loss: 0.596943, acc.: 62.50%] [G loss: 0.863030]\n",
            "3763 [D loss: 0.597824, acc.: 62.50%] [G loss: 0.864541]\n",
            "3764 [D loss: 0.598624, acc.: 62.50%] [G loss: 0.863307]\n",
            "3765 [D loss: 0.597486, acc.: 62.50%] [G loss: 0.864317]\n",
            "3766 [D loss: 0.598344, acc.: 62.50%] [G loss: 0.865370]\n",
            "3767 [D loss: 0.597544, acc.: 62.50%] [G loss: 0.868138]\n",
            "3768 [D loss: 0.597858, acc.: 62.50%] [G loss: 0.867376]\n",
            "3769 [D loss: 0.597059, acc.: 62.50%] [G loss: 0.865304]\n",
            "3770 [D loss: 0.599461, acc.: 62.50%] [G loss: 0.864652]\n",
            "3771 [D loss: 0.598065, acc.: 62.50%] [G loss: 0.864722]\n",
            "3772 [D loss: 0.597193, acc.: 62.50%] [G loss: 0.864496]\n",
            "3773 [D loss: 0.598057, acc.: 62.50%] [G loss: 0.867853]\n",
            "3774 [D loss: 0.597145, acc.: 62.50%] [G loss: 0.865248]\n",
            "3775 [D loss: 0.598406, acc.: 62.50%] [G loss: 0.865822]\n",
            "3776 [D loss: 0.597738, acc.: 62.50%] [G loss: 0.866042]\n",
            "3777 [D loss: 0.597727, acc.: 62.50%] [G loss: 0.863088]\n",
            "3778 [D loss: 0.597015, acc.: 62.50%] [G loss: 0.869297]\n",
            "3779 [D loss: 0.597437, acc.: 62.50%] [G loss: 0.866796]\n",
            "3780 [D loss: 0.597209, acc.: 62.50%] [G loss: 0.866974]\n",
            "3781 [D loss: 0.598125, acc.: 62.50%] [G loss: 0.870298]\n",
            "3782 [D loss: 0.598834, acc.: 62.50%] [G loss: 0.866980]\n",
            "3783 [D loss: 0.595813, acc.: 62.50%] [G loss: 0.873544]\n",
            "3784 [D loss: 0.597919, acc.: 62.50%] [G loss: 0.869972]\n",
            "3785 [D loss: 0.596676, acc.: 62.50%] [G loss: 0.869693]\n",
            "3786 [D loss: 0.596776, acc.: 62.50%] [G loss: 0.873872]\n",
            "3787 [D loss: 0.598553, acc.: 62.50%] [G loss: 0.871920]\n",
            "3788 [D loss: 0.597697, acc.: 62.50%] [G loss: 0.876265]\n",
            "3789 [D loss: 0.598169, acc.: 62.50%] [G loss: 0.872803]\n",
            "3790 [D loss: 0.596908, acc.: 62.50%] [G loss: 0.871913]\n",
            "3791 [D loss: 0.598913, acc.: 62.50%] [G loss: 0.869563]\n",
            "3792 [D loss: 0.595790, acc.: 62.50%] [G loss: 0.873520]\n",
            "3793 [D loss: 0.596803, acc.: 62.50%] [G loss: 0.871751]\n",
            "3794 [D loss: 0.597554, acc.: 62.50%] [G loss: 0.872555]\n",
            "3795 [D loss: 0.597827, acc.: 62.50%] [G loss: 0.873961]\n",
            "3796 [D loss: 0.598020, acc.: 62.50%] [G loss: 0.870196]\n",
            "3797 [D loss: 0.598409, acc.: 62.50%] [G loss: 0.876372]\n",
            "3798 [D loss: 0.595715, acc.: 62.50%] [G loss: 0.873706]\n",
            "3799 [D loss: 0.601793, acc.: 62.50%] [G loss: 0.868930]\n",
            "3800 [D loss: 0.597561, acc.: 62.50%] [G loss: 0.869158]\n",
            "generated_data\n",
            "3801 [D loss: 0.597418, acc.: 62.50%] [G loss: 0.870991]\n",
            "3802 [D loss: 0.598119, acc.: 62.50%] [G loss: 0.869641]\n",
            "3803 [D loss: 0.596376, acc.: 62.50%] [G loss: 0.873220]\n",
            "3804 [D loss: 0.599206, acc.: 62.50%] [G loss: 0.867751]\n",
            "3805 [D loss: 0.596019, acc.: 62.50%] [G loss: 0.877299]\n",
            "3806 [D loss: 0.600975, acc.: 62.50%] [G loss: 0.873429]\n",
            "3807 [D loss: 0.597311, acc.: 62.50%] [G loss: 0.874424]\n",
            "3808 [D loss: 0.597881, acc.: 62.50%] [G loss: 0.872260]\n",
            "3809 [D loss: 0.595717, acc.: 62.50%] [G loss: 0.877212]\n",
            "3810 [D loss: 0.596597, acc.: 62.50%] [G loss: 0.874750]\n",
            "3811 [D loss: 0.598193, acc.: 62.50%] [G loss: 0.872959]\n",
            "3812 [D loss: 0.595811, acc.: 62.50%] [G loss: 0.877662]\n",
            "3813 [D loss: 0.598162, acc.: 62.50%] [G loss: 0.876584]\n",
            "3814 [D loss: 0.599766, acc.: 62.50%] [G loss: 0.870464]\n",
            "3815 [D loss: 0.597324, acc.: 62.50%] [G loss: 0.875420]\n",
            "3816 [D loss: 0.596807, acc.: 62.50%] [G loss: 0.872122]\n",
            "3817 [D loss: 0.593443, acc.: 62.50%] [G loss: 0.862177]\n",
            "3818 [D loss: 0.598070, acc.: 62.50%] [G loss: 0.868992]\n",
            "3819 [D loss: 0.599645, acc.: 62.50%] [G loss: 0.862424]\n",
            "3820 [D loss: 0.598276, acc.: 62.50%] [G loss: 0.870748]\n",
            "3821 [D loss: 0.595910, acc.: 62.50%] [G loss: 0.870567]\n",
            "3822 [D loss: 0.597145, acc.: 62.50%] [G loss: 0.871638]\n",
            "3823 [D loss: 0.596628, acc.: 62.50%] [G loss: 0.871010]\n",
            "3824 [D loss: 0.598454, acc.: 62.50%] [G loss: 0.869314]\n",
            "3825 [D loss: 0.597079, acc.: 62.50%] [G loss: 0.866412]\n",
            "3826 [D loss: 0.596319, acc.: 62.50%] [G loss: 0.872056]\n",
            "3827 [D loss: 0.601236, acc.: 62.50%] [G loss: 0.867427]\n",
            "3828 [D loss: 0.596744, acc.: 62.50%] [G loss: 0.865167]\n",
            "3829 [D loss: 0.596154, acc.: 62.50%] [G loss: 0.864044]\n",
            "3830 [D loss: 0.597705, acc.: 62.50%] [G loss: 0.864230]\n",
            "3831 [D loss: 0.596707, acc.: 62.50%] [G loss: 0.866554]\n",
            "3832 [D loss: 0.595375, acc.: 62.50%] [G loss: 0.868857]\n",
            "3833 [D loss: 0.595143, acc.: 62.50%] [G loss: 0.867991]\n",
            "3834 [D loss: 0.597762, acc.: 62.50%] [G loss: 0.867817]\n",
            "3835 [D loss: 0.600275, acc.: 62.50%] [G loss: 0.864101]\n",
            "3836 [D loss: 0.594433, acc.: 62.50%] [G loss: 0.870984]\n",
            "3837 [D loss: 0.598891, acc.: 62.50%] [G loss: 0.867800]\n",
            "3838 [D loss: 0.597295, acc.: 62.50%] [G loss: 0.873642]\n",
            "3839 [D loss: 0.596158, acc.: 62.50%] [G loss: 0.867022]\n",
            "3840 [D loss: 0.600460, acc.: 62.50%] [G loss: 0.869897]\n",
            "3841 [D loss: 0.598248, acc.: 62.50%] [G loss: 0.858600]\n",
            "3842 [D loss: 0.597505, acc.: 62.50%] [G loss: 0.864045]\n",
            "3843 [D loss: 0.595294, acc.: 62.50%] [G loss: 0.865028]\n",
            "3844 [D loss: 0.598218, acc.: 62.50%] [G loss: 0.862966]\n",
            "3845 [D loss: 0.596731, acc.: 62.50%] [G loss: 0.861862]\n",
            "3846 [D loss: 0.595283, acc.: 62.50%] [G loss: 0.869393]\n",
            "3847 [D loss: 0.597257, acc.: 62.50%] [G loss: 0.869430]\n",
            "3848 [D loss: 0.598235, acc.: 62.50%] [G loss: 0.868328]\n",
            "3849 [D loss: 0.598689, acc.: 62.50%] [G loss: 0.866582]\n",
            "3850 [D loss: 0.597740, acc.: 62.50%] [G loss: 0.865510]\n",
            "3851 [D loss: 0.600139, acc.: 62.50%] [G loss: 0.872240]\n",
            "3852 [D loss: 0.597463, acc.: 62.50%] [G loss: 0.870247]\n",
            "3853 [D loss: 0.596970, acc.: 62.50%] [G loss: 0.871228]\n",
            "3854 [D loss: 0.599058, acc.: 62.50%] [G loss: 0.875375]\n",
            "3855 [D loss: 0.598965, acc.: 62.50%] [G loss: 0.866715]\n",
            "3856 [D loss: 0.596929, acc.: 62.50%] [G loss: 0.872566]\n",
            "3857 [D loss: 0.596235, acc.: 62.50%] [G loss: 0.873112]\n",
            "3858 [D loss: 0.596726, acc.: 62.50%] [G loss: 0.873791]\n",
            "3859 [D loss: 0.598485, acc.: 62.50%] [G loss: 0.875196]\n",
            "3860 [D loss: 0.599269, acc.: 62.50%] [G loss: 0.873509]\n",
            "3861 [D loss: 0.599963, acc.: 62.50%] [G loss: 0.872809]\n",
            "3862 [D loss: 0.598051, acc.: 62.50%] [G loss: 0.877473]\n",
            "3863 [D loss: 0.598084, acc.: 62.50%] [G loss: 0.872684]\n",
            "3864 [D loss: 0.597790, acc.: 62.50%] [G loss: 0.874640]\n",
            "3865 [D loss: 0.599680, acc.: 62.50%] [G loss: 0.870476]\n",
            "3866 [D loss: 0.597757, acc.: 62.50%] [G loss: 0.869137]\n",
            "3867 [D loss: 0.596224, acc.: 62.50%] [G loss: 0.869640]\n",
            "3868 [D loss: 0.597105, acc.: 62.50%] [G loss: 0.870186]\n",
            "3869 [D loss: 0.595955, acc.: 62.50%] [G loss: 0.875536]\n",
            "3870 [D loss: 0.598872, acc.: 62.50%] [G loss: 0.871816]\n",
            "3871 [D loss: 0.598119, acc.: 62.50%] [G loss: 0.872743]\n",
            "3872 [D loss: 0.598869, acc.: 62.50%] [G loss: 0.867784]\n",
            "3873 [D loss: 0.597286, acc.: 62.50%] [G loss: 0.873207]\n",
            "3874 [D loss: 0.597900, acc.: 62.50%] [G loss: 0.877353]\n",
            "3875 [D loss: 0.598712, acc.: 62.50%] [G loss: 0.874586]\n",
            "3876 [D loss: 0.599389, acc.: 62.50%] [G loss: 0.870827]\n",
            "3877 [D loss: 0.596965, acc.: 62.50%] [G loss: 0.879517]\n",
            "3878 [D loss: 0.598025, acc.: 62.50%] [G loss: 0.874767]\n",
            "3879 [D loss: 0.595963, acc.: 62.50%] [G loss: 0.877352]\n",
            "3880 [D loss: 0.599779, acc.: 62.50%] [G loss: 0.875645]\n",
            "3881 [D loss: 0.598922, acc.: 62.50%] [G loss: 0.875962]\n",
            "3882 [D loss: 0.598341, acc.: 62.50%] [G loss: 0.875658]\n",
            "3883 [D loss: 0.599520, acc.: 62.50%] [G loss: 0.867416]\n",
            "3884 [D loss: 0.597478, acc.: 62.50%] [G loss: 0.865706]\n",
            "3885 [D loss: 0.601021, acc.: 60.94%] [G loss: 0.876650]\n",
            "3886 [D loss: 0.596973, acc.: 62.50%] [G loss: 0.874627]\n",
            "3887 [D loss: 0.599281, acc.: 62.50%] [G loss: 0.873089]\n",
            "3888 [D loss: 0.597353, acc.: 62.50%] [G loss: 0.874737]\n",
            "3889 [D loss: 0.596798, acc.: 62.50%] [G loss: 0.878067]\n",
            "3890 [D loss: 0.594083, acc.: 62.50%] [G loss: 0.880501]\n",
            "3891 [D loss: 0.595378, acc.: 62.50%] [G loss: 0.877491]\n",
            "3892 [D loss: 0.598772, acc.: 62.50%] [G loss: 0.873136]\n",
            "3893 [D loss: 0.601016, acc.: 62.50%] [G loss: 0.865302]\n",
            "3894 [D loss: 0.598410, acc.: 62.50%] [G loss: 0.858222]\n",
            "3895 [D loss: 0.600071, acc.: 62.50%] [G loss: 0.857061]\n",
            "3896 [D loss: 0.597794, acc.: 62.50%] [G loss: 0.859039]\n",
            "3897 [D loss: 0.598096, acc.: 62.50%] [G loss: 0.852325]\n",
            "3898 [D loss: 0.599309, acc.: 62.50%] [G loss: 0.855130]\n",
            "3899 [D loss: 0.599485, acc.: 62.50%] [G loss: 0.864047]\n",
            "3900 [D loss: 0.598016, acc.: 62.50%] [G loss: 0.859227]\n",
            "generated_data\n",
            "3901 [D loss: 0.599689, acc.: 62.50%] [G loss: 0.857437]\n",
            "3902 [D loss: 0.597674, acc.: 62.50%] [G loss: 0.864505]\n",
            "3903 [D loss: 0.597400, acc.: 62.50%] [G loss: 0.859813]\n",
            "3904 [D loss: 0.596012, acc.: 62.50%] [G loss: 0.874560]\n",
            "3905 [D loss: 0.597999, acc.: 62.50%] [G loss: 0.870968]\n",
            "3906 [D loss: 0.600398, acc.: 62.50%] [G loss: 0.867841]\n",
            "3907 [D loss: 0.595403, acc.: 62.50%] [G loss: 0.861679]\n",
            "3908 [D loss: 0.599278, acc.: 62.50%] [G loss: 0.865716]\n",
            "3909 [D loss: 0.599290, acc.: 62.50%] [G loss: 0.856360]\n",
            "3910 [D loss: 0.598690, acc.: 62.50%] [G loss: 0.860586]\n",
            "3911 [D loss: 0.597076, acc.: 62.50%] [G loss: 0.856879]\n",
            "3912 [D loss: 0.597504, acc.: 62.50%] [G loss: 0.860972]\n",
            "3913 [D loss: 0.599378, acc.: 62.50%] [G loss: 0.859372]\n",
            "3914 [D loss: 0.596819, acc.: 62.50%] [G loss: 0.861952]\n",
            "3915 [D loss: 0.597750, acc.: 62.50%] [G loss: 0.858875]\n",
            "3916 [D loss: 0.598460, acc.: 62.50%] [G loss: 0.859540]\n",
            "3917 [D loss: 0.598184, acc.: 62.50%] [G loss: 0.861187]\n",
            "3918 [D loss: 0.598758, acc.: 62.50%] [G loss: 0.861388]\n",
            "3919 [D loss: 0.597582, acc.: 62.50%] [G loss: 0.861652]\n",
            "3920 [D loss: 0.596974, acc.: 62.50%] [G loss: 0.860096]\n",
            "3921 [D loss: 0.599652, acc.: 62.50%] [G loss: 0.859606]\n",
            "3922 [D loss: 0.595271, acc.: 62.50%] [G loss: 0.861846]\n",
            "3923 [D loss: 0.598814, acc.: 62.50%] [G loss: 0.858235]\n",
            "3924 [D loss: 0.596791, acc.: 62.50%] [G loss: 0.857500]\n",
            "3925 [D loss: 0.598537, acc.: 62.50%] [G loss: 0.860251]\n",
            "3926 [D loss: 0.596503, acc.: 62.50%] [G loss: 0.861515]\n",
            "3927 [D loss: 0.599501, acc.: 62.50%] [G loss: 0.860341]\n",
            "3928 [D loss: 0.597986, acc.: 62.50%] [G loss: 0.856850]\n",
            "3929 [D loss: 0.619008, acc.: 57.81%] [G loss: 0.862159]\n",
            "3930 [D loss: 0.596535, acc.: 62.50%] [G loss: 0.860507]\n",
            "3931 [D loss: 0.599417, acc.: 62.50%] [G loss: 0.864509]\n",
            "3932 [D loss: 0.597923, acc.: 62.50%] [G loss: 0.866218]\n",
            "3933 [D loss: 0.598692, acc.: 62.50%] [G loss: 0.862829]\n",
            "3934 [D loss: 0.599001, acc.: 62.50%] [G loss: 0.862482]\n",
            "3935 [D loss: 0.597270, acc.: 62.50%] [G loss: 0.867293]\n",
            "3936 [D loss: 0.597620, acc.: 62.50%] [G loss: 0.860177]\n",
            "3937 [D loss: 0.596122, acc.: 62.50%] [G loss: 0.859558]\n",
            "3938 [D loss: 0.597892, acc.: 62.50%] [G loss: 0.864664]\n",
            "3939 [D loss: 0.598210, acc.: 62.50%] [G loss: 0.868655]\n",
            "3940 [D loss: 0.597566, acc.: 62.50%] [G loss: 0.872052]\n",
            "3941 [D loss: 0.598077, acc.: 62.50%] [G loss: 0.875779]\n",
            "3942 [D loss: 0.597698, acc.: 62.50%] [G loss: 0.882524]\n",
            "3943 [D loss: 0.597955, acc.: 62.50%] [G loss: 0.871465]\n",
            "3944 [D loss: 0.599441, acc.: 62.50%] [G loss: 0.864481]\n",
            "3945 [D loss: 0.596187, acc.: 62.50%] [G loss: 0.867239]\n",
            "3946 [D loss: 0.599185, acc.: 62.50%] [G loss: 0.869257]\n",
            "3947 [D loss: 0.598037, acc.: 62.50%] [G loss: 0.873427]\n",
            "3948 [D loss: 0.597424, acc.: 62.50%] [G loss: 0.871206]\n",
            "3949 [D loss: 0.600722, acc.: 62.50%] [G loss: 0.863546]\n",
            "3950 [D loss: 0.600040, acc.: 62.50%] [G loss: 0.861556]\n",
            "3951 [D loss: 0.599046, acc.: 62.50%] [G loss: 0.870767]\n",
            "3952 [D loss: 0.597656, acc.: 62.50%] [G loss: 0.871863]\n",
            "3953 [D loss: 0.598521, acc.: 62.50%] [G loss: 0.863538]\n",
            "3954 [D loss: 0.596295, acc.: 62.50%] [G loss: 0.869066]\n",
            "3955 [D loss: 0.598102, acc.: 62.50%] [G loss: 0.867408]\n",
            "3956 [D loss: 0.599411, acc.: 62.50%] [G loss: 0.862452]\n",
            "3957 [D loss: 0.598669, acc.: 62.50%] [G loss: 0.863580]\n",
            "3958 [D loss: 0.597730, acc.: 62.50%] [G loss: 0.856584]\n",
            "3959 [D loss: 0.600531, acc.: 62.50%] [G loss: 0.868156]\n",
            "3960 [D loss: 0.597738, acc.: 62.50%] [G loss: 0.857947]\n",
            "3961 [D loss: 0.599521, acc.: 62.50%] [G loss: 0.853047]\n",
            "3962 [D loss: 0.599875, acc.: 62.50%] [G loss: 0.857676]\n",
            "3963 [D loss: 0.597668, acc.: 62.50%] [G loss: 0.861685]\n",
            "3964 [D loss: 0.598073, acc.: 62.50%] [G loss: 0.866197]\n",
            "3965 [D loss: 0.599457, acc.: 62.50%] [G loss: 0.861421]\n",
            "3966 [D loss: 0.598255, acc.: 62.50%] [G loss: 0.866835]\n",
            "3967 [D loss: 0.600177, acc.: 62.50%] [G loss: 0.853209]\n",
            "3968 [D loss: 0.599107, acc.: 62.50%] [G loss: 0.862435]\n",
            "3969 [D loss: 0.600136, acc.: 62.50%] [G loss: 0.861952]\n",
            "3970 [D loss: 0.597056, acc.: 62.50%] [G loss: 0.859344]\n",
            "3971 [D loss: 0.596205, acc.: 62.50%] [G loss: 0.858618]\n",
            "3972 [D loss: 0.599548, acc.: 62.50%] [G loss: 0.855888]\n",
            "3973 [D loss: 0.596891, acc.: 62.50%] [G loss: 0.866773]\n",
            "3974 [D loss: 0.598850, acc.: 62.50%] [G loss: 0.868477]\n",
            "3975 [D loss: 0.597946, acc.: 62.50%] [G loss: 0.864261]\n",
            "3976 [D loss: 0.595684, acc.: 62.50%] [G loss: 0.871229]\n",
            "3977 [D loss: 0.600840, acc.: 62.50%] [G loss: 0.869750]\n",
            "3978 [D loss: 0.597974, acc.: 62.50%] [G loss: 0.867164]\n",
            "3979 [D loss: 0.598180, acc.: 62.50%] [G loss: 0.859348]\n",
            "3980 [D loss: 0.599949, acc.: 62.50%] [G loss: 0.863036]\n",
            "3981 [D loss: 0.597627, acc.: 62.50%] [G loss: 0.861697]\n",
            "3982 [D loss: 0.598928, acc.: 62.50%] [G loss: 0.861228]\n",
            "3983 [D loss: 0.597177, acc.: 62.50%] [G loss: 0.860664]\n",
            "3984 [D loss: 0.598867, acc.: 62.50%] [G loss: 0.862419]\n",
            "3985 [D loss: 0.597175, acc.: 62.50%] [G loss: 0.870153]\n",
            "3986 [D loss: 0.596247, acc.: 62.50%] [G loss: 0.871823]\n",
            "3987 [D loss: 0.598624, acc.: 62.50%] [G loss: 0.872429]\n",
            "3988 [D loss: 0.595230, acc.: 62.50%] [G loss: 0.880299]\n",
            "3989 [D loss: 0.590040, acc.: 62.50%] [G loss: 0.914817]\n",
            "3990 [D loss: 0.602626, acc.: 62.50%] [G loss: 0.873013]\n",
            "3991 [D loss: 0.676518, acc.: 45.31%] [G loss: 0.896219]\n",
            "3992 [D loss: 0.611847, acc.: 62.50%] [G loss: 0.870676]\n",
            "3993 [D loss: 0.599366, acc.: 62.50%] [G loss: 0.871275]\n",
            "3994 [D loss: 0.597593, acc.: 62.50%] [G loss: 0.884759]\n",
            "3995 [D loss: 0.603662, acc.: 62.50%] [G loss: 0.869287]\n",
            "3996 [D loss: 0.597698, acc.: 62.50%] [G loss: 0.867803]\n",
            "3997 [D loss: 0.595586, acc.: 62.50%] [G loss: 0.887072]\n",
            "3998 [D loss: 0.603322, acc.: 62.50%] [G loss: 0.873485]\n",
            "3999 [D loss: 0.597520, acc.: 62.50%] [G loss: 0.872116]\n",
            "4000 [D loss: 0.597758, acc.: 62.50%] [G loss: 0.874568]\n",
            "generated_data\n",
            "4001 [D loss: 0.596526, acc.: 62.50%] [G loss: 0.875754]\n",
            "4002 [D loss: 0.600955, acc.: 62.50%] [G loss: 0.870525]\n",
            "4003 [D loss: 0.599572, acc.: 62.50%] [G loss: 0.868152]\n",
            "4004 [D loss: 0.597970, acc.: 62.50%] [G loss: 0.871303]\n",
            "4005 [D loss: 0.598441, acc.: 62.50%] [G loss: 0.869414]\n",
            "4006 [D loss: 0.599464, acc.: 62.50%] [G loss: 0.871716]\n",
            "4007 [D loss: 0.598052, acc.: 62.50%] [G loss: 0.863858]\n",
            "4008 [D loss: 0.596758, acc.: 62.50%] [G loss: 0.868347]\n",
            "4009 [D loss: 0.598201, acc.: 62.50%] [G loss: 0.867588]\n",
            "4010 [D loss: 0.599548, acc.: 62.50%] [G loss: 0.867765]\n",
            "4011 [D loss: 0.597632, acc.: 62.50%] [G loss: 0.867630]\n",
            "4012 [D loss: 0.598174, acc.: 62.50%] [G loss: 0.865480]\n",
            "4013 [D loss: 0.596638, acc.: 62.50%] [G loss: 0.866465]\n",
            "4014 [D loss: 0.599145, acc.: 62.50%] [G loss: 0.864852]\n",
            "4015 [D loss: 0.595323, acc.: 62.50%] [G loss: 0.862926]\n",
            "4016 [D loss: 0.588134, acc.: 62.50%] [G loss: 0.962237]\n",
            "4017 [D loss: 0.619175, acc.: 62.50%] [G loss: 0.872986]\n",
            "4018 [D loss: 0.600126, acc.: 62.50%] [G loss: 0.879132]\n",
            "4019 [D loss: 0.596334, acc.: 62.50%] [G loss: 0.867541]\n",
            "4020 [D loss: 0.603101, acc.: 62.50%] [G loss: 0.871486]\n",
            "4021 [D loss: 0.596595, acc.: 62.50%] [G loss: 0.869646]\n",
            "4022 [D loss: 0.594975, acc.: 62.50%] [G loss: 0.863869]\n",
            "4023 [D loss: 0.602129, acc.: 62.50%] [G loss: 0.860982]\n",
            "4024 [D loss: 0.598005, acc.: 62.50%] [G loss: 0.869152]\n",
            "4025 [D loss: 0.598718, acc.: 62.50%] [G loss: 0.866790]\n",
            "4026 [D loss: 0.596560, acc.: 62.50%] [G loss: 0.859589]\n",
            "4027 [D loss: 0.596479, acc.: 62.50%] [G loss: 0.862494]\n",
            "4028 [D loss: 0.596995, acc.: 62.50%] [G loss: 0.862829]\n",
            "4029 [D loss: 0.597137, acc.: 62.50%] [G loss: 0.864093]\n",
            "4030 [D loss: 0.596341, acc.: 62.50%] [G loss: 0.861266]\n",
            "4031 [D loss: 0.597620, acc.: 62.50%] [G loss: 0.860347]\n",
            "4032 [D loss: 0.597659, acc.: 62.50%] [G loss: 0.860814]\n",
            "4033 [D loss: 0.598741, acc.: 62.50%] [G loss: 0.857635]\n",
            "4034 [D loss: 0.597819, acc.: 62.50%] [G loss: 0.848884]\n",
            "4035 [D loss: 0.597349, acc.: 62.50%] [G loss: 0.860480]\n",
            "4036 [D loss: 0.599610, acc.: 62.50%] [G loss: 0.861418]\n",
            "4037 [D loss: 0.602252, acc.: 62.50%] [G loss: 0.858475]\n",
            "4038 [D loss: 0.598627, acc.: 62.50%] [G loss: 0.850569]\n",
            "4039 [D loss: 0.597071, acc.: 62.50%] [G loss: 0.853036]\n",
            "4040 [D loss: 0.599133, acc.: 62.50%] [G loss: 0.855200]\n",
            "4041 [D loss: 0.598502, acc.: 62.50%] [G loss: 0.853210]\n",
            "4042 [D loss: 0.599100, acc.: 62.50%] [G loss: 0.857405]\n",
            "4043 [D loss: 0.596697, acc.: 62.50%] [G loss: 0.868465]\n",
            "4044 [D loss: 0.592631, acc.: 62.50%] [G loss: 0.865116]\n",
            "4045 [D loss: 0.600226, acc.: 62.50%] [G loss: 0.880672]\n",
            "4046 [D loss: 0.597069, acc.: 62.50%] [G loss: 0.865749]\n",
            "4047 [D loss: 0.605225, acc.: 62.50%] [G loss: 0.863072]\n",
            "4048 [D loss: 0.597184, acc.: 62.50%] [G loss: 0.857776]\n",
            "4049 [D loss: 0.600098, acc.: 62.50%] [G loss: 0.860716]\n",
            "4050 [D loss: 0.597676, acc.: 62.50%] [G loss: 0.863370]\n",
            "4051 [D loss: 0.599545, acc.: 62.50%] [G loss: 0.859159]\n",
            "4052 [D loss: 0.598315, acc.: 62.50%] [G loss: 0.863817]\n",
            "4053 [D loss: 0.599771, acc.: 62.50%] [G loss: 0.855717]\n",
            "4054 [D loss: 0.597773, acc.: 62.50%] [G loss: 0.861910]\n",
            "4055 [D loss: 0.596863, acc.: 62.50%] [G loss: 0.862244]\n",
            "4056 [D loss: 0.593759, acc.: 62.50%] [G loss: 0.861291]\n",
            "4057 [D loss: 0.596949, acc.: 62.50%] [G loss: 0.861196]\n",
            "4058 [D loss: 0.598962, acc.: 62.50%] [G loss: 0.859781]\n",
            "4059 [D loss: 0.596219, acc.: 62.50%] [G loss: 0.860133]\n",
            "4060 [D loss: 0.598776, acc.: 62.50%] [G loss: 0.860557]\n",
            "4061 [D loss: 0.596574, acc.: 62.50%] [G loss: 0.853808]\n",
            "4062 [D loss: 0.596310, acc.: 62.50%] [G loss: 0.864630]\n",
            "4063 [D loss: 0.595635, acc.: 62.50%] [G loss: 0.868171]\n",
            "4064 [D loss: 0.595000, acc.: 64.06%] [G loss: 0.866856]\n",
            "4065 [D loss: 0.588076, acc.: 67.19%] [G loss: 0.807752]\n",
            "4066 [D loss: 0.698092, acc.: 40.62%] [G loss: 0.864351]\n",
            "4067 [D loss: 0.595922, acc.: 62.50%] [G loss: 0.877564]\n",
            "4068 [D loss: 0.602607, acc.: 62.50%] [G loss: 0.890048]\n",
            "4069 [D loss: 0.599968, acc.: 62.50%] [G loss: 0.887505]\n",
            "4070 [D loss: 0.604165, acc.: 62.50%] [G loss: 0.887605]\n",
            "4071 [D loss: 0.605504, acc.: 62.50%] [G loss: 0.872817]\n",
            "4072 [D loss: 0.604668, acc.: 62.50%] [G loss: 0.862482]\n",
            "4073 [D loss: 0.605920, acc.: 62.50%] [G loss: 0.856181]\n",
            "4074 [D loss: 0.600959, acc.: 62.50%] [G loss: 0.853331]\n",
            "4075 [D loss: 0.601699, acc.: 64.06%] [G loss: 0.843813]\n",
            "4076 [D loss: 0.603576, acc.: 62.50%] [G loss: 0.841491]\n",
            "4077 [D loss: 0.602308, acc.: 62.50%] [G loss: 0.840616]\n",
            "4078 [D loss: 0.599117, acc.: 62.50%] [G loss: 0.839297]\n",
            "4079 [D loss: 0.597802, acc.: 62.50%] [G loss: 0.839577]\n",
            "4080 [D loss: 0.598045, acc.: 62.50%] [G loss: 0.840470]\n",
            "4081 [D loss: 0.598349, acc.: 62.50%] [G loss: 0.845265]\n",
            "4082 [D loss: 0.596595, acc.: 62.50%] [G loss: 0.840314]\n",
            "4083 [D loss: 0.597628, acc.: 62.50%] [G loss: 0.840708]\n",
            "4084 [D loss: 0.594376, acc.: 62.50%] [G loss: 0.848036]\n",
            "4085 [D loss: 0.602359, acc.: 62.50%] [G loss: 0.841552]\n",
            "4086 [D loss: 0.601996, acc.: 62.50%] [G loss: 0.844666]\n",
            "4087 [D loss: 0.597559, acc.: 62.50%] [G loss: 0.846656]\n",
            "4088 [D loss: 0.599260, acc.: 62.50%] [G loss: 0.843418]\n",
            "4089 [D loss: 0.600314, acc.: 62.50%] [G loss: 0.842798]\n",
            "4090 [D loss: 0.599279, acc.: 62.50%] [G loss: 0.842983]\n",
            "4091 [D loss: 0.597280, acc.: 62.50%] [G loss: 0.843477]\n",
            "4092 [D loss: 0.595844, acc.: 62.50%] [G loss: 0.845656]\n",
            "4093 [D loss: 0.596764, acc.: 62.50%] [G loss: 0.846237]\n",
            "4094 [D loss: 0.597499, acc.: 62.50%] [G loss: 0.844359]\n",
            "4095 [D loss: 0.597617, acc.: 62.50%] [G loss: 0.841752]\n",
            "4096 [D loss: 0.597317, acc.: 62.50%] [G loss: 0.846203]\n",
            "4097 [D loss: 0.597339, acc.: 62.50%] [G loss: 0.843972]\n",
            "4098 [D loss: 0.600326, acc.: 62.50%] [G loss: 0.846046]\n",
            "4099 [D loss: 0.597181, acc.: 62.50%] [G loss: 0.847648]\n",
            "4100 [D loss: 0.597828, acc.: 62.50%] [G loss: 0.843168]\n",
            "generated_data\n",
            "4101 [D loss: 0.597586, acc.: 62.50%] [G loss: 0.843299]\n",
            "4102 [D loss: 0.597438, acc.: 62.50%] [G loss: 0.849878]\n",
            "4103 [D loss: 0.598104, acc.: 62.50%] [G loss: 0.844985]\n",
            "4104 [D loss: 0.597845, acc.: 62.50%] [G loss: 0.841825]\n",
            "4105 [D loss: 0.597832, acc.: 62.50%] [G loss: 0.853026]\n",
            "4106 [D loss: 0.598154, acc.: 62.50%] [G loss: 0.859173]\n",
            "4107 [D loss: 0.599645, acc.: 62.50%] [G loss: 0.849150]\n",
            "4108 [D loss: 0.597106, acc.: 62.50%] [G loss: 0.844614]\n",
            "4109 [D loss: 0.598464, acc.: 62.50%] [G loss: 0.847243]\n",
            "4110 [D loss: 0.599681, acc.: 62.50%] [G loss: 0.845514]\n",
            "4111 [D loss: 0.599184, acc.: 62.50%] [G loss: 0.845868]\n",
            "4112 [D loss: 0.597358, acc.: 62.50%] [G loss: 0.849738]\n",
            "4113 [D loss: 0.596245, acc.: 62.50%] [G loss: 0.847485]\n",
            "4114 [D loss: 0.597956, acc.: 62.50%] [G loss: 0.846453]\n",
            "4115 [D loss: 0.599521, acc.: 62.50%] [G loss: 0.853310]\n",
            "4116 [D loss: 0.599980, acc.: 62.50%] [G loss: 0.850428]\n",
            "4117 [D loss: 0.597508, acc.: 62.50%] [G loss: 0.917453]\n",
            "4118 [D loss: 0.614068, acc.: 62.50%] [G loss: 0.849528]\n",
            "4119 [D loss: 0.599906, acc.: 62.50%] [G loss: 0.843975]\n",
            "4120 [D loss: 0.597115, acc.: 62.50%] [G loss: 0.844720]\n",
            "4121 [D loss: 0.599240, acc.: 62.50%] [G loss: 0.844758]\n",
            "4122 [D loss: 0.598210, acc.: 62.50%] [G loss: 0.848194]\n",
            "4123 [D loss: 0.598146, acc.: 62.50%] [G loss: 0.846343]\n",
            "4124 [D loss: 0.597995, acc.: 62.50%] [G loss: 0.848714]\n",
            "4125 [D loss: 0.597491, acc.: 62.50%] [G loss: 0.850544]\n",
            "4126 [D loss: 0.599527, acc.: 62.50%] [G loss: 0.859297]\n",
            "4127 [D loss: 0.601121, acc.: 62.50%] [G loss: 0.849566]\n",
            "4128 [D loss: 0.599360, acc.: 62.50%] [G loss: 0.851650]\n",
            "4129 [D loss: 0.596727, acc.: 62.50%] [G loss: 0.853228]\n",
            "4130 [D loss: 0.597066, acc.: 62.50%] [G loss: 0.850335]\n",
            "4131 [D loss: 0.597826, acc.: 62.50%] [G loss: 0.857009]\n",
            "4132 [D loss: 0.596883, acc.: 62.50%] [G loss: 0.859206]\n",
            "4133 [D loss: 0.597667, acc.: 62.50%] [G loss: 0.857301]\n",
            "4134 [D loss: 0.599747, acc.: 62.50%] [G loss: 0.852643]\n",
            "4135 [D loss: 0.599059, acc.: 62.50%] [G loss: 0.851299]\n",
            "4136 [D loss: 0.599491, acc.: 62.50%] [G loss: 0.854559]\n",
            "4137 [D loss: 0.596702, acc.: 62.50%] [G loss: 0.854608]\n",
            "4138 [D loss: 0.595239, acc.: 62.50%] [G loss: 0.859132]\n",
            "4139 [D loss: 0.598636, acc.: 62.50%] [G loss: 0.857857]\n",
            "4140 [D loss: 0.602355, acc.: 62.50%] [G loss: 0.852195]\n",
            "4141 [D loss: 0.596708, acc.: 62.50%] [G loss: 0.852243]\n",
            "4142 [D loss: 0.599177, acc.: 62.50%] [G loss: 0.861978]\n",
            "4143 [D loss: 0.597558, acc.: 62.50%] [G loss: 0.860918]\n",
            "4144 [D loss: 0.598705, acc.: 62.50%] [G loss: 0.866874]\n",
            "4145 [D loss: 0.598383, acc.: 62.50%] [G loss: 0.855444]\n",
            "4146 [D loss: 0.601815, acc.: 62.50%] [G loss: 0.854194]\n",
            "4147 [D loss: 0.598771, acc.: 62.50%] [G loss: 0.854687]\n",
            "4148 [D loss: 0.595141, acc.: 62.50%] [G loss: 0.858595]\n",
            "4149 [D loss: 0.597041, acc.: 62.50%] [G loss: 0.860501]\n",
            "4150 [D loss: 0.597559, acc.: 62.50%] [G loss: 0.858698]\n",
            "4151 [D loss: 0.598476, acc.: 62.50%] [G loss: 0.862813]\n",
            "4152 [D loss: 0.597522, acc.: 62.50%] [G loss: 0.858903]\n",
            "4153 [D loss: 0.599318, acc.: 62.50%] [G loss: 0.867159]\n",
            "4154 [D loss: 0.598905, acc.: 62.50%] [G loss: 0.861700]\n",
            "4155 [D loss: 0.598774, acc.: 62.50%] [G loss: 0.863327]\n",
            "4156 [D loss: 0.598814, acc.: 62.50%] [G loss: 0.860019]\n",
            "4157 [D loss: 0.595942, acc.: 62.50%] [G loss: 0.864836]\n",
            "4158 [D loss: 0.596426, acc.: 62.50%] [G loss: 0.864529]\n",
            "4159 [D loss: 0.596748, acc.: 62.50%] [G loss: 0.866965]\n",
            "4160 [D loss: 0.598335, acc.: 62.50%] [G loss: 0.862211]\n",
            "4161 [D loss: 0.599205, acc.: 62.50%] [G loss: 0.863677]\n",
            "4162 [D loss: 0.597694, acc.: 62.50%] [G loss: 0.862582]\n",
            "4163 [D loss: 0.596712, acc.: 62.50%] [G loss: 0.863002]\n",
            "4164 [D loss: 0.596872, acc.: 62.50%] [G loss: 0.869583]\n",
            "4165 [D loss: 0.600024, acc.: 62.50%] [G loss: 0.863996]\n",
            "4166 [D loss: 0.598531, acc.: 62.50%] [G loss: 0.856491]\n",
            "4167 [D loss: 0.596711, acc.: 62.50%] [G loss: 0.870528]\n",
            "4168 [D loss: 0.598234, acc.: 62.50%] [G loss: 0.861028]\n",
            "4169 [D loss: 0.598586, acc.: 62.50%] [G loss: 0.855179]\n",
            "4170 [D loss: 0.596753, acc.: 62.50%] [G loss: 0.864118]\n",
            "4171 [D loss: 0.599399, acc.: 62.50%] [G loss: 0.865020]\n",
            "4172 [D loss: 0.597485, acc.: 62.50%] [G loss: 0.862181]\n",
            "4173 [D loss: 0.598205, acc.: 62.50%] [G loss: 0.865296]\n",
            "4174 [D loss: 0.599692, acc.: 62.50%] [G loss: 0.862724]\n",
            "4175 [D loss: 0.596502, acc.: 62.50%] [G loss: 0.859146]\n",
            "4176 [D loss: 0.597895, acc.: 62.50%] [G loss: 0.863095]\n",
            "4177 [D loss: 0.599019, acc.: 62.50%] [G loss: 0.863166]\n",
            "4178 [D loss: 0.596646, acc.: 62.50%] [G loss: 0.876018]\n",
            "4179 [D loss: 0.597151, acc.: 62.50%] [G loss: 0.877661]\n",
            "4180 [D loss: 0.598670, acc.: 62.50%] [G loss: 0.885106]\n",
            "4181 [D loss: 0.600572, acc.: 62.50%] [G loss: 0.878007]\n",
            "4182 [D loss: 0.597399, acc.: 62.50%] [G loss: 0.879862]\n",
            "4183 [D loss: 0.599701, acc.: 62.50%] [G loss: 0.875665]\n",
            "4184 [D loss: 0.602580, acc.: 62.50%] [G loss: 0.878258]\n",
            "4185 [D loss: 0.599504, acc.: 62.50%] [G loss: 0.872416]\n",
            "4186 [D loss: 0.600111, acc.: 62.50%] [G loss: 0.881491]\n",
            "4187 [D loss: 0.597646, acc.: 62.50%] [G loss: 0.875872]\n",
            "4188 [D loss: 0.601551, acc.: 62.50%] [G loss: 0.869975]\n",
            "4189 [D loss: 0.597469, acc.: 62.50%] [G loss: 0.865604]\n",
            "4190 [D loss: 0.597215, acc.: 62.50%] [G loss: 0.873586]\n",
            "4191 [D loss: 0.597737, acc.: 62.50%] [G loss: 0.868790]\n",
            "4192 [D loss: 0.598398, acc.: 62.50%] [G loss: 0.873418]\n",
            "4193 [D loss: 0.596690, acc.: 62.50%] [G loss: 0.874829]\n",
            "4194 [D loss: 0.597952, acc.: 62.50%] [G loss: 0.867348]\n",
            "4195 [D loss: 0.596968, acc.: 62.50%] [G loss: 0.869701]\n",
            "4196 [D loss: 0.596677, acc.: 62.50%] [G loss: 0.961206]\n",
            "4197 [D loss: 0.613970, acc.: 62.50%] [G loss: 0.862504]\n",
            "4198 [D loss: 0.599435, acc.: 62.50%] [G loss: 0.868772]\n",
            "4199 [D loss: 0.597203, acc.: 62.50%] [G loss: 0.862544]\n",
            "4200 [D loss: 0.597048, acc.: 62.50%] [G loss: 0.864854]\n",
            "generated_data\n",
            "4201 [D loss: 0.596828, acc.: 62.50%] [G loss: 0.872066]\n",
            "4202 [D loss: 0.596513, acc.: 62.50%] [G loss: 0.863933]\n",
            "4203 [D loss: 0.598005, acc.: 62.50%] [G loss: 0.867050]\n",
            "4204 [D loss: 0.596813, acc.: 62.50%] [G loss: 0.865445]\n",
            "4205 [D loss: 0.595920, acc.: 62.50%] [G loss: 0.865998]\n",
            "4206 [D loss: 0.597251, acc.: 62.50%] [G loss: 0.872107]\n",
            "4207 [D loss: 0.596800, acc.: 62.50%] [G loss: 0.865480]\n",
            "4208 [D loss: 0.600092, acc.: 62.50%] [G loss: 0.866076]\n",
            "4209 [D loss: 0.599341, acc.: 62.50%] [G loss: 0.866590]\n",
            "4210 [D loss: 0.599300, acc.: 62.50%] [G loss: 0.863691]\n",
            "4211 [D loss: 0.598636, acc.: 62.50%] [G loss: 0.862736]\n",
            "4212 [D loss: 0.594790, acc.: 62.50%] [G loss: 0.864152]\n",
            "4213 [D loss: 0.596178, acc.: 62.50%] [G loss: 0.864419]\n",
            "4214 [D loss: 0.597905, acc.: 62.50%] [G loss: 0.859668]\n",
            "4215 [D loss: 0.595916, acc.: 62.50%] [G loss: 0.861575]\n",
            "4216 [D loss: 0.594132, acc.: 62.50%] [G loss: 0.853910]\n",
            "4217 [D loss: 0.595915, acc.: 62.50%] [G loss: 0.847011]\n",
            "4218 [D loss: 0.604187, acc.: 60.94%] [G loss: 0.847083]\n",
            "4219 [D loss: 0.591720, acc.: 62.50%] [G loss: 0.852462]\n",
            "4220 [D loss: 0.607605, acc.: 59.38%] [G loss: 0.858105]\n",
            "4221 [D loss: 0.597532, acc.: 62.50%] [G loss: 0.860548]\n",
            "4222 [D loss: 0.596524, acc.: 62.50%] [G loss: 0.857724]\n",
            "4223 [D loss: 0.598082, acc.: 62.50%] [G loss: 0.857542]\n",
            "4224 [D loss: 0.599402, acc.: 62.50%] [G loss: 0.862278]\n",
            "4225 [D loss: 0.597426, acc.: 62.50%] [G loss: 0.861354]\n",
            "4226 [D loss: 0.597090, acc.: 62.50%] [G loss: 0.864407]\n",
            "4227 [D loss: 0.595888, acc.: 62.50%] [G loss: 0.863717]\n",
            "4228 [D loss: 0.596143, acc.: 62.50%] [G loss: 0.865615]\n",
            "4229 [D loss: 0.596493, acc.: 62.50%] [G loss: 0.866953]\n",
            "4230 [D loss: 0.596496, acc.: 62.50%] [G loss: 0.851146]\n",
            "4231 [D loss: 0.598585, acc.: 62.50%] [G loss: 0.856874]\n",
            "4232 [D loss: 0.596959, acc.: 62.50%] [G loss: 0.853779]\n",
            "4233 [D loss: 0.596220, acc.: 62.50%] [G loss: 0.838789]\n",
            "4234 [D loss: 0.608302, acc.: 62.50%] [G loss: 0.851683]\n",
            "4235 [D loss: 0.602196, acc.: 62.50%] [G loss: 0.905028]\n",
            "4236 [D loss: 0.611097, acc.: 62.50%] [G loss: 0.857144]\n",
            "4237 [D loss: 0.600332, acc.: 62.50%] [G loss: 0.857286]\n",
            "4238 [D loss: 0.598964, acc.: 62.50%] [G loss: 0.840973]\n",
            "4239 [D loss: 0.601099, acc.: 62.50%] [G loss: 0.846986]\n",
            "4240 [D loss: 0.594167, acc.: 62.50%] [G loss: 0.862044]\n",
            "4241 [D loss: 0.594827, acc.: 62.50%] [G loss: 0.852408]\n",
            "4242 [D loss: 0.598642, acc.: 62.50%] [G loss: 0.862827]\n",
            "4243 [D loss: 0.601611, acc.: 62.50%] [G loss: 0.865619]\n",
            "4244 [D loss: 0.593987, acc.: 62.50%] [G loss: 0.850736]\n",
            "4245 [D loss: 0.601106, acc.: 62.50%] [G loss: 0.869880]\n",
            "4246 [D loss: 0.597771, acc.: 62.50%] [G loss: 0.853000]\n",
            "4247 [D loss: 0.600095, acc.: 62.50%] [G loss: 0.864190]\n",
            "4248 [D loss: 0.596970, acc.: 62.50%] [G loss: 0.855798]\n",
            "4249 [D loss: 0.598611, acc.: 62.50%] [G loss: 0.859202]\n",
            "4250 [D loss: 0.599505, acc.: 62.50%] [G loss: 0.861633]\n",
            "4251 [D loss: 0.597246, acc.: 62.50%] [G loss: 0.861345]\n",
            "4252 [D loss: 0.599029, acc.: 62.50%] [G loss: 0.854715]\n",
            "4253 [D loss: 0.600230, acc.: 62.50%] [G loss: 0.853764]\n",
            "4254 [D loss: 0.598525, acc.: 62.50%] [G loss: 0.862506]\n",
            "4255 [D loss: 0.594192, acc.: 62.50%] [G loss: 0.871316]\n",
            "4256 [D loss: 0.598570, acc.: 62.50%] [G loss: 0.870241]\n",
            "4257 [D loss: 0.599282, acc.: 62.50%] [G loss: 0.863074]\n",
            "4258 [D loss: 0.601106, acc.: 62.50%] [G loss: 0.858671]\n",
            "4259 [D loss: 0.599215, acc.: 62.50%] [G loss: 0.867476]\n",
            "4260 [D loss: 0.597932, acc.: 62.50%] [G loss: 0.869776]\n",
            "4261 [D loss: 0.596185, acc.: 62.50%] [G loss: 0.875025]\n",
            "4262 [D loss: 0.593805, acc.: 62.50%] [G loss: 0.884068]\n",
            "4263 [D loss: 0.600271, acc.: 62.50%] [G loss: 0.893140]\n",
            "4264 [D loss: 0.601316, acc.: 62.50%] [G loss: 0.885460]\n",
            "4265 [D loss: 0.598386, acc.: 62.50%] [G loss: 0.878923]\n",
            "4266 [D loss: 0.598665, acc.: 62.50%] [G loss: 0.873719]\n",
            "4267 [D loss: 0.597089, acc.: 62.50%] [G loss: 0.866617]\n",
            "4268 [D loss: 0.599639, acc.: 62.50%] [G loss: 0.867338]\n",
            "4269 [D loss: 0.599791, acc.: 62.50%] [G loss: 0.863588]\n",
            "4270 [D loss: 0.600594, acc.: 62.50%] [G loss: 0.863625]\n",
            "4271 [D loss: 0.596306, acc.: 62.50%] [G loss: 0.864443]\n",
            "4272 [D loss: 0.597810, acc.: 62.50%] [G loss: 0.872213]\n",
            "4273 [D loss: 0.618295, acc.: 56.25%] [G loss: 0.868852]\n",
            "4274 [D loss: 0.595483, acc.: 62.50%] [G loss: 0.866214]\n",
            "4275 [D loss: 0.597122, acc.: 62.50%] [G loss: 0.866743]\n",
            "4276 [D loss: 0.597056, acc.: 62.50%] [G loss: 0.869861]\n",
            "4277 [D loss: 0.597665, acc.: 62.50%] [G loss: 0.865204]\n",
            "4278 [D loss: 0.598681, acc.: 62.50%] [G loss: 0.863696]\n",
            "4279 [D loss: 0.595972, acc.: 62.50%] [G loss: 0.869834]\n",
            "4280 [D loss: 0.596597, acc.: 62.50%] [G loss: 0.869045]\n",
            "4281 [D loss: 0.597044, acc.: 62.50%] [G loss: 0.869949]\n",
            "4282 [D loss: 0.598256, acc.: 62.50%] [G loss: 0.866077]\n",
            "4283 [D loss: 0.600266, acc.: 62.50%] [G loss: 0.867169]\n",
            "4284 [D loss: 0.597399, acc.: 62.50%] [G loss: 0.871092]\n",
            "4285 [D loss: 0.596780, acc.: 62.50%] [G loss: 0.867640]\n",
            "4286 [D loss: 0.597391, acc.: 62.50%] [G loss: 0.871675]\n",
            "4287 [D loss: 0.596759, acc.: 62.50%] [G loss: 0.860873]\n",
            "4288 [D loss: 0.598665, acc.: 62.50%] [G loss: 0.863551]\n",
            "4289 [D loss: 0.598350, acc.: 62.50%] [G loss: 0.866223]\n",
            "4290 [D loss: 0.597452, acc.: 62.50%] [G loss: 0.865715]\n",
            "4291 [D loss: 0.599694, acc.: 62.50%] [G loss: 0.859774]\n",
            "4292 [D loss: 0.600760, acc.: 62.50%] [G loss: 0.863342]\n",
            "4293 [D loss: 0.599600, acc.: 62.50%] [G loss: 0.860826]\n",
            "4294 [D loss: 0.597356, acc.: 62.50%] [G loss: 0.856837]\n",
            "4295 [D loss: 0.596496, acc.: 62.50%] [G loss: 0.861820]\n",
            "4296 [D loss: 0.599454, acc.: 62.50%] [G loss: 0.860907]\n",
            "4297 [D loss: 0.598161, acc.: 62.50%] [G loss: 0.861750]\n",
            "4298 [D loss: 0.597438, acc.: 62.50%] [G loss: 0.866139]\n",
            "4299 [D loss: 0.597514, acc.: 62.50%] [G loss: 0.866256]\n",
            "4300 [D loss: 0.597518, acc.: 62.50%] [G loss: 0.861844]\n",
            "generated_data\n",
            "4301 [D loss: 0.598935, acc.: 62.50%] [G loss: 0.862467]\n",
            "4302 [D loss: 0.598185, acc.: 62.50%] [G loss: 0.858382]\n",
            "4303 [D loss: 0.595680, acc.: 62.50%] [G loss: 0.859920]\n",
            "4304 [D loss: 0.596700, acc.: 62.50%] [G loss: 0.861769]\n",
            "4305 [D loss: 0.598229, acc.: 62.50%] [G loss: 0.860378]\n",
            "4306 [D loss: 0.600671, acc.: 62.50%] [G loss: 0.860822]\n",
            "4307 [D loss: 0.596917, acc.: 62.50%] [G loss: 0.866108]\n",
            "4308 [D loss: 0.597145, acc.: 62.50%] [G loss: 0.867515]\n",
            "4309 [D loss: 0.597303, acc.: 62.50%] [G loss: 0.860079]\n",
            "4310 [D loss: 0.599925, acc.: 62.50%] [G loss: 0.858496]\n",
            "4311 [D loss: 0.598167, acc.: 62.50%] [G loss: 0.859999]\n",
            "4312 [D loss: 0.596094, acc.: 62.50%] [G loss: 0.862910]\n",
            "4313 [D loss: 0.597581, acc.: 62.50%] [G loss: 0.865751]\n",
            "4314 [D loss: 0.597989, acc.: 62.50%] [G loss: 0.872890]\n",
            "4315 [D loss: 0.599124, acc.: 62.50%] [G loss: 0.866030]\n",
            "4316 [D loss: 0.596617, acc.: 62.50%] [G loss: 0.867906]\n",
            "4317 [D loss: 0.597319, acc.: 62.50%] [G loss: 0.870775]\n",
            "4318 [D loss: 0.599731, acc.: 62.50%] [G loss: 0.868362]\n",
            "4319 [D loss: 0.596968, acc.: 62.50%] [G loss: 0.867233]\n",
            "4320 [D loss: 0.596083, acc.: 62.50%] [G loss: 0.872606]\n",
            "4321 [D loss: 0.597449, acc.: 62.50%] [G loss: 0.877796]\n",
            "4322 [D loss: 0.598782, acc.: 62.50%] [G loss: 0.868559]\n",
            "4323 [D loss: 0.593724, acc.: 62.50%] [G loss: 0.865770]\n",
            "4324 [D loss: 0.593614, acc.: 62.50%] [G loss: 0.878011]\n",
            "4325 [D loss: 0.595316, acc.: 62.50%] [G loss: 0.876046]\n",
            "4326 [D loss: 0.599165, acc.: 62.50%] [G loss: 0.868396]\n",
            "4327 [D loss: 0.594457, acc.: 62.50%] [G loss: 0.873984]\n",
            "4328 [D loss: 0.594937, acc.: 62.50%] [G loss: 0.880458]\n",
            "4329 [D loss: 0.598208, acc.: 62.50%] [G loss: 0.873155]\n",
            "4330 [D loss: 0.596379, acc.: 62.50%] [G loss: 0.876389]\n",
            "4331 [D loss: 0.598199, acc.: 62.50%] [G loss: 0.869933]\n",
            "4332 [D loss: 0.595942, acc.: 62.50%] [G loss: 0.868354]\n",
            "4333 [D loss: 0.593783, acc.: 62.50%] [G loss: 0.877034]\n",
            "4334 [D loss: 0.595753, acc.: 62.50%] [G loss: 0.874295]\n",
            "4335 [D loss: 0.592281, acc.: 62.50%] [G loss: 0.873374]\n",
            "4336 [D loss: 0.596597, acc.: 62.50%] [G loss: 0.877746]\n",
            "4337 [D loss: 0.597184, acc.: 62.50%] [G loss: 0.873756]\n",
            "4338 [D loss: 0.600799, acc.: 62.50%] [G loss: 0.863428]\n",
            "4339 [D loss: 0.598139, acc.: 62.50%] [G loss: 0.859371]\n",
            "4340 [D loss: 0.594752, acc.: 62.50%] [G loss: 0.862909]\n",
            "4341 [D loss: 0.601619, acc.: 62.50%] [G loss: 0.848011]\n",
            "4342 [D loss: 0.598704, acc.: 62.50%] [G loss: 0.851520]\n",
            "4343 [D loss: 0.597537, acc.: 62.50%] [G loss: 0.849967]\n",
            "4344 [D loss: 0.597867, acc.: 62.50%] [G loss: 0.850877]\n",
            "4345 [D loss: 0.600531, acc.: 62.50%] [G loss: 0.856785]\n",
            "4346 [D loss: 0.595295, acc.: 62.50%] [G loss: 0.850850]\n",
            "4347 [D loss: 0.597691, acc.: 62.50%] [G loss: 0.856462]\n",
            "4348 [D loss: 0.595947, acc.: 62.50%] [G loss: 0.857267]\n",
            "4349 [D loss: 0.597939, acc.: 62.50%] [G loss: 0.853887]\n",
            "4350 [D loss: 0.597630, acc.: 62.50%] [G loss: 0.885444]\n",
            "4351 [D loss: 0.610818, acc.: 62.50%] [G loss: 0.867330]\n",
            "4352 [D loss: 0.595706, acc.: 62.50%] [G loss: 0.868354]\n",
            "4353 [D loss: 0.602643, acc.: 62.50%] [G loss: 0.866767]\n",
            "4354 [D loss: 0.600123, acc.: 62.50%] [G loss: 0.868573]\n",
            "4355 [D loss: 0.601684, acc.: 62.50%] [G loss: 0.866542]\n",
            "4356 [D loss: 0.598113, acc.: 62.50%] [G loss: 0.864600]\n",
            "4357 [D loss: 0.599387, acc.: 62.50%] [G loss: 0.866521]\n",
            "4358 [D loss: 0.599397, acc.: 62.50%] [G loss: 0.864963]\n",
            "4359 [D loss: 0.598516, acc.: 62.50%] [G loss: 0.860204]\n",
            "4360 [D loss: 0.599223, acc.: 62.50%] [G loss: 0.863086]\n",
            "4361 [D loss: 0.597694, acc.: 62.50%] [G loss: 0.865522]\n",
            "4362 [D loss: 0.596928, acc.: 62.50%] [G loss: 0.861075]\n",
            "4363 [D loss: 0.597357, acc.: 62.50%] [G loss: 0.859078]\n",
            "4364 [D loss: 0.599179, acc.: 62.50%] [G loss: 0.860448]\n",
            "4365 [D loss: 0.598398, acc.: 62.50%] [G loss: 0.862557]\n",
            "4366 [D loss: 0.598395, acc.: 62.50%] [G loss: 0.858174]\n",
            "4367 [D loss: 0.597723, acc.: 62.50%] [G loss: 0.857010]\n",
            "4368 [D loss: 0.597033, acc.: 62.50%] [G loss: 0.858463]\n",
            "4369 [D loss: 0.599736, acc.: 62.50%] [G loss: 0.858686]\n",
            "4370 [D loss: 0.596340, acc.: 62.50%] [G loss: 0.860578]\n",
            "4371 [D loss: 0.597883, acc.: 62.50%] [G loss: 0.861838]\n",
            "4372 [D loss: 0.598344, acc.: 62.50%] [G loss: 0.861844]\n",
            "4373 [D loss: 0.596081, acc.: 62.50%] [G loss: 0.861189]\n",
            "4374 [D loss: 0.598601, acc.: 62.50%] [G loss: 0.857846]\n",
            "4375 [D loss: 0.599128, acc.: 62.50%] [G loss: 0.863355]\n",
            "4376 [D loss: 0.595137, acc.: 62.50%] [G loss: 0.858711]\n",
            "4377 [D loss: 0.596160, acc.: 62.50%] [G loss: 0.859233]\n",
            "4378 [D loss: 0.597742, acc.: 62.50%] [G loss: 0.858884]\n",
            "4379 [D loss: 0.597845, acc.: 62.50%] [G loss: 0.858884]\n",
            "4380 [D loss: 0.597560, acc.: 62.50%] [G loss: 0.859211]\n",
            "4381 [D loss: 0.596487, acc.: 62.50%] [G loss: 0.860926]\n",
            "4382 [D loss: 0.596897, acc.: 62.50%] [G loss: 0.862178]\n",
            "4383 [D loss: 0.595805, acc.: 62.50%] [G loss: 0.861645]\n",
            "4384 [D loss: 0.601264, acc.: 62.50%] [G loss: 0.852470]\n",
            "4385 [D loss: 0.599341, acc.: 62.50%] [G loss: 0.854924]\n",
            "4386 [D loss: 0.597883, acc.: 62.50%] [G loss: 0.854774]\n",
            "4387 [D loss: 0.597450, acc.: 62.50%] [G loss: 0.855788]\n",
            "4388 [D loss: 0.598417, acc.: 62.50%] [G loss: 0.857034]\n",
            "4389 [D loss: 0.598575, acc.: 62.50%] [G loss: 0.853043]\n",
            "4390 [D loss: 0.597780, acc.: 62.50%] [G loss: 0.855300]\n",
            "4391 [D loss: 0.598082, acc.: 62.50%] [G loss: 0.855469]\n",
            "4392 [D loss: 0.600731, acc.: 62.50%] [G loss: 0.851155]\n",
            "4393 [D loss: 0.597995, acc.: 62.50%] [G loss: 0.855177]\n",
            "4394 [D loss: 0.597853, acc.: 62.50%] [G loss: 0.853956]\n",
            "4395 [D loss: 0.598406, acc.: 62.50%] [G loss: 0.854046]\n",
            "4396 [D loss: 0.597835, acc.: 62.50%] [G loss: 0.853350]\n",
            "4397 [D loss: 0.598396, acc.: 62.50%] [G loss: 0.854282]\n",
            "4398 [D loss: 0.598287, acc.: 62.50%] [G loss: 0.853750]\n",
            "4399 [D loss: 0.598091, acc.: 62.50%] [G loss: 0.853755]\n",
            "4400 [D loss: 0.599622, acc.: 62.50%] [G loss: 0.852965]\n",
            "generated_data\n",
            "4401 [D loss: 0.597163, acc.: 62.50%] [G loss: 0.853150]\n",
            "4402 [D loss: 0.597118, acc.: 62.50%] [G loss: 0.851611]\n",
            "4403 [D loss: 0.598363, acc.: 62.50%] [G loss: 0.852144]\n",
            "4404 [D loss: 0.599368, acc.: 62.50%] [G loss: 0.850776]\n",
            "4405 [D loss: 0.598477, acc.: 62.50%] [G loss: 0.851554]\n",
            "4406 [D loss: 0.598586, acc.: 62.50%] [G loss: 0.849298]\n",
            "4407 [D loss: 0.598241, acc.: 62.50%] [G loss: 0.851838]\n",
            "4408 [D loss: 0.597946, acc.: 62.50%] [G loss: 0.849547]\n",
            "4409 [D loss: 0.596765, acc.: 62.50%] [G loss: 0.854502]\n",
            "4410 [D loss: 0.597666, acc.: 62.50%] [G loss: 0.849050]\n",
            "4411 [D loss: 0.596912, acc.: 62.50%] [G loss: 0.849982]\n",
            "4412 [D loss: 0.620150, acc.: 57.81%] [G loss: 0.848267]\n",
            "4413 [D loss: 0.599816, acc.: 62.50%] [G loss: 0.853206]\n",
            "4414 [D loss: 0.597401, acc.: 62.50%] [G loss: 0.852351]\n",
            "4415 [D loss: 0.599276, acc.: 62.50%] [G loss: 0.854048]\n",
            "4416 [D loss: 0.598071, acc.: 62.50%] [G loss: 0.849844]\n",
            "4417 [D loss: 0.597825, acc.: 62.50%] [G loss: 0.851046]\n",
            "4418 [D loss: 0.598111, acc.: 62.50%] [G loss: 0.852536]\n",
            "4419 [D loss: 0.598589, acc.: 62.50%] [G loss: 0.851947]\n",
            "4420 [D loss: 0.597010, acc.: 62.50%] [G loss: 0.851042]\n",
            "4421 [D loss: 0.619499, acc.: 62.50%] [G loss: 0.855198]\n",
            "4422 [D loss: 0.597484, acc.: 62.50%] [G loss: 0.855510]\n",
            "4423 [D loss: 0.600034, acc.: 62.50%] [G loss: 0.856937]\n",
            "4424 [D loss: 0.598408, acc.: 62.50%] [G loss: 0.851373]\n",
            "4425 [D loss: 0.592736, acc.: 62.50%] [G loss: 0.861798]\n",
            "4426 [D loss: 0.597982, acc.: 62.50%] [G loss: 0.858680]\n",
            "4427 [D loss: 0.598652, acc.: 62.50%] [G loss: 0.864820]\n",
            "4428 [D loss: 0.598013, acc.: 62.50%] [G loss: 0.855085]\n",
            "4429 [D loss: 0.600123, acc.: 62.50%] [G loss: 0.855534]\n",
            "4430 [D loss: 0.598868, acc.: 62.50%] [G loss: 0.854081]\n",
            "4431 [D loss: 0.598749, acc.: 62.50%] [G loss: 0.855581]\n",
            "4432 [D loss: 0.597858, acc.: 62.50%] [G loss: 0.854632]\n",
            "4433 [D loss: 0.598458, acc.: 62.50%] [G loss: 0.853905]\n",
            "4434 [D loss: 0.598635, acc.: 62.50%] [G loss: 0.853311]\n",
            "4435 [D loss: 0.597719, acc.: 62.50%] [G loss: 0.854225]\n",
            "4436 [D loss: 0.599139, acc.: 62.50%] [G loss: 0.853980]\n",
            "4437 [D loss: 0.598262, acc.: 62.50%] [G loss: 0.853764]\n",
            "4438 [D loss: 0.598351, acc.: 62.50%] [G loss: 0.852163]\n",
            "4439 [D loss: 0.599955, acc.: 62.50%] [G loss: 0.854252]\n",
            "4440 [D loss: 0.597912, acc.: 62.50%] [G loss: 0.854188]\n",
            "4441 [D loss: 0.597802, acc.: 62.50%] [G loss: 0.854170]\n",
            "4442 [D loss: 0.597734, acc.: 62.50%] [G loss: 0.854161]\n",
            "4443 [D loss: 0.597360, acc.: 62.50%] [G loss: 0.853672]\n",
            "4444 [D loss: 0.598133, acc.: 62.50%] [G loss: 0.852257]\n",
            "4445 [D loss: 0.598861, acc.: 62.50%] [G loss: 0.850349]\n",
            "4446 [D loss: 0.598038, acc.: 62.50%] [G loss: 0.854017]\n",
            "4447 [D loss: 0.597531, acc.: 62.50%] [G loss: 0.852823]\n",
            "4448 [D loss: 0.597637, acc.: 62.50%] [G loss: 0.853607]\n",
            "4449 [D loss: 0.597734, acc.: 62.50%] [G loss: 0.852944]\n",
            "4450 [D loss: 0.598069, acc.: 62.50%] [G loss: 0.852868]\n",
            "4451 [D loss: 0.597658, acc.: 62.50%] [G loss: 0.851486]\n",
            "4452 [D loss: 0.598173, acc.: 62.50%] [G loss: 0.853960]\n",
            "4453 [D loss: 0.597198, acc.: 62.50%] [G loss: 0.853355]\n",
            "4454 [D loss: 0.597703, acc.: 62.50%] [G loss: 0.853162]\n",
            "4455 [D loss: 0.597914, acc.: 62.50%] [G loss: 0.860573]\n",
            "4456 [D loss: 0.599071, acc.: 62.50%] [G loss: 0.882920]\n",
            "4457 [D loss: 0.605129, acc.: 62.50%] [G loss: 0.853023]\n",
            "4458 [D loss: 0.598745, acc.: 62.50%] [G loss: 0.853282]\n",
            "4459 [D loss: 0.598719, acc.: 62.50%] [G loss: 0.851827]\n",
            "4460 [D loss: 0.597815, acc.: 62.50%] [G loss: 0.851596]\n",
            "4461 [D loss: 0.597465, acc.: 62.50%] [G loss: 0.854279]\n",
            "4462 [D loss: 0.598112, acc.: 62.50%] [G loss: 0.854128]\n",
            "4463 [D loss: 0.598093, acc.: 62.50%] [G loss: 0.853705]\n",
            "4464 [D loss: 0.599456, acc.: 62.50%] [G loss: 0.850438]\n",
            "4465 [D loss: 0.596567, acc.: 62.50%] [G loss: 0.857958]\n",
            "4466 [D loss: 0.600014, acc.: 62.50%] [G loss: 0.855818]\n",
            "4467 [D loss: 0.598063, acc.: 62.50%] [G loss: 0.855386]\n",
            "4468 [D loss: 0.598788, acc.: 62.50%] [G loss: 0.851785]\n",
            "4469 [D loss: 0.598481, acc.: 62.50%] [G loss: 0.857412]\n",
            "4470 [D loss: 0.596931, acc.: 62.50%] [G loss: 0.856619]\n",
            "4471 [D loss: 0.595701, acc.: 62.50%] [G loss: 0.858810]\n",
            "4472 [D loss: 0.598719, acc.: 62.50%] [G loss: 0.857045]\n",
            "4473 [D loss: 0.598807, acc.: 62.50%] [G loss: 0.857346]\n",
            "4474 [D loss: 0.597069, acc.: 62.50%] [G loss: 0.858716]\n",
            "4475 [D loss: 0.598106, acc.: 62.50%] [G loss: 0.861627]\n",
            "4476 [D loss: 0.599710, acc.: 62.50%] [G loss: 0.863317]\n",
            "4477 [D loss: 0.603428, acc.: 62.50%] [G loss: 0.857246]\n",
            "4478 [D loss: 0.594292, acc.: 62.50%] [G loss: 1.171466]\n",
            "4479 [D loss: 0.715113, acc.: 62.50%] [G loss: 0.855744]\n",
            "4480 [D loss: 0.598583, acc.: 62.50%] [G loss: 0.857187]\n",
            "4481 [D loss: 0.600272, acc.: 62.50%] [G loss: 0.857123]\n",
            "4482 [D loss: 0.595084, acc.: 62.50%] [G loss: 0.855256]\n",
            "4483 [D loss: 0.596804, acc.: 62.50%] [G loss: 0.860232]\n",
            "4484 [D loss: 0.596456, acc.: 62.50%] [G loss: 0.883397]\n",
            "4485 [D loss: 0.603184, acc.: 62.50%] [G loss: 0.858435]\n",
            "4486 [D loss: 0.600135, acc.: 62.50%] [G loss: 0.858663]\n",
            "4487 [D loss: 0.595065, acc.: 62.50%] [G loss: 0.857745]\n",
            "4488 [D loss: 0.598630, acc.: 62.50%] [G loss: 0.862223]\n",
            "4489 [D loss: 0.599264, acc.: 62.50%] [G loss: 0.855043]\n",
            "4490 [D loss: 0.597189, acc.: 62.50%] [G loss: 0.854511]\n",
            "4491 [D loss: 0.598929, acc.: 62.50%] [G loss: 0.850655]\n",
            "4492 [D loss: 0.597562, acc.: 62.50%] [G loss: 0.857068]\n",
            "4493 [D loss: 0.598781, acc.: 62.50%] [G loss: 0.852595]\n",
            "4494 [D loss: 0.596439, acc.: 62.50%] [G loss: 0.852992]\n",
            "4495 [D loss: 0.597630, acc.: 62.50%] [G loss: 0.853108]\n",
            "4496 [D loss: 0.597046, acc.: 62.50%] [G loss: 0.850549]\n",
            "4497 [D loss: 0.597041, acc.: 62.50%] [G loss: 0.853151]\n",
            "4498 [D loss: 0.597386, acc.: 62.50%] [G loss: 0.849493]\n",
            "4499 [D loss: 0.597280, acc.: 62.50%] [G loss: 0.851489]\n",
            "4500 [D loss: 0.596359, acc.: 62.50%] [G loss: 0.852196]\n",
            "generated_data\n",
            "4501 [D loss: 0.599136, acc.: 62.50%] [G loss: 0.854015]\n",
            "4502 [D loss: 0.597701, acc.: 62.50%] [G loss: 0.854639]\n",
            "4503 [D loss: 0.596688, acc.: 62.50%] [G loss: 0.853683]\n",
            "4504 [D loss: 0.596536, acc.: 62.50%] [G loss: 0.854495]\n",
            "4505 [D loss: 0.597038, acc.: 62.50%] [G loss: 0.851188]\n",
            "4506 [D loss: 0.597626, acc.: 62.50%] [G loss: 0.850207]\n",
            "4507 [D loss: 0.597095, acc.: 62.50%] [G loss: 0.854481]\n",
            "4508 [D loss: 0.595969, acc.: 62.50%] [G loss: 0.855894]\n",
            "4509 [D loss: 0.598517, acc.: 62.50%] [G loss: 0.853323]\n",
            "4510 [D loss: 0.597746, acc.: 62.50%] [G loss: 0.852443]\n",
            "4511 [D loss: 0.596705, acc.: 62.50%] [G loss: 0.854956]\n",
            "4512 [D loss: 0.596647, acc.: 62.50%] [G loss: 0.854089]\n",
            "4513 [D loss: 0.594813, acc.: 62.50%] [G loss: 0.853287]\n",
            "4514 [D loss: 0.596842, acc.: 62.50%] [G loss: 0.854871]\n",
            "4515 [D loss: 0.598461, acc.: 62.50%] [G loss: 0.856024]\n",
            "4516 [D loss: 0.600330, acc.: 62.50%] [G loss: 0.854608]\n",
            "4517 [D loss: 0.596252, acc.: 62.50%] [G loss: 0.851489]\n",
            "4518 [D loss: 0.596059, acc.: 62.50%] [G loss: 0.857023]\n",
            "4519 [D loss: 0.593233, acc.: 62.50%] [G loss: 0.876702]\n",
            "4520 [D loss: 0.601933, acc.: 62.50%] [G loss: 0.866215]\n",
            "4521 [D loss: 0.598886, acc.: 62.50%] [G loss: 0.860222]\n",
            "4522 [D loss: 0.595769, acc.: 62.50%] [G loss: 0.852928]\n",
            "4523 [D loss: 0.596865, acc.: 62.50%] [G loss: 0.852298]\n",
            "4524 [D loss: 0.595595, acc.: 62.50%] [G loss: 0.852662]\n",
            "4525 [D loss: 0.597025, acc.: 62.50%] [G loss: 0.856899]\n",
            "4526 [D loss: 0.596258, acc.: 62.50%] [G loss: 0.854459]\n",
            "4527 [D loss: 0.596509, acc.: 62.50%] [G loss: 0.856385]\n",
            "4528 [D loss: 0.596038, acc.: 62.50%] [G loss: 0.853958]\n",
            "4529 [D loss: 0.597398, acc.: 62.50%] [G loss: 0.851986]\n",
            "4530 [D loss: 0.597058, acc.: 62.50%] [G loss: 0.856130]\n",
            "4531 [D loss: 0.597715, acc.: 62.50%] [G loss: 0.859102]\n",
            "4532 [D loss: 0.595247, acc.: 62.50%] [G loss: 0.904595]\n",
            "4533 [D loss: 0.613045, acc.: 62.50%] [G loss: 0.853189]\n",
            "4534 [D loss: 0.597071, acc.: 62.50%] [G loss: 0.851665]\n",
            "4535 [D loss: 0.597945, acc.: 62.50%] [G loss: 0.854687]\n",
            "4536 [D loss: 0.597705, acc.: 62.50%] [G loss: 0.851152]\n",
            "4537 [D loss: 0.596351, acc.: 62.50%] [G loss: 0.851367]\n",
            "4538 [D loss: 0.597986, acc.: 62.50%] [G loss: 0.849768]\n",
            "4539 [D loss: 0.596603, acc.: 62.50%] [G loss: 0.852714]\n",
            "4540 [D loss: 0.597144, acc.: 62.50%] [G loss: 0.850276]\n",
            "4541 [D loss: 0.596573, acc.: 62.50%] [G loss: 0.852295]\n",
            "4542 [D loss: 0.597078, acc.: 62.50%] [G loss: 0.851463]\n",
            "4543 [D loss: 0.597043, acc.: 62.50%] [G loss: 0.850690]\n",
            "4544 [D loss: 0.596545, acc.: 62.50%] [G loss: 0.854080]\n",
            "4545 [D loss: 0.597405, acc.: 62.50%] [G loss: 0.854580]\n",
            "4546 [D loss: 0.597153, acc.: 62.50%] [G loss: 0.850322]\n",
            "4547 [D loss: 0.597281, acc.: 62.50%] [G loss: 0.850423]\n",
            "4548 [D loss: 0.598179, acc.: 62.50%] [G loss: 0.851622]\n",
            "4549 [D loss: 0.597132, acc.: 62.50%] [G loss: 0.851750]\n",
            "4550 [D loss: 0.595175, acc.: 62.50%] [G loss: 0.853844]\n",
            "4551 [D loss: 0.597220, acc.: 62.50%] [G loss: 0.855285]\n",
            "4552 [D loss: 0.596165, acc.: 62.50%] [G loss: 0.852481]\n",
            "4553 [D loss: 0.598376, acc.: 62.50%] [G loss: 0.852693]\n",
            "4554 [D loss: 0.595514, acc.: 62.50%] [G loss: 0.851619]\n",
            "4555 [D loss: 0.598093, acc.: 62.50%] [G loss: 0.852701]\n",
            "4556 [D loss: 0.596909, acc.: 62.50%] [G loss: 0.853451]\n",
            "4557 [D loss: 0.595907, acc.: 62.50%] [G loss: 0.856014]\n",
            "4558 [D loss: 0.597060, acc.: 62.50%] [G loss: 0.853951]\n",
            "4559 [D loss: 0.598509, acc.: 62.50%] [G loss: 0.853410]\n",
            "4560 [D loss: 0.597232, acc.: 62.50%] [G loss: 0.853162]\n",
            "4561 [D loss: 0.597192, acc.: 62.50%] [G loss: 0.849797]\n",
            "4562 [D loss: 0.598030, acc.: 62.50%] [G loss: 0.849885]\n",
            "4563 [D loss: 0.596631, acc.: 62.50%] [G loss: 0.853245]\n",
            "4564 [D loss: 0.597427, acc.: 62.50%] [G loss: 0.851839]\n",
            "4565 [D loss: 0.597107, acc.: 62.50%] [G loss: 0.855071]\n",
            "4566 [D loss: 0.596788, acc.: 62.50%] [G loss: 0.853293]\n",
            "4567 [D loss: 0.597130, acc.: 62.50%] [G loss: 0.854098]\n",
            "4568 [D loss: 0.596623, acc.: 62.50%] [G loss: 0.851008]\n",
            "4569 [D loss: 0.597112, acc.: 62.50%] [G loss: 0.857192]\n",
            "4570 [D loss: 0.596137, acc.: 62.50%] [G loss: 0.853469]\n",
            "4571 [D loss: 0.597106, acc.: 62.50%] [G loss: 0.851843]\n",
            "4572 [D loss: 0.597952, acc.: 62.50%] [G loss: 0.854021]\n",
            "4573 [D loss: 0.598348, acc.: 62.50%] [G loss: 0.853869]\n",
            "4574 [D loss: 0.597634, acc.: 62.50%] [G loss: 0.858003]\n",
            "4575 [D loss: 0.596734, acc.: 62.50%] [G loss: 0.848594]\n",
            "4576 [D loss: 0.597090, acc.: 62.50%] [G loss: 0.851198]\n",
            "4577 [D loss: 0.596155, acc.: 62.50%] [G loss: 0.856653]\n",
            "4578 [D loss: 0.601227, acc.: 62.50%] [G loss: 0.848850]\n",
            "4579 [D loss: 0.597249, acc.: 62.50%] [G loss: 0.848774]\n",
            "4580 [D loss: 0.596407, acc.: 62.50%] [G loss: 0.850169]\n",
            "4581 [D loss: 0.595478, acc.: 62.50%] [G loss: 0.848230]\n",
            "4582 [D loss: 0.596095, acc.: 62.50%] [G loss: 0.854524]\n",
            "4583 [D loss: 0.598764, acc.: 62.50%] [G loss: 0.849763]\n",
            "4584 [D loss: 0.597360, acc.: 62.50%] [G loss: 0.853886]\n",
            "4585 [D loss: 0.596588, acc.: 62.50%] [G loss: 0.853249]\n",
            "4586 [D loss: 0.598557, acc.: 62.50%] [G loss: 0.848248]\n",
            "4587 [D loss: 0.597093, acc.: 62.50%] [G loss: 0.850275]\n",
            "4588 [D loss: 0.598466, acc.: 62.50%] [G loss: 0.847644]\n",
            "4589 [D loss: 0.595857, acc.: 62.50%] [G loss: 0.848272]\n",
            "4590 [D loss: 0.595076, acc.: 62.50%] [G loss: 0.850163]\n",
            "4591 [D loss: 0.596734, acc.: 62.50%] [G loss: 0.852504]\n",
            "4592 [D loss: 0.596616, acc.: 62.50%] [G loss: 0.848250]\n",
            "4593 [D loss: 0.597156, acc.: 62.50%] [G loss: 0.851306]\n",
            "4594 [D loss: 0.597197, acc.: 62.50%] [G loss: 0.849460]\n",
            "4595 [D loss: 0.596635, acc.: 62.50%] [G loss: 0.845707]\n",
            "4596 [D loss: 0.596588, acc.: 62.50%] [G loss: 0.849001]\n",
            "4597 [D loss: 0.596963, acc.: 62.50%] [G loss: 0.851337]\n",
            "4598 [D loss: 0.597546, acc.: 62.50%] [G loss: 0.847195]\n",
            "4599 [D loss: 0.598337, acc.: 62.50%] [G loss: 0.849346]\n",
            "4600 [D loss: 0.596507, acc.: 62.50%] [G loss: 0.849725]\n",
            "generated_data\n",
            "4601 [D loss: 0.597267, acc.: 62.50%] [G loss: 0.848282]\n",
            "4602 [D loss: 0.598335, acc.: 62.50%] [G loss: 0.848002]\n",
            "4603 [D loss: 0.598156, acc.: 62.50%] [G loss: 0.848128]\n",
            "4604 [D loss: 0.596192, acc.: 62.50%] [G loss: 0.846786]\n",
            "4605 [D loss: 0.598089, acc.: 62.50%] [G loss: 0.849646]\n",
            "4606 [D loss: 0.598265, acc.: 62.50%] [G loss: 0.849675]\n",
            "4607 [D loss: 0.597737, acc.: 62.50%] [G loss: 0.852530]\n",
            "4608 [D loss: 0.595059, acc.: 62.50%] [G loss: 0.850568]\n",
            "4609 [D loss: 0.595643, acc.: 62.50%] [G loss: 0.852418]\n",
            "4610 [D loss: 0.598031, acc.: 62.50%] [G loss: 0.853335]\n",
            "4611 [D loss: 0.598695, acc.: 62.50%] [G loss: 0.849992]\n",
            "4612 [D loss: 0.599134, acc.: 62.50%] [G loss: 0.847878]\n",
            "4613 [D loss: 0.599379, acc.: 62.50%] [G loss: 0.846881]\n",
            "4614 [D loss: 0.599672, acc.: 62.50%] [G loss: 0.845403]\n",
            "4615 [D loss: 0.598631, acc.: 62.50%] [G loss: 0.848700]\n",
            "4616 [D loss: 0.598868, acc.: 62.50%] [G loss: 0.848910]\n",
            "4617 [D loss: 0.598896, acc.: 62.50%] [G loss: 0.848484]\n",
            "4618 [D loss: 0.596834, acc.: 62.50%] [G loss: 0.845773]\n",
            "4619 [D loss: 0.599353, acc.: 62.50%] [G loss: 0.844450]\n",
            "4620 [D loss: 0.599893, acc.: 62.50%] [G loss: 0.844708]\n",
            "4621 [D loss: 0.597030, acc.: 62.50%] [G loss: 0.843998]\n",
            "4622 [D loss: 0.597531, acc.: 62.50%] [G loss: 0.844551]\n",
            "4623 [D loss: 0.596038, acc.: 62.50%] [G loss: 0.842790]\n",
            "4624 [D loss: 0.599752, acc.: 62.50%] [G loss: 0.848793]\n",
            "4625 [D loss: 0.595938, acc.: 62.50%] [G loss: 0.852796]\n",
            "4626 [D loss: 0.597079, acc.: 62.50%] [G loss: 0.849913]\n",
            "4627 [D loss: 0.595307, acc.: 62.50%] [G loss: 0.848274]\n",
            "4628 [D loss: 0.596538, acc.: 62.50%] [G loss: 0.852294]\n",
            "4629 [D loss: 0.597356, acc.: 62.50%] [G loss: 0.851097]\n",
            "4630 [D loss: 0.595906, acc.: 62.50%] [G loss: 0.840289]\n",
            "4631 [D loss: 0.596032, acc.: 62.50%] [G loss: 0.847356]\n",
            "4632 [D loss: 0.599403, acc.: 62.50%] [G loss: 0.841063]\n",
            "4633 [D loss: 0.598187, acc.: 62.50%] [G loss: 0.844157]\n",
            "4634 [D loss: 0.595982, acc.: 62.50%] [G loss: 0.844247]\n",
            "4635 [D loss: 0.596327, acc.: 62.50%] [G loss: 0.855112]\n",
            "4636 [D loss: 0.594355, acc.: 62.50%] [G loss: 0.859437]\n",
            "4637 [D loss: 0.599381, acc.: 62.50%] [G loss: 0.867106]\n",
            "4638 [D loss: 0.599882, acc.: 62.50%] [G loss: 0.855197]\n",
            "4639 [D loss: 0.598667, acc.: 62.50%] [G loss: 0.844891]\n",
            "4640 [D loss: 0.597257, acc.: 62.50%] [G loss: 0.845825]\n",
            "4641 [D loss: 0.598754, acc.: 62.50%] [G loss: 0.845917]\n",
            "4642 [D loss: 0.595346, acc.: 62.50%] [G loss: 0.853038]\n",
            "4643 [D loss: 0.598410, acc.: 62.50%] [G loss: 0.849165]\n",
            "4644 [D loss: 0.600403, acc.: 62.50%] [G loss: 0.839745]\n",
            "4645 [D loss: 0.611441, acc.: 59.38%] [G loss: 0.850198]\n",
            "4646 [D loss: 0.597424, acc.: 62.50%] [G loss: 0.845269]\n",
            "4647 [D loss: 0.595365, acc.: 62.50%] [G loss: 0.847095]\n",
            "4648 [D loss: 0.598976, acc.: 62.50%] [G loss: 0.846833]\n",
            "4649 [D loss: 0.596969, acc.: 62.50%] [G loss: 0.844709]\n",
            "4650 [D loss: 0.596665, acc.: 62.50%] [G loss: 0.849844]\n",
            "4651 [D loss: 0.597829, acc.: 62.50%] [G loss: 0.850153]\n",
            "4652 [D loss: 0.596201, acc.: 62.50%] [G loss: 0.843559]\n",
            "4653 [D loss: 0.596434, acc.: 62.50%] [G loss: 0.849145]\n",
            "4654 [D loss: 0.598706, acc.: 62.50%] [G loss: 0.842539]\n",
            "4655 [D loss: 0.598483, acc.: 62.50%] [G loss: 0.844109]\n",
            "4656 [D loss: 0.599973, acc.: 62.50%] [G loss: 0.846883]\n",
            "4657 [D loss: 0.597828, acc.: 62.50%] [G loss: 0.851753]\n",
            "4658 [D loss: 0.597131, acc.: 62.50%] [G loss: 0.843955]\n",
            "4659 [D loss: 0.597615, acc.: 62.50%] [G loss: 0.847953]\n",
            "4660 [D loss: 0.597074, acc.: 62.50%] [G loss: 0.848449]\n",
            "4661 [D loss: 0.598436, acc.: 62.50%] [G loss: 0.849500]\n",
            "4662 [D loss: 0.597039, acc.: 62.50%] [G loss: 0.844249]\n",
            "4663 [D loss: 0.598714, acc.: 62.50%] [G loss: 0.848105]\n",
            "4664 [D loss: 0.597145, acc.: 62.50%] [G loss: 0.849491]\n",
            "4665 [D loss: 0.597214, acc.: 62.50%] [G loss: 0.847131]\n",
            "4666 [D loss: 0.597225, acc.: 62.50%] [G loss: 0.848994]\n",
            "4667 [D loss: 0.596573, acc.: 62.50%] [G loss: 0.846957]\n",
            "4668 [D loss: 0.596430, acc.: 62.50%] [G loss: 0.851135]\n",
            "4669 [D loss: 0.599118, acc.: 62.50%] [G loss: 0.848668]\n",
            "4670 [D loss: 0.597192, acc.: 62.50%] [G loss: 0.849258]\n",
            "4671 [D loss: 0.596906, acc.: 62.50%] [G loss: 0.849341]\n",
            "4672 [D loss: 0.597102, acc.: 62.50%] [G loss: 0.845185]\n",
            "4673 [D loss: 0.597746, acc.: 62.50%] [G loss: 0.847907]\n",
            "4674 [D loss: 0.595896, acc.: 62.50%] [G loss: 0.851570]\n",
            "4675 [D loss: 0.597659, acc.: 62.50%] [G loss: 0.845031]\n",
            "4676 [D loss: 0.596125, acc.: 62.50%] [G loss: 0.844172]\n",
            "4677 [D loss: 0.595475, acc.: 62.50%] [G loss: 0.848586]\n",
            "4678 [D loss: 0.596965, acc.: 62.50%] [G loss: 0.848426]\n",
            "4679 [D loss: 0.599482, acc.: 62.50%] [G loss: 0.851158]\n",
            "4680 [D loss: 0.596970, acc.: 62.50%] [G loss: 0.847235]\n",
            "4681 [D loss: 0.597696, acc.: 62.50%] [G loss: 0.844501]\n",
            "4682 [D loss: 0.594246, acc.: 62.50%] [G loss: 0.849458]\n",
            "4683 [D loss: 0.598298, acc.: 62.50%] [G loss: 0.849163]\n",
            "4684 [D loss: 0.597630, acc.: 62.50%] [G loss: 0.850436]\n",
            "4685 [D loss: 0.595352, acc.: 62.50%] [G loss: 0.846024]\n",
            "4686 [D loss: 0.595829, acc.: 62.50%] [G loss: 0.846219]\n",
            "4687 [D loss: 0.599216, acc.: 62.50%] [G loss: 0.834055]\n",
            "4688 [D loss: 0.601800, acc.: 62.50%] [G loss: 0.849911]\n",
            "4689 [D loss: 0.596314, acc.: 62.50%] [G loss: 0.848176]\n",
            "4690 [D loss: 0.596940, acc.: 62.50%] [G loss: 0.850634]\n",
            "4691 [D loss: 0.596236, acc.: 62.50%] [G loss: 0.848289]\n",
            "4692 [D loss: 0.596954, acc.: 62.50%] [G loss: 0.850369]\n",
            "4693 [D loss: 0.597432, acc.: 62.50%] [G loss: 0.848194]\n",
            "4694 [D loss: 0.598620, acc.: 62.50%] [G loss: 0.859571]\n",
            "4695 [D loss: 0.597195, acc.: 62.50%] [G loss: 0.865580]\n",
            "4696 [D loss: 0.598596, acc.: 62.50%] [G loss: 0.851931]\n",
            "4697 [D loss: 0.598121, acc.: 62.50%] [G loss: 0.854938]\n",
            "4698 [D loss: 0.595838, acc.: 62.50%] [G loss: 0.886325]\n",
            "4699 [D loss: 0.607449, acc.: 62.50%] [G loss: 0.852811]\n",
            "4700 [D loss: 0.597454, acc.: 62.50%] [G loss: 0.849505]\n",
            "generated_data\n",
            "4701 [D loss: 0.598971, acc.: 62.50%] [G loss: 0.851395]\n",
            "4702 [D loss: 0.596449, acc.: 62.50%] [G loss: 0.846943]\n",
            "4703 [D loss: 0.599695, acc.: 62.50%] [G loss: 0.849297]\n",
            "4704 [D loss: 0.597284, acc.: 62.50%] [G loss: 0.849519]\n",
            "4705 [D loss: 0.597678, acc.: 62.50%] [G loss: 0.852369]\n",
            "4706 [D loss: 0.595936, acc.: 62.50%] [G loss: 0.853927]\n",
            "4707 [D loss: 0.598208, acc.: 62.50%] [G loss: 0.853067]\n",
            "4708 [D loss: 0.597113, acc.: 62.50%] [G loss: 0.849809]\n",
            "4709 [D loss: 0.597562, acc.: 62.50%] [G loss: 0.851568]\n",
            "4710 [D loss: 0.595963, acc.: 62.50%] [G loss: 0.851219]\n",
            "4711 [D loss: 0.597049, acc.: 62.50%] [G loss: 0.850650]\n",
            "4712 [D loss: 0.597080, acc.: 62.50%] [G loss: 0.848090]\n",
            "4713 [D loss: 0.596105, acc.: 62.50%] [G loss: 0.848069]\n",
            "4714 [D loss: 0.594765, acc.: 62.50%] [G loss: 0.849609]\n",
            "4715 [D loss: 0.597406, acc.: 62.50%] [G loss: 0.855372]\n",
            "4716 [D loss: 0.598508, acc.: 62.50%] [G loss: 0.849544]\n",
            "4717 [D loss: 0.597964, acc.: 62.50%] [G loss: 0.847766]\n",
            "4718 [D loss: 0.598521, acc.: 62.50%] [G loss: 0.845694]\n",
            "4719 [D loss: 0.597360, acc.: 62.50%] [G loss: 0.853618]\n",
            "4720 [D loss: 0.597421, acc.: 62.50%] [G loss: 0.853651]\n",
            "4721 [D loss: 0.597612, acc.: 62.50%] [G loss: 0.845661]\n",
            "4722 [D loss: 0.596677, acc.: 62.50%] [G loss: 0.847389]\n",
            "4723 [D loss: 0.596371, acc.: 62.50%] [G loss: 0.850185]\n",
            "4724 [D loss: 0.596363, acc.: 62.50%] [G loss: 0.852610]\n",
            "4725 [D loss: 0.596861, acc.: 62.50%] [G loss: 0.888925]\n",
            "4726 [D loss: 0.607086, acc.: 62.50%] [G loss: 0.851107]\n",
            "4727 [D loss: 0.596899, acc.: 62.50%] [G loss: 0.852776]\n",
            "4728 [D loss: 0.597089, acc.: 62.50%] [G loss: 0.847165]\n",
            "4729 [D loss: 0.595994, acc.: 62.50%] [G loss: 0.845905]\n",
            "4730 [D loss: 0.597865, acc.: 62.50%] [G loss: 0.850035]\n",
            "4731 [D loss: 0.595768, acc.: 62.50%] [G loss: 0.849264]\n",
            "4732 [D loss: 0.598321, acc.: 62.50%] [G loss: 0.851267]\n",
            "4733 [D loss: 0.597638, acc.: 62.50%] [G loss: 0.849480]\n",
            "4734 [D loss: 0.596522, acc.: 62.50%] [G loss: 0.848325]\n",
            "4735 [D loss: 0.597602, acc.: 62.50%] [G loss: 0.848756]\n",
            "4736 [D loss: 0.597011, acc.: 62.50%] [G loss: 0.847251]\n",
            "4737 [D loss: 0.598980, acc.: 62.50%] [G loss: 0.848524]\n",
            "4738 [D loss: 0.599269, acc.: 62.50%] [G loss: 0.845966]\n",
            "4739 [D loss: 0.596402, acc.: 62.50%] [G loss: 0.847086]\n",
            "4740 [D loss: 0.598216, acc.: 62.50%] [G loss: 0.848558]\n",
            "4741 [D loss: 0.596686, acc.: 62.50%] [G loss: 0.847102]\n",
            "4742 [D loss: 0.596176, acc.: 62.50%] [G loss: 0.849323]\n",
            "4743 [D loss: 0.596742, acc.: 62.50%] [G loss: 0.843386]\n",
            "4744 [D loss: 0.598351, acc.: 62.50%] [G loss: 0.845719]\n",
            "4745 [D loss: 0.598396, acc.: 62.50%] [G loss: 0.851099]\n",
            "4746 [D loss: 0.598201, acc.: 62.50%] [G loss: 0.849495]\n",
            "4747 [D loss: 0.598818, acc.: 62.50%] [G loss: 0.848170]\n",
            "4748 [D loss: 0.597385, acc.: 62.50%] [G loss: 0.846521]\n",
            "4749 [D loss: 0.594883, acc.: 62.50%] [G loss: 0.848662]\n",
            "4750 [D loss: 0.598072, acc.: 62.50%] [G loss: 0.845119]\n",
            "4751 [D loss: 0.596519, acc.: 62.50%] [G loss: 0.848232]\n",
            "4752 [D loss: 0.597331, acc.: 62.50%] [G loss: 0.849412]\n",
            "4753 [D loss: 0.597885, acc.: 62.50%] [G loss: 0.847943]\n",
            "4754 [D loss: 0.598370, acc.: 62.50%] [G loss: 0.851897]\n",
            "4755 [D loss: 0.598513, acc.: 62.50%] [G loss: 0.847873]\n",
            "4756 [D loss: 0.597912, acc.: 62.50%] [G loss: 0.849717]\n",
            "4757 [D loss: 0.598282, acc.: 62.50%] [G loss: 0.852394]\n",
            "4758 [D loss: 0.596653, acc.: 62.50%] [G loss: 0.855383]\n",
            "4759 [D loss: 0.597021, acc.: 62.50%] [G loss: 0.850905]\n",
            "4760 [D loss: 0.599145, acc.: 62.50%] [G loss: 0.845882]\n",
            "4761 [D loss: 0.598021, acc.: 62.50%] [G loss: 0.846176]\n",
            "4762 [D loss: 0.596669, acc.: 62.50%] [G loss: 0.849248]\n",
            "4763 [D loss: 0.598249, acc.: 62.50%] [G loss: 0.848617]\n",
            "4764 [D loss: 0.597114, acc.: 62.50%] [G loss: 0.845874]\n",
            "4765 [D loss: 0.596676, acc.: 62.50%] [G loss: 0.846817]\n",
            "4766 [D loss: 0.597420, acc.: 62.50%] [G loss: 0.842773]\n",
            "4767 [D loss: 0.596190, acc.: 62.50%] [G loss: 0.852781]\n",
            "4768 [D loss: 0.596644, acc.: 62.50%] [G loss: 0.844167]\n",
            "4769 [D loss: 0.597291, acc.: 62.50%] [G loss: 0.843695]\n",
            "4770 [D loss: 0.597863, acc.: 62.50%] [G loss: 0.847310]\n",
            "4771 [D loss: 0.597353, acc.: 62.50%] [G loss: 0.846825]\n",
            "4772 [D loss: 0.596346, acc.: 62.50%] [G loss: 0.850434]\n",
            "4773 [D loss: 0.596194, acc.: 62.50%] [G loss: 0.845115]\n",
            "4774 [D loss: 0.595525, acc.: 62.50%] [G loss: 0.847434]\n",
            "4775 [D loss: 0.597917, acc.: 62.50%] [G loss: 0.840976]\n",
            "4776 [D loss: 0.598650, acc.: 62.50%] [G loss: 0.844257]\n",
            "4777 [D loss: 0.596839, acc.: 62.50%] [G loss: 0.843423]\n",
            "4778 [D loss: 0.598290, acc.: 62.50%] [G loss: 0.845766]\n",
            "4779 [D loss: 0.597060, acc.: 62.50%] [G loss: 0.845886]\n",
            "4780 [D loss: 0.594917, acc.: 62.50%] [G loss: 0.847130]\n",
            "4781 [D loss: 0.596628, acc.: 62.50%] [G loss: 0.846681]\n",
            "4782 [D loss: 0.595535, acc.: 62.50%] [G loss: 0.849179]\n",
            "4783 [D loss: 0.596976, acc.: 62.50%] [G loss: 0.844218]\n",
            "4784 [D loss: 0.598911, acc.: 62.50%] [G loss: 0.847131]\n",
            "4785 [D loss: 0.597241, acc.: 62.50%] [G loss: 0.842906]\n",
            "4786 [D loss: 0.597469, acc.: 62.50%] [G loss: 0.846790]\n",
            "4787 [D loss: 0.595911, acc.: 62.50%] [G loss: 0.844457]\n",
            "4788 [D loss: 0.596606, acc.: 62.50%] [G loss: 0.852906]\n",
            "4789 [D loss: 0.597115, acc.: 62.50%] [G loss: 0.855486]\n",
            "4790 [D loss: 0.596313, acc.: 62.50%] [G loss: 0.848539]\n",
            "4791 [D loss: 0.595942, acc.: 62.50%] [G loss: 0.849003]\n",
            "4792 [D loss: 0.596269, acc.: 62.50%] [G loss: 0.845086]\n",
            "4793 [D loss: 0.599317, acc.: 62.50%] [G loss: 0.847542]\n",
            "4794 [D loss: 0.597325, acc.: 62.50%] [G loss: 0.844655]\n",
            "4795 [D loss: 0.597384, acc.: 62.50%] [G loss: 0.846317]\n",
            "4796 [D loss: 0.596256, acc.: 62.50%] [G loss: 0.845478]\n",
            "4797 [D loss: 0.595049, acc.: 62.50%] [G loss: 0.841937]\n",
            "4798 [D loss: 0.596155, acc.: 62.50%] [G loss: 0.848477]\n",
            "4799 [D loss: 0.597217, acc.: 62.50%] [G loss: 0.845855]\n",
            "4800 [D loss: 0.597335, acc.: 62.50%] [G loss: 0.843821]\n",
            "generated_data\n",
            "4801 [D loss: 0.595125, acc.: 62.50%] [G loss: 0.846664]\n",
            "4802 [D loss: 0.594665, acc.: 62.50%] [G loss: 0.842527]\n",
            "4803 [D loss: 0.598423, acc.: 62.50%] [G loss: 0.843259]\n",
            "4804 [D loss: 0.596373, acc.: 62.50%] [G loss: 0.851675]\n",
            "4805 [D loss: 0.596431, acc.: 62.50%] [G loss: 0.849494]\n",
            "4806 [D loss: 0.600330, acc.: 62.50%] [G loss: 0.847838]\n",
            "4807 [D loss: 0.598179, acc.: 62.50%] [G loss: 0.843175]\n",
            "4808 [D loss: 0.593986, acc.: 62.50%] [G loss: 0.837603]\n",
            "4809 [D loss: 0.595215, acc.: 62.50%] [G loss: 0.850329]\n",
            "4810 [D loss: 0.596066, acc.: 62.50%] [G loss: 0.841934]\n",
            "4811 [D loss: 0.597886, acc.: 62.50%] [G loss: 0.847132]\n",
            "4812 [D loss: 0.595883, acc.: 62.50%] [G loss: 0.844445]\n",
            "4813 [D loss: 0.599188, acc.: 62.50%] [G loss: 0.842244]\n",
            "4814 [D loss: 0.598901, acc.: 62.50%] [G loss: 0.840539]\n",
            "4815 [D loss: 0.597770, acc.: 62.50%] [G loss: 0.842090]\n",
            "4816 [D loss: 0.596451, acc.: 62.50%] [G loss: 0.846062]\n",
            "4817 [D loss: 0.595666, acc.: 62.50%] [G loss: 0.849430]\n",
            "4818 [D loss: 0.596018, acc.: 62.50%] [G loss: 0.849407]\n",
            "4819 [D loss: 0.598676, acc.: 62.50%] [G loss: 0.852562]\n",
            "4820 [D loss: 0.597805, acc.: 62.50%] [G loss: 0.848571]\n",
            "4821 [D loss: 0.596481, acc.: 62.50%] [G loss: 0.847417]\n",
            "4822 [D loss: 0.596265, acc.: 62.50%] [G loss: 0.847469]\n",
            "4823 [D loss: 0.595362, acc.: 62.50%] [G loss: 0.846798]\n",
            "4824 [D loss: 0.596083, acc.: 62.50%] [G loss: 0.852463]\n",
            "4825 [D loss: 0.597077, acc.: 62.50%] [G loss: 0.852869]\n",
            "4826 [D loss: 0.595387, acc.: 62.50%] [G loss: 0.850079]\n",
            "4827 [D loss: 0.598561, acc.: 62.50%] [G loss: 0.841109]\n",
            "4828 [D loss: 0.595101, acc.: 62.50%] [G loss: 0.844294]\n",
            "4829 [D loss: 0.594417, acc.: 62.50%] [G loss: 0.854395]\n",
            "4830 [D loss: 0.598815, acc.: 62.50%] [G loss: 0.845350]\n",
            "4831 [D loss: 0.599547, acc.: 62.50%] [G loss: 0.844366]\n",
            "4832 [D loss: 0.596286, acc.: 62.50%] [G loss: 0.844211]\n",
            "4833 [D loss: 0.596644, acc.: 62.50%] [G loss: 0.848455]\n",
            "4834 [D loss: 0.599675, acc.: 62.50%] [G loss: 0.840586]\n",
            "4835 [D loss: 0.598735, acc.: 62.50%] [G loss: 0.847435]\n",
            "4836 [D loss: 0.598626, acc.: 62.50%] [G loss: 0.845325]\n",
            "4837 [D loss: 0.597246, acc.: 62.50%] [G loss: 0.844311]\n",
            "4838 [D loss: 0.599144, acc.: 62.50%] [G loss: 0.845505]\n",
            "4839 [D loss: 0.597406, acc.: 62.50%] [G loss: 0.847855]\n",
            "4840 [D loss: 0.594905, acc.: 62.50%] [G loss: 0.846625]\n",
            "4841 [D loss: 0.596220, acc.: 62.50%] [G loss: 0.851233]\n",
            "4842 [D loss: 0.597160, acc.: 62.50%] [G loss: 0.850488]\n",
            "4843 [D loss: 0.597279, acc.: 62.50%] [G loss: 0.849750]\n",
            "4844 [D loss: 0.597513, acc.: 62.50%] [G loss: 0.851298]\n",
            "4845 [D loss: 0.597123, acc.: 62.50%] [G loss: 0.852745]\n",
            "4846 [D loss: 0.596806, acc.: 62.50%] [G loss: 0.851047]\n",
            "4847 [D loss: 0.598046, acc.: 62.50%] [G loss: 0.850349]\n",
            "4848 [D loss: 0.598055, acc.: 62.50%] [G loss: 0.848261]\n",
            "4849 [D loss: 0.597638, acc.: 62.50%] [G loss: 0.852073]\n",
            "4850 [D loss: 0.597746, acc.: 62.50%] [G loss: 0.850376]\n",
            "4851 [D loss: 0.596408, acc.: 62.50%] [G loss: 0.854505]\n",
            "4852 [D loss: 0.597327, acc.: 62.50%] [G loss: 0.851420]\n",
            "4853 [D loss: 0.596681, acc.: 62.50%] [G loss: 0.849677]\n",
            "4854 [D loss: 0.597936, acc.: 62.50%] [G loss: 0.852792]\n",
            "4855 [D loss: 0.597648, acc.: 62.50%] [G loss: 0.850148]\n",
            "4856 [D loss: 0.596307, acc.: 62.50%] [G loss: 0.849018]\n",
            "4857 [D loss: 0.596921, acc.: 62.50%] [G loss: 0.850418]\n",
            "4858 [D loss: 0.597848, acc.: 62.50%] [G loss: 0.853330]\n",
            "4859 [D loss: 0.597276, acc.: 62.50%] [G loss: 0.853403]\n",
            "4860 [D loss: 0.595480, acc.: 62.50%] [G loss: 0.851812]\n",
            "4861 [D loss: 0.596847, acc.: 62.50%] [G loss: 0.853441]\n",
            "4862 [D loss: 0.595217, acc.: 62.50%] [G loss: 0.858599]\n",
            "4863 [D loss: 0.596118, acc.: 62.50%] [G loss: 0.853644]\n",
            "4864 [D loss: 0.598796, acc.: 62.50%] [G loss: 0.851311]\n",
            "4865 [D loss: 0.598052, acc.: 62.50%] [G loss: 0.853141]\n",
            "4866 [D loss: 0.596855, acc.: 62.50%] [G loss: 0.853089]\n",
            "4867 [D loss: 0.597245, acc.: 62.50%] [G loss: 0.852736]\n",
            "4868 [D loss: 0.598106, acc.: 62.50%] [G loss: 0.852103]\n",
            "4869 [D loss: 0.595504, acc.: 62.50%] [G loss: 0.852310]\n",
            "4870 [D loss: 0.596909, acc.: 62.50%] [G loss: 0.850521]\n",
            "4871 [D loss: 0.595349, acc.: 62.50%] [G loss: 0.850731]\n",
            "4872 [D loss: 0.595903, acc.: 62.50%] [G loss: 0.853566]\n",
            "4873 [D loss: 0.595412, acc.: 62.50%] [G loss: 0.857408]\n",
            "4874 [D loss: 0.596266, acc.: 62.50%] [G loss: 0.857793]\n",
            "4875 [D loss: 0.599249, acc.: 62.50%] [G loss: 0.854819]\n",
            "4876 [D loss: 0.598480, acc.: 62.50%] [G loss: 0.852930]\n",
            "4877 [D loss: 0.596294, acc.: 62.50%] [G loss: 0.852876]\n",
            "4878 [D loss: 0.594893, acc.: 62.50%] [G loss: 0.849770]\n",
            "4879 [D loss: 0.596429, acc.: 62.50%] [G loss: 0.854276]\n",
            "4880 [D loss: 0.595524, acc.: 62.50%] [G loss: 0.850501]\n",
            "4881 [D loss: 0.597738, acc.: 62.50%] [G loss: 0.849262]\n",
            "4882 [D loss: 0.595739, acc.: 62.50%] [G loss: 0.853602]\n",
            "4883 [D loss: 0.595675, acc.: 62.50%] [G loss: 0.851068]\n",
            "4884 [D loss: 0.597197, acc.: 62.50%] [G loss: 0.850542]\n",
            "4885 [D loss: 0.598005, acc.: 62.50%] [G loss: 0.849355]\n",
            "4886 [D loss: 0.596293, acc.: 62.50%] [G loss: 0.851679]\n",
            "4887 [D loss: 0.597560, acc.: 62.50%] [G loss: 0.846425]\n",
            "4888 [D loss: 0.597029, acc.: 62.50%] [G loss: 0.850117]\n",
            "4889 [D loss: 0.598471, acc.: 62.50%] [G loss: 0.850538]\n",
            "4890 [D loss: 0.597142, acc.: 62.50%] [G loss: 0.851211]\n",
            "4891 [D loss: 0.596647, acc.: 62.50%] [G loss: 0.851981]\n",
            "4892 [D loss: 0.597956, acc.: 62.50%] [G loss: 0.850037]\n",
            "4893 [D loss: 0.596615, acc.: 62.50%] [G loss: 0.882421]\n",
            "4894 [D loss: 0.608177, acc.: 62.50%] [G loss: 0.854072]\n",
            "4895 [D loss: 0.596420, acc.: 62.50%] [G loss: 0.851449]\n",
            "4896 [D loss: 0.600020, acc.: 62.50%] [G loss: 0.853553]\n",
            "4897 [D loss: 0.598129, acc.: 62.50%] [G loss: 0.849710]\n",
            "4898 [D loss: 0.596678, acc.: 62.50%] [G loss: 0.847505]\n",
            "4899 [D loss: 0.596511, acc.: 62.50%] [G loss: 0.850082]\n",
            "4900 [D loss: 0.597774, acc.: 62.50%] [G loss: 0.850158]\n",
            "generated_data\n",
            "4901 [D loss: 0.595752, acc.: 62.50%] [G loss: 0.849002]\n",
            "4902 [D loss: 0.596547, acc.: 62.50%] [G loss: 0.853624]\n",
            "4903 [D loss: 0.597492, acc.: 62.50%] [G loss: 0.849660]\n",
            "4904 [D loss: 0.597549, acc.: 62.50%] [G loss: 0.850823]\n",
            "4905 [D loss: 0.597673, acc.: 62.50%] [G loss: 0.852670]\n",
            "4906 [D loss: 0.598124, acc.: 62.50%] [G loss: 0.851795]\n",
            "4907 [D loss: 0.596920, acc.: 62.50%] [G loss: 0.852841]\n",
            "4908 [D loss: 0.598070, acc.: 62.50%] [G loss: 0.852429]\n",
            "4909 [D loss: 0.598164, acc.: 62.50%] [G loss: 0.849254]\n",
            "4910 [D loss: 0.596720, acc.: 62.50%] [G loss: 0.849389]\n",
            "4911 [D loss: 0.597499, acc.: 62.50%] [G loss: 0.851180]\n",
            "4912 [D loss: 0.597489, acc.: 62.50%] [G loss: 0.852287]\n",
            "4913 [D loss: 0.598018, acc.: 62.50%] [G loss: 0.852772]\n",
            "4914 [D loss: 0.595820, acc.: 62.50%] [G loss: 0.854205]\n",
            "4915 [D loss: 0.596944, acc.: 62.50%] [G loss: 0.850228]\n",
            "4916 [D loss: 0.597264, acc.: 62.50%] [G loss: 0.849518]\n",
            "4917 [D loss: 0.595605, acc.: 62.50%] [G loss: 0.850988]\n",
            "4918 [D loss: 0.596915, acc.: 62.50%] [G loss: 0.852513]\n",
            "4919 [D loss: 0.596651, acc.: 62.50%] [G loss: 0.850920]\n",
            "4920 [D loss: 0.597429, acc.: 62.50%] [G loss: 0.850425]\n",
            "4921 [D loss: 0.596615, acc.: 62.50%] [G loss: 0.851045]\n",
            "4922 [D loss: 0.597749, acc.: 62.50%] [G loss: 0.847987]\n",
            "4923 [D loss: 0.597457, acc.: 62.50%] [G loss: 0.848176]\n",
            "4924 [D loss: 0.596325, acc.: 62.50%] [G loss: 0.846138]\n",
            "4925 [D loss: 0.596570, acc.: 62.50%] [G loss: 0.847638]\n",
            "4926 [D loss: 0.598323, acc.: 62.50%] [G loss: 0.848979]\n",
            "4927 [D loss: 0.597023, acc.: 62.50%] [G loss: 0.846864]\n",
            "4928 [D loss: 0.597073, acc.: 62.50%] [G loss: 0.847975]\n",
            "4929 [D loss: 0.596777, acc.: 62.50%] [G loss: 0.849374]\n",
            "4930 [D loss: 0.596482, acc.: 62.50%] [G loss: 0.844889]\n",
            "4931 [D loss: 0.597156, acc.: 62.50%] [G loss: 0.850945]\n",
            "4932 [D loss: 0.599772, acc.: 62.50%] [G loss: 0.846735]\n",
            "4933 [D loss: 0.597314, acc.: 62.50%] [G loss: 0.847096]\n",
            "4934 [D loss: 0.596868, acc.: 62.50%] [G loss: 0.845460]\n",
            "4935 [D loss: 0.596285, acc.: 62.50%] [G loss: 0.849373]\n",
            "4936 [D loss: 0.597623, acc.: 62.50%] [G loss: 0.846343]\n",
            "4937 [D loss: 0.598335, acc.: 62.50%] [G loss: 0.848634]\n",
            "4938 [D loss: 0.597402, acc.: 62.50%] [G loss: 0.844991]\n",
            "4939 [D loss: 0.597794, acc.: 62.50%] [G loss: 0.848766]\n",
            "4940 [D loss: 0.596264, acc.: 62.50%] [G loss: 0.848709]\n",
            "4941 [D loss: 0.596329, acc.: 62.50%] [G loss: 0.848484]\n",
            "4942 [D loss: 0.597448, acc.: 62.50%] [G loss: 0.852716]\n",
            "4943 [D loss: 0.596660, acc.: 62.50%] [G loss: 0.850314]\n",
            "4944 [D loss: 0.598663, acc.: 62.50%] [G loss: 0.846474]\n",
            "4945 [D loss: 0.597016, acc.: 62.50%] [G loss: 0.855629]\n",
            "4946 [D loss: 0.597197, acc.: 62.50%] [G loss: 0.847114]\n",
            "4947 [D loss: 0.597117, acc.: 62.50%] [G loss: 0.849982]\n",
            "4948 [D loss: 0.597993, acc.: 62.50%] [G loss: 0.848739]\n",
            "4949 [D loss: 0.595757, acc.: 62.50%] [G loss: 0.849313]\n",
            "4950 [D loss: 0.597594, acc.: 62.50%] [G loss: 0.849420]\n",
            "4951 [D loss: 0.597843, acc.: 62.50%] [G loss: 0.848922]\n",
            "4952 [D loss: 0.597010, acc.: 62.50%] [G loss: 0.846906]\n",
            "4953 [D loss: 0.597131, acc.: 62.50%] [G loss: 0.845970]\n",
            "4954 [D loss: 0.597931, acc.: 62.50%] [G loss: 0.848508]\n",
            "4955 [D loss: 0.596816, acc.: 62.50%] [G loss: 0.848539]\n",
            "4956 [D loss: 0.596917, acc.: 62.50%] [G loss: 0.848549]\n",
            "4957 [D loss: 0.597659, acc.: 62.50%] [G loss: 0.850527]\n",
            "4958 [D loss: 0.595935, acc.: 62.50%] [G loss: 0.850655]\n",
            "4959 [D loss: 0.596469, acc.: 62.50%] [G loss: 0.849783]\n",
            "4960 [D loss: 0.598405, acc.: 62.50%] [G loss: 0.853089]\n",
            "4961 [D loss: 0.596633, acc.: 62.50%] [G loss: 0.851102]\n",
            "4962 [D loss: 0.596669, acc.: 62.50%] [G loss: 0.847631]\n",
            "4963 [D loss: 0.596517, acc.: 62.50%] [G loss: 0.850774]\n",
            "4964 [D loss: 0.598386, acc.: 62.50%] [G loss: 0.850180]\n",
            "4965 [D loss: 0.596970, acc.: 62.50%] [G loss: 0.848924]\n",
            "4966 [D loss: 0.596936, acc.: 62.50%] [G loss: 0.850680]\n",
            "4967 [D loss: 0.595832, acc.: 62.50%] [G loss: 0.852170]\n",
            "4968 [D loss: 0.596338, acc.: 62.50%] [G loss: 0.856586]\n",
            "4969 [D loss: 0.593996, acc.: 62.50%] [G loss: 0.864137]\n",
            "4970 [D loss: 0.594196, acc.: 62.50%] [G loss: 0.892943]\n",
            "4971 [D loss: 0.597841, acc.: 62.50%] [G loss: 0.863184]\n",
            "4972 [D loss: 0.595786, acc.: 62.50%] [G loss: 0.856933]\n",
            "4973 [D loss: 0.596058, acc.: 62.50%] [G loss: 0.859129]\n",
            "4974 [D loss: 0.596511, acc.: 62.50%] [G loss: 0.858466]\n",
            "4975 [D loss: 0.597173, acc.: 62.50%] [G loss: 0.852491]\n",
            "4976 [D loss: 0.598007, acc.: 62.50%] [G loss: 0.850841]\n",
            "4977 [D loss: 0.599082, acc.: 62.50%] [G loss: 0.852638]\n",
            "4978 [D loss: 0.597459, acc.: 62.50%] [G loss: 0.850777]\n",
            "4979 [D loss: 0.594162, acc.: 62.50%] [G loss: 0.871673]\n",
            "4980 [D loss: 0.629854, acc.: 51.56%] [G loss: 0.851665]\n",
            "4981 [D loss: 0.596020, acc.: 62.50%] [G loss: 0.850448]\n",
            "4982 [D loss: 0.595655, acc.: 62.50%] [G loss: 0.850714]\n",
            "4983 [D loss: 0.595779, acc.: 62.50%] [G loss: 0.863515]\n",
            "4984 [D loss: 0.596833, acc.: 62.50%] [G loss: 0.859908]\n",
            "4985 [D loss: 0.597691, acc.: 62.50%] [G loss: 0.854094]\n",
            "4986 [D loss: 0.597503, acc.: 62.50%] [G loss: 0.852562]\n",
            "4987 [D loss: 0.597800, acc.: 62.50%] [G loss: 0.855031]\n",
            "4988 [D loss: 0.595725, acc.: 62.50%] [G loss: 0.848769]\n",
            "4989 [D loss: 0.597496, acc.: 62.50%] [G loss: 0.857053]\n",
            "4990 [D loss: 0.597414, acc.: 62.50%] [G loss: 0.851078]\n",
            "4991 [D loss: 0.597658, acc.: 62.50%] [G loss: 0.848831]\n",
            "4992 [D loss: 0.597451, acc.: 62.50%] [G loss: 0.854938]\n",
            "4993 [D loss: 0.597171, acc.: 62.50%] [G loss: 0.853016]\n",
            "4994 [D loss: 0.596442, acc.: 62.50%] [G loss: 0.852532]\n",
            "4995 [D loss: 0.597322, acc.: 62.50%] [G loss: 0.854640]\n",
            "4996 [D loss: 0.595954, acc.: 62.50%] [G loss: 0.850433]\n",
            "4997 [D loss: 0.596669, acc.: 62.50%] [G loss: 0.854010]\n",
            "4998 [D loss: 0.596067, acc.: 62.50%] [G loss: 0.851061]\n",
            "4999 [D loss: 0.594499, acc.: 62.50%] [G loss: 0.852703]\n",
            "5000 [D loss: 0.599128, acc.: 62.50%] [G loss: 0.852195]\n",
            "generated_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model/gan\n",
        "!mkdir model/gan/saved"
      ],
      "metadata": {
        "id": "-pnvO3IIAzDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthesizer.generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaXiltIPA8Jq",
        "outputId": "9703238d-2417-47bd-a35c-f2486a057c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(32, 32)]                0         \n",
            "                                                                 \n",
            " dense (Dense)               (32, 128)                 4224      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (32, 256)                 33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (32, 512)                 131584    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (32, 10)                  5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173,962\n",
            "Trainable params: 173,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthesizer.discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VBsbNU3A_Gp",
        "outputId": "3a4ae808-8b08-404f-b174-c0a12969bd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(32, 10)]                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (32, 512)                 5632      \n",
            "                                                                 \n",
            " dropout (Dropout)           (32, 512)                 0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (32, 256)                 131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (32, 256)                 0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (32, 128)                 32896     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (32, 1)                   129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 169,985\n",
            "Trainable params: 0\n",
            "Non-trainable params: 169,985\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'GAN': ['GAN', False, synthesizer.generator]}"
      ],
      "metadata": {
        "id": "LCZ1VLvoJqut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['GAN']\n",
        "colors = ['deepskyblue','blue']\n",
        "markers = ['o','^']"
      ],
      "metadata": {
        "id": "_WMOIa5HPx3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI0qOWh2Q8ES",
        "outputId": "83930295-69db-4f50-e17b-99cbe59a26af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['conc (ppm)',\n",
              " 'ad dose(g/L)',\n",
              " 'ph value',\n",
              " 'Temperature(⁰C)',\n",
              " 'time',\n",
              " 'absorbance',\n",
              " 'conc',\n",
              " 'real conc',\n",
              " 'removal',\n",
              " '%removal  ']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup parameters visualization parameters\n",
        "seed = 17\n",
        "test_size = 151\n",
        "noise_dim = 32\n",
        "\n",
        "np.random.seed(seed)\n",
        "z = np.random.normal(size=(test_size, noise_dim))\n",
        "real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n",
        "real_samples = pd.DataFrame(real, columns=data_cols)\n"
      ],
      "metadata": {
        "id": "kyw9blFPRJdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col1,col2,col3,col4,col5,col6,col7,col8,col9,col10='conc (ppm)','ad dose(g/L)','ph value','Temperature(⁰C)','time','absorbance','conc','real conc','removal','%removal  '"
      ],
      "metadata": {
        "id": "GzwJZhlMR-eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_steps= [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
        "rows = len(model_steps)\n",
        "columns = 1"
      ],
      "metadata": {
        "id": "oSjcQ01ZTKy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = 'model/'"
      ],
      "metadata": {
        "id": "4RvQsPDO1ZUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for model_step_ix, model_step in enumerate(model_steps):\n",
        "  [model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "  generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
        "\n",
        "  #ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
        "\n",
        "  g_z = generator_model.predict(z)\n",
        "  gen_samples = pd.DataFrame(g_z, columns=data_cols)  \n"
      ],
      "metadata": {
        "id": "oDidOv-0T3-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " gen_samples.to_csv('Generated_sample.csv')"
      ],
      "metadata": {
        "id": "tlET3qrw1d2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy=gen_samples.iloc[:,-1]\n",
        "xx=gen_samples.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "GGpjYY3bcGWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " gen_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tMDhnCR-VSR7",
        "outputId": "2ae952d2-79ef-46f5-f258-d2b1e7676903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aca950cc-5bf6-4802-95e7-8ad1b0dbfe93\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.818941</td>\n",
              "      <td>1.246992</td>\n",
              "      <td>4.092569</td>\n",
              "      <td>28.023741</td>\n",
              "      <td>10.565492</td>\n",
              "      <td>0.839113</td>\n",
              "      <td>3.726613</td>\n",
              "      <td>4.576278</td>\n",
              "      <td>22.853926</td>\n",
              "      <td>78.747673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22.483795</td>\n",
              "      <td>4.536206</td>\n",
              "      <td>3.432658</td>\n",
              "      <td>29.118717</td>\n",
              "      <td>39.634861</td>\n",
              "      <td>-0.213572</td>\n",
              "      <td>2.840472</td>\n",
              "      <td>0.612078</td>\n",
              "      <td>22.311686</td>\n",
              "      <td>83.047218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.789936</td>\n",
              "      <td>7.339098</td>\n",
              "      <td>4.772743</td>\n",
              "      <td>46.768780</td>\n",
              "      <td>73.599854</td>\n",
              "      <td>-1.967138</td>\n",
              "      <td>7.328980</td>\n",
              "      <td>2.021305</td>\n",
              "      <td>32.822090</td>\n",
              "      <td>122.169601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.372044</td>\n",
              "      <td>1.335835</td>\n",
              "      <td>3.306332</td>\n",
              "      <td>28.604965</td>\n",
              "      <td>11.382053</td>\n",
              "      <td>0.576325</td>\n",
              "      <td>3.564733</td>\n",
              "      <td>4.785166</td>\n",
              "      <td>23.344597</td>\n",
              "      <td>81.198929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.722561</td>\n",
              "      <td>4.450108</td>\n",
              "      <td>2.989595</td>\n",
              "      <td>28.788498</td>\n",
              "      <td>43.957611</td>\n",
              "      <td>-0.845752</td>\n",
              "      <td>4.244786</td>\n",
              "      <td>1.268954</td>\n",
              "      <td>20.606926</td>\n",
              "      <td>76.768784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>24.343243</td>\n",
              "      <td>3.823370</td>\n",
              "      <td>3.833190</td>\n",
              "      <td>31.374702</td>\n",
              "      <td>28.174414</td>\n",
              "      <td>0.253567</td>\n",
              "      <td>2.680391</td>\n",
              "      <td>1.608387</td>\n",
              "      <td>23.266228</td>\n",
              "      <td>85.779030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>27.789547</td>\n",
              "      <td>0.636933</td>\n",
              "      <td>2.437756</td>\n",
              "      <td>32.717197</td>\n",
              "      <td>12.064705</td>\n",
              "      <td>1.026199</td>\n",
              "      <td>4.306577</td>\n",
              "      <td>6.374099</td>\n",
              "      <td>22.542580</td>\n",
              "      <td>77.305450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>20.776632</td>\n",
              "      <td>5.602990</td>\n",
              "      <td>4.166401</td>\n",
              "      <td>33.044666</td>\n",
              "      <td>52.454540</td>\n",
              "      <td>-0.558148</td>\n",
              "      <td>5.100426</td>\n",
              "      <td>0.960695</td>\n",
              "      <td>24.887739</td>\n",
              "      <td>94.021698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>33.384750</td>\n",
              "      <td>3.927462</td>\n",
              "      <td>5.439592</td>\n",
              "      <td>41.028702</td>\n",
              "      <td>27.715914</td>\n",
              "      <td>0.336721</td>\n",
              "      <td>4.286004</td>\n",
              "      <td>3.990602</td>\n",
              "      <td>30.985470</td>\n",
              "      <td>109.060081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>31.555012</td>\n",
              "      <td>6.422427</td>\n",
              "      <td>4.379303</td>\n",
              "      <td>42.141502</td>\n",
              "      <td>67.401848</td>\n",
              "      <td>-1.691420</td>\n",
              "      <td>7.091534</td>\n",
              "      <td>1.887751</td>\n",
              "      <td>29.565063</td>\n",
              "      <td>110.525993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>151 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aca950cc-5bf6-4802-95e7-8ad1b0dbfe93')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aca950cc-5bf6-4802-95e7-8ad1b0dbfe93 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aca950cc-5bf6-4802-95e7-8ad1b0dbfe93');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)       time  \\\n",
              "0     25.818941      1.246992  4.092569        28.023741  10.565492   \n",
              "1     22.483795      4.536206  3.432658        29.118717  39.634861   \n",
              "2     34.789936      7.339098  4.772743        46.768780  73.599854   \n",
              "3     25.372044      1.335835  3.306332        28.604965  11.382053   \n",
              "4     21.722561      4.450108  2.989595        28.788498  43.957611   \n",
              "..          ...           ...       ...              ...        ...   \n",
              "146   24.343243      3.823370  3.833190        31.374702  28.174414   \n",
              "147   27.789547      0.636933  2.437756        32.717197  12.064705   \n",
              "148   20.776632      5.602990  4.166401        33.044666  52.454540   \n",
              "149   33.384750      3.927462  5.439592        41.028702  27.715914   \n",
              "150   31.555012      6.422427  4.379303        42.141502  67.401848   \n",
              "\n",
              "     absorbance      conc  real conc    removal  %removal    \n",
              "0      0.839113  3.726613   4.576278  22.853926   78.747673  \n",
              "1     -0.213572  2.840472   0.612078  22.311686   83.047218  \n",
              "2     -1.967138  7.328980   2.021305  32.822090  122.169601  \n",
              "3      0.576325  3.564733   4.785166  23.344597   81.198929  \n",
              "4     -0.845752  4.244786   1.268954  20.606926   76.768784  \n",
              "..          ...       ...        ...        ...         ...  \n",
              "146    0.253567  2.680391   1.608387  23.266228   85.779030  \n",
              "147    1.026199  4.306577   6.374099  22.542580   77.305450  \n",
              "148   -0.558148  5.100426   0.960695  24.887739   94.021698  \n",
              "149    0.336721  4.286004   3.990602  30.985470  109.060081  \n",
              "150   -1.691420  7.091534   1.887751  29.565063  110.525993  \n",
              "\n",
              "[151 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN2pUu4iaZDr",
        "outputId": "f92239cf-ebb7-41a7-c786-7630086511a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-XjLCIBaets",
        "outputId": "aa0235d5-92ec-427a-c0b0-c00dce04eddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model(hp):\n",
        "  build_model=Sequential()\n",
        "  build_model.add(Dense(units=hp.Int('n',min_value=16,max_value=512,step=16),activation='relu',input_dim=Xtrain.shape[1]))\n",
        "  for i in range(hp.Int('nlayers',min_value=2,max_value=10)):\n",
        "    build_model.add(Dense(units=hp.Int('units',min_value=16,max_value=256,step=16),activation='relu'))\n",
        "  build_model.add(Dense(units=1,activation='linear'))\n",
        "  build_model.compile(optimizer=Adam(hp.Choice('lr',[1e-2,1e-3,1e-4])),loss='mean_absolute_error',metrics=['mean_absolute_error'])\n",
        "  return build_model"
      ],
      "metadata": {
        "id": "H4paFoHKavOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner=RandomSearch(model,objective='val_mean_absolute_error',max_trials=5,executions_per_trial=3,directory='output',project_name='chemann')"
      ],
      "metadata": {
        "id": "OG_AKi7da3_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yg=ddgan.iloc[:,-1]\n",
        "xg=ddgan.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "HpSnPLph7O0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest=ddtest.iloc[:,-1]\n",
        "xtest=ddtest.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "gK81NI5P7k8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v093a7i971rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(xg,yg,epochs=20,validation_data=(xtest,ytest),batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-gNJYa2a7BJ",
        "outputId": "db6016c9-38ab-41ff-dd82-4b70dc3d18d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 01m 03s]\n",
            "val_mean_absolute_error: 16.78976027170817\n",
            "\n",
            "Best val_mean_absolute_error So Far: 9.9497439066569\n",
            "Total elapsed time: 00h 04m 08s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model=tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "rzS8rCPVb0lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBYXRprMcWKR",
        "outputId": "0a77d507-2896-4035-99c0-74d476ca17fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 144)               864       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               37120     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 367,201\n",
            "Trainable params: 367,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ygpred=best_model.predict(xtest)"
      ],
      "metadata": {
        "id": "MBieCePfcaTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ygpred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KdNq7XYdZ4P",
        "outputId": "870b8fe8-a11a-4f76-e6f6-85e3e8b0aee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 88.43119 ],\n",
              "       [  8.206042],\n",
              "       [108.27571 ],\n",
              "       [ 87.96087 ],\n",
              "       [ 95.08157 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffDy31nJ9y1V",
        "outputId": "0da44295-c585-4c0f-c79e-4ab18b9a2750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6     97.844586\n",
              "20     5.213172\n",
              "87    85.523586\n",
              "53    98.418724\n",
              "66    95.054276\n",
              "Name: %removal  , dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "mae(ytest,ygpred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM47l5wu99mu",
        "outputId": "04ebe1bd-cc23-4512-db37-6763cf297e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.128708674305887"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yd=dd.iloc[:,-1]\n",
        "xd=dd.iloc[:,0:5]"
      ],
      "metadata": {
        "id": "By4Bt_86dcvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(hp):\n",
        "  build_model=Sequential()\n",
        "  build_model.add(Dense(units=hp.Int('n',min_value=16,max_value=512,step=16),activation='relu',input_dim=Xtrain.shape[1]))\n",
        "  for i in range(hp.Int('nlayers',min_value=2,max_value=10)):\n",
        "    build_model.add(Dense(units=hp.Int('units',min_value=16,max_value=256,step=16),activation='relu'))\n",
        "  build_model.add(Dense(units=1,activation='linear'))\n",
        "  build_model.compile(optimizer=Adam(hp.Choice('lr',[1e-2,1e-3,1e-4])),loss='mean_absolute_error',metrics=['mean_absolute_error'])\n",
        "  return build_model"
      ],
      "metadata": {
        "id": "xK424ceh825v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner=RandomSearch(model,objective='val_mean_absolute_error',max_trials=5,executions_per_trial=3,directory='output',project_name='chemanbn')"
      ],
      "metadata": {
        "id": "ZVV_9XmB9DX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(xd,yd,epochs=20,validation_data=(xtest,ytest),batch_size=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u32Esk99FvK",
        "outputId": "413a30fc-463f-4d4f-cf9a-943dbf278ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 00m 26s]\n",
            "val_mean_absolute_error: 12.119081497192383\n",
            "\n",
            "Best val_mean_absolute_error So Far: 10.68249257405599\n",
            "Total elapsed time: 00h 02m 00s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2=tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "cVj7Wn8s9a6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z8xcR_U9bmF",
        "outputId": "c942d363-e716-4ce3-9f6c-fd43cd99ac64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 496)               2976      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 224)               111328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 224)               50400     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 224)               50400     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 225       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215,329\n",
            "Trainable params: 215,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred=best_model2.predict(xtest)"
      ],
      "metadata": {
        "id": "TObPAzb9-a-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGUsR11d-jV1",
        "outputId": "aa449ee8-cb00-4b87-df17-87edaad21cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[102.503975 ],\n",
              "       [  5.4238796],\n",
              "       [123.481735 ],\n",
              "       [ 93.87606  ],\n",
              "       [ 94.04458  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ygpred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dUuabZC-kUy",
        "outputId": "671bc83d-eef6-45c9-c9c8-c7bf978bed5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 88.43119 ],\n",
              "       [  8.206042],\n",
              "       [108.27571 ],\n",
              "       [ 87.96087 ],\n",
              "       [ 95.08157 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCbMhpJ1-mlZ",
        "outputId": "02a31562-30af-47e1-d64e-7e2659ba4ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6     97.844586\n",
              "20     5.213172\n",
              "87    85.523586\n",
              "53    98.418724\n",
              "66    95.054276\n",
              "Name: %removal  , dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae(ytest,ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCozE718-p6p",
        "outputId": "4497f570-306b-4169-a454-2bd12254cd98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.676121180356779"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae(ytest,ygpred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc_0u7li-u8y",
        "outputId": "825ed5cc-f595-44da-ea1f-51f4d569bbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.128708674305887"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score as r\n",
        "r(ytest,ypred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYHOLxBl-yyl",
        "outputId": "a884c7c3-3187-42f1-f1fd-a8e864362bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7696538789619606"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r(ytest,ygpred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HdMGQX7_TEs",
        "outputId": "834cd483-be9c-40c1-d116-1c1dac5d7f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8875458696821553"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "LxsYGyyk_VnU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(tidyverse)"
      ],
      "metadata": {
        "id": "HjKYu9K0HevT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2167f346-df1b-438d-cd5d-f77753a9be69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "R[write to console]: ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
            "\n",
            "R[write to console]: ✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n",
            "✔ tibble  3.1.6     ✔ dplyr   1.0.8\n",
            "✔ tidyr   1.2.0     ✔ stringr 1.4.0\n",
            "✔ readr   2.1.2     ✔ forcats 0.5.1\n",
            "\n",
            "R[write to console]: ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "✖ dplyr::filter() masks stats::filter()\n",
            "✖ dplyr::lag()    masks stats::lag()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "install.packages('rsm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bptso3gMHoRj",
        "outputId": "f6591ed1-09b9-45fc-bf70-4e139f9b7c44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: also installing the dependency ‘estimability’\n",
            "\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/estimability_1.3.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 9481 bytes\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 9481 bytes\n",
            "\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/rsm_2.10.3.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 793635 bytes (775 KB)\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 775 KB\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: \n",
            "R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpQhxVee/downloaded_packages’\n",
            "R[write to console]: \n",
            "R[write to console]: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "library(\"rsm\")"
      ],
      "metadata": {
        "id": "NirTQ5RkHxKU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "xaAsusIcIPCi",
        "outputId": "cbb5fbf6-4548-4eab-de41-863993ce88bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bacd7a72-72dc-42c1-8f49-0ec17bbc1ade\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conc (ppm)</th>\n",
              "      <th>ad dose(g/L)</th>\n",
              "      <th>ph value</th>\n",
              "      <th>Temperature(⁰C)</th>\n",
              "      <th>time</th>\n",
              "      <th>absorbance</th>\n",
              "      <th>conc</th>\n",
              "      <th>real conc</th>\n",
              "      <th>removal</th>\n",
              "      <th>%removal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0229</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>0.789655</td>\n",
              "      <td>29.210345</td>\n",
              "      <td>97.270448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0153</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>0.527586</td>\n",
              "      <td>29.472414</td>\n",
              "      <td>98.143138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>29.475862</td>\n",
              "      <td>98.154621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>29.517241</td>\n",
              "      <td>98.292414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>0.379310</td>\n",
              "      <td>29.620690</td>\n",
              "      <td>98.636897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.0248</td>\n",
              "      <td>0.855172</td>\n",
              "      <td>0.855172</td>\n",
              "      <td>29.144828</td>\n",
              "      <td>97.052276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>0.617241</td>\n",
              "      <td>29.382759</td>\n",
              "      <td>97.844586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.503448</td>\n",
              "      <td>0.503448</td>\n",
              "      <td>29.496552</td>\n",
              "      <td>98.223517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.437931</td>\n",
              "      <td>0.437931</td>\n",
              "      <td>29.562069</td>\n",
              "      <td>98.441690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.382759</td>\n",
              "      <td>0.382759</td>\n",
              "      <td>29.617241</td>\n",
              "      <td>98.625414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.2555</td>\n",
              "      <td>8.810345</td>\n",
              "      <td>8.810345</td>\n",
              "      <td>21.189655</td>\n",
              "      <td>70.561552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.0880</td>\n",
              "      <td>3.034483</td>\n",
              "      <td>3.034483</td>\n",
              "      <td>26.965517</td>\n",
              "      <td>89.795172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.0499</td>\n",
              "      <td>1.720690</td>\n",
              "      <td>1.720690</td>\n",
              "      <td>28.279310</td>\n",
              "      <td>94.170103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.955172</td>\n",
              "      <td>0.955172</td>\n",
              "      <td>29.044828</td>\n",
              "      <td>96.719276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>29.400000</td>\n",
              "      <td>97.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.8256</td>\n",
              "      <td>28.468966</td>\n",
              "      <td>28.468966</td>\n",
              "      <td>1.531034</td>\n",
              "      <td>5.098345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.6474</td>\n",
              "      <td>22.324138</td>\n",
              "      <td>22.324138</td>\n",
              "      <td>7.675862</td>\n",
              "      <td>25.560621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.6065</td>\n",
              "      <td>20.913793</td>\n",
              "      <td>20.913793</td>\n",
              "      <td>9.086207</td>\n",
              "      <td>30.257069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.4463</td>\n",
              "      <td>15.389655</td>\n",
              "      <td>15.389655</td>\n",
              "      <td>14.610345</td>\n",
              "      <td>48.652448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>60</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.0961</td>\n",
              "      <td>3.313793</td>\n",
              "      <td>3.313793</td>\n",
              "      <td>26.686207</td>\n",
              "      <td>88.865069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.8246</td>\n",
              "      <td>28.434483</td>\n",
              "      <td>28.434483</td>\n",
              "      <td>1.565517</td>\n",
              "      <td>5.213172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.6648</td>\n",
              "      <td>22.924138</td>\n",
              "      <td>22.924138</td>\n",
              "      <td>7.075862</td>\n",
              "      <td>23.562621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.6524</td>\n",
              "      <td>22.496552</td>\n",
              "      <td>22.496552</td>\n",
              "      <td>7.503448</td>\n",
              "      <td>24.986483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.5590</td>\n",
              "      <td>19.275862</td>\n",
              "      <td>19.275862</td>\n",
              "      <td>10.724138</td>\n",
              "      <td>35.711379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>90</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.3287</td>\n",
              "      <td>11.334483</td>\n",
              "      <td>11.334483</td>\n",
              "      <td>18.665517</td>\n",
              "      <td>62.156172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>30</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>15</td>\n",
              "      <td>0.5202</td>\n",
              "      <td>17.937931</td>\n",
              "      <td>17.937931</td>\n",
              "      <td>12.062069</td>\n",
              "      <td>40.166690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>30</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.3684</td>\n",
              "      <td>12.703448</td>\n",
              "      <td>12.703448</td>\n",
              "      <td>17.296552</td>\n",
              "      <td>57.597517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>30</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>45</td>\n",
              "      <td>0.3256</td>\n",
              "      <td>11.227586</td>\n",
              "      <td>11.227586</td>\n",
              "      <td>18.772414</td>\n",
              "      <td>62.512138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>0.3184</td>\n",
              "      <td>10.979310</td>\n",
              "      <td>10.979310</td>\n",
              "      <td>19.020690</td>\n",
              "      <td>63.338897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>0.5</td>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>0.2629</td>\n",
              "      <td>9.065517</td>\n",
              "      <td>9.065517</td>\n",
              "      <td>20.934483</td>\n",
              "      <td>69.711828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bacd7a72-72dc-42c1-8f49-0ec17bbc1ade')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bacd7a72-72dc-42c1-8f49-0ec17bbc1ade button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bacd7a72-72dc-42c1-8f49-0ec17bbc1ade');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    conc (ppm)  ad dose(g/L)  ph value  Temperature(⁰C)  time  absorbance  \\\n",
              "0           10           1.0         6               30    15      0.0229   \n",
              "1           10           1.0         6               30    30      0.0153   \n",
              "2           10           1.0         6               30    45      0.0152   \n",
              "3           10           1.0         6               30    60      0.0140   \n",
              "4           10           1.0         6               30   120      0.0110   \n",
              "5           20           1.0         6               30    15      0.0248   \n",
              "6           20           1.0         6               30    30      0.0179   \n",
              "7           20           1.0         6               30    45      0.0146   \n",
              "8           20           1.0         6               30    60      0.0127   \n",
              "9           20           1.0         6               30   120      0.0111   \n",
              "10          30           1.0         6               30    15      0.2555   \n",
              "11          30           1.0         6               30    30      0.0880   \n",
              "12          30           1.0         6               30    45      0.0499   \n",
              "13          30           1.0         6               30    60      0.0277   \n",
              "14          30           1.0         6               30   120      0.0174   \n",
              "15          60           1.0         6               30    15      0.8256   \n",
              "16          60           1.0         6               30    30      0.6474   \n",
              "17          60           1.0         6               30    45      0.6065   \n",
              "18          60           1.0         6               30    60      0.4463   \n",
              "19          60           1.0         6               30   120      0.0961   \n",
              "20          90           1.0         6               30    15      0.8246   \n",
              "21          90           1.0         6               30    30      0.6648   \n",
              "22          90           1.0         6               30    45      0.6524   \n",
              "23          90           1.0         6               30    60      0.5590   \n",
              "24          90           1.0         6               30   120      0.3287   \n",
              "25          30           0.5         6               30    15      0.5202   \n",
              "26          30           0.5         6               30    30      0.3684   \n",
              "27          30           0.5         6               30    45      0.3256   \n",
              "28          30           0.5         6               30    60      0.3184   \n",
              "29          30           0.5         6               30   120      0.2629   \n",
              "\n",
              "         conc  real conc    removal  %removal    \n",
              "0    0.789655   0.789655  29.210345   97.270448  \n",
              "1    0.527586   0.527586  29.472414   98.143138  \n",
              "2    0.524138   0.524138  29.475862   98.154621  \n",
              "3    0.482759   0.482759  29.517241   98.292414  \n",
              "4    0.379310   0.379310  29.620690   98.636897  \n",
              "5    0.855172   0.855172  29.144828   97.052276  \n",
              "6    0.617241   0.617241  29.382759   97.844586  \n",
              "7    0.503448   0.503448  29.496552   98.223517  \n",
              "8    0.437931   0.437931  29.562069   98.441690  \n",
              "9    0.382759   0.382759  29.617241   98.625414  \n",
              "10   8.810345   8.810345  21.189655   70.561552  \n",
              "11   3.034483   3.034483  26.965517   89.795172  \n",
              "12   1.720690   1.720690  28.279310   94.170103  \n",
              "13   0.955172   0.955172  29.044828   96.719276  \n",
              "14   0.600000   0.600000  29.400000   97.902000  \n",
              "15  28.468966  28.468966   1.531034    5.098345  \n",
              "16  22.324138  22.324138   7.675862   25.560621  \n",
              "17  20.913793  20.913793   9.086207   30.257069  \n",
              "18  15.389655  15.389655  14.610345   48.652448  \n",
              "19   3.313793   3.313793  26.686207   88.865069  \n",
              "20  28.434483  28.434483   1.565517    5.213172  \n",
              "21  22.924138  22.924138   7.075862   23.562621  \n",
              "22  22.496552  22.496552   7.503448   24.986483  \n",
              "23  19.275862  19.275862  10.724138   35.711379  \n",
              "24  11.334483  11.334483  18.665517   62.156172  \n",
              "25  17.937931  17.937931  12.062069   40.166690  \n",
              "26  12.703448  12.703448  17.296552   57.597517  \n",
              "27  11.227586  11.227586  18.772414   62.512138  \n",
              "28  10.979310  10.979310  19.020690   63.338897  \n",
              "29   9.065517   9.065517  20.934483   69.711828  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['conc (ppm)'].unique()"
      ],
      "metadata": {
        "id": "3325dWIOXZv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea997c3-5bdd-400a-de9f-926c31ccf907"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 20, 30, 60, 90])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['ad dose(g/L)'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKZ1EEzTXsl2",
        "outputId": "53426ec0-f472-499f-eba7-6b12f2ef4a38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1. , 0.5, 1.5, 2. ])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['ph value'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXq5F68LXuPt",
        "outputId": "f11a739b-8d51-4bec-97f8-ab7fcb926e04"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6,  2,  4,  8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Temperature(⁰C)'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-7plEIOXvCb",
        "outputId": "7b35daf4-4e4d-40dd-d3e1-6cc61f0694e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.54945054945055"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['time'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd_QEy9qXvhL",
        "outputId": "e3bd39eb-19d7-46c2-ae96-2fe5d190ac48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53.40659340659341"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5N-hIHsFNo8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "CR1 <- coded.data(data, x1 ~ (time - 60)/60, x2 ~ (ph value-6.0)/4,x3=(ad dose(g/L)-1.5)/0.5)\n",
        "\n"
      ],
      "metadata": {
        "id": "J3YCgQBwIPvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "5d72bea8-240c-418c-8df5-06cf2c542cf0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RParsingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRParsingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d092706eb354>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CR1 <- coded.data(data, x1 ~ (time - 60)/60, x2 ~ (ph value-6.0)/4,x3=(ad dose(g/L)-1.5)/0.5)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-119>\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/ipython/rmagic.py\u001b[0m in \u001b[0;36mR\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0mreturn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mtext_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mtext_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/ipython/rmagic.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# Need the newline in case the last line in code is a comment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"withVisible({%s\\n})\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;31m# Otherwise next return seems to have copy of error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/robjects/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/rinterface_lib/conversion.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_res_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# TODO: test cdata is of the expected CType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cdata_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(text, num)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mrobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStrSexpVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmemorymanagement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrmemory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sexp__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rpy2/rinterface_lib/_rinterface_capi.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(cdata, num, rmemory)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mopenrlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPARSE_OK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         raise RParsingError('Parsing status not OK',\n\u001b[0;32m--> 653\u001b[0;31m                             status=PARSING_STATUS(status[0]))\n\u001b[0m\u001b[1;32m    654\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRParsingError\u001b[0m: Parsing status not OK - PARSING_STATUS.PARSE_ERROR"
          ]
        }
      ]
    }
  ]
}